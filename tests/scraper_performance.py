#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æŠ“å–æ€§èƒ½æµ‹è¯•æ¨¡å—

æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
1. AmazonæŠ“å–é€Ÿåº¦
2. TikTokæŠ“å–æ•ˆç‡
3. æ‰¹é‡å¤„ç†èƒ½åŠ›
4. é”™è¯¯ç‡ç»Ÿè®¡

æµ‹è¯•æŒ‡æ ‡ï¼š
- æŠ“å–æˆåŠŸç‡: > 95%
- å•é¡µé¢æŠ“å–æ—¶é—´: < 5s
- æ‰¹å¤„ç†ååé‡: > 10 pages/minute
- é”™è¯¯ç‡: < 5%
"""

import time
import json
import statistics
import random
from typing import Dict, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import logging
import threading
import queue
from dataclasses import dataclass, asdict
from contextlib import contextmanager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapingTestResult:
    """æŠ“å–æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    platform: str
    test_name: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time: float
    success_rate: float
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    errors: List[str]

class MockScraper:
    """æ¨¡æ‹ŸæŠ“å–å™¨ï¼Œç”¨äºæ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, platform: str):
        self.platform = platform
        self.success_rate = 0.95 if platform == 'amazon' else 0.93  # AmazonæˆåŠŸç‡ç¨é«˜
        self.avg_response_time = 2.5 if platform == 'amazon' else 3.2  # TikTokå“åº”ç¨æ…¢
    
    def scrape_page(self, page_num: int, force_error: bool = False) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå•é¡µé¢æŠ“å–"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        response_time = random.uniform(0.5, 6.0) * self.avg_response_time
        time.sleep(response_time)
        
        # æ¨¡æ‹ŸæˆåŠŸ/å¤±è´¥
        if force_error or random.random() > self.success_rate:
            end_time = time.time()
            return {
                'success': False,
                'error': f'æŠ“å–å¤±è´¥: {self.platform} page {page_num}',
                'response_time': end_time - start_time
            }
        
        # æ¨¡æ‹ŸæˆåŠŸè¿”å›çš„æ•°æ®
        products = []
        for i in range(random.randint(3, 15)):
            product = {
                'id': f'{self.platform}_product_{page_num}_{i}',
                'name': f'{self.platform.title()} äº§å“ {page_num}-{i}',
                'price': round(random.uniform(15.99, 89.99), 2),
                'rating': round(random.uniform(3.0, 5.0), 1),
                'reviews': random.randint(10, 1000),
                'platform': self.platform,
                'category': random.choice(['tshirt', 'hoodie', 'sweatshirt'])
            }
            products.append(product)
        
        end_time = time.time()
        
        return {
            'success': True,
            'data': {
                'products': products,
                'total_products': len(products),
                'page': page_num,
                'platform': self.platform
            },
            'response_time': end_time - start_time
        }
    
    def scrape_with_retry(self, page_num: int, max_retries: int = 3) -> Tuple[bool, Any]:
        """å¸¦é‡è¯•çš„æŠ“å–"""
        for attempt in range(max_retries):
            result = self.scrape_page(page_num, force_error=attempt < max_retries-1 and random.random() < 0.3)
            if result['success']:
                return True, result
            elif attempt == max_retries - 1:
                return False, result
        
        return False, result

class ScrapingPerformanceTest:
    """æ•°æ®æŠ“å–æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.test_results = {}
        self.mock_data = {}
        
    def test_single_page_performance(self):
        """æµ‹è¯•å•é¡µé¢æŠ“å–æ€§èƒ½"""
        logger.info("æµ‹è¯•å•é¡µé¢æŠ“å–æ€§èƒ½...")
        
        platforms = ['amazon', 'tiktok']
        test_results = {}
        
        for platform in platforms:
            logger.info(f"æµ‹è¯• {platform} å¹³å°...")
            
            scraper = MockScraper(platform)
            response_times = []
            successful = 0
            failed = 0
            errors = []
            
            # æµ‹è¯•100æ¬¡å•é¡µé¢æŠ“å–
            for page_num in range(1, 101):
                result = scraper.scrape_page(page_num)
                response_times.append(result['response_time'])
                
                if result['success']:
                    successful += 1
                else:
                    failed += 1
                    errors.append(result['error'])
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            total_requests = len(response_times)
            success_rate = (successful / total_requests) * 100
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            
            platform_result = {
                'total_requests': total_requests,
                'successful_requests': successful,
                'failed_requests': failed,
                'success_rate': round(success_rate, 2),
                'avg_response_time': round(avg_response_time, 2),
                'min_response_time': round(min_response_time, 2),
                'max_response_time': round(max_response_time, 2),
                'errors': errors[:10],  # åªä¿ç•™å‰10ä¸ªé”™è¯¯
                'target_met': success_rate >= 95 and avg_response_time < 5.0
            }
            
            test_results[platform] = platform_result
            logger.info(f"{platform}: æˆåŠŸç‡ {success_rate:.1f}%, å¹³å‡å“åº” {avg_response_time:.2f}s")
        
        self.test_results['single_page_performance'] = test_results
        return test_results
    
    def test_batch_processing_performance(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½"""
        logger.info("æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½...")
        
        platforms = ['amazon', 'tiktok']
        batch_results = {}
        
        for platform in platforms:
            logger.info(f"æµ‹è¯• {platform} æ‰¹é‡å¤„ç†...")
            
            scraper = MockScraper(platform)
            batch_sizes = [5, 10, 20, 50]
            batch_performance = {}
            
            for batch_size in batch_sizes:
                logger.info(f"  æ‰¹é‡å¤§å°: {batch_size}")
                
                start_time = time.time()
                successful = 0
                failed = 0
                response_times = []
                
                # ä¸²è¡Œæ‰¹é‡å¤„ç†
                for page_num in range(1, batch_size + 1):
                    result = scraper.scrape_page(page_num)
                    response_times.append(result['response_time'])
                    
                    if result['success']:
                        successful += 1
                    else:
                        failed += 1
                
                total_time = time.time() - start_time
                throughput = batch_size / total_time  # pages/minute
                
                batch_performance[f'batch_{batch_size}'] = {
                    'batch_size': batch_size,
                    'total_time': round(total_time, 2),
                    'throughput_pages_per_minute': round(throughput * 60, 2),
                    'successful': successful,
                    'failed': failed,
                    'success_rate': round((successful / batch_size) * 100, 2),
                    'avg_response_time': round(statistics.mean(response_times), 2)
                }
                
                logger.info(f"    æ‰¹é‡ {batch_size}: {throughput*60:.1f} pages/min, æˆåŠŸç‡ {(successful/batch_size)*100:.1f}%")
            
            batch_results[platform] = batch_performance
        
        self.test_results['batch_processing_performance'] = batch_results
        return batch_results
    
    def test_concurrent_scraping(self):
        """æµ‹è¯•å¹¶å‘æŠ“å–æ€§èƒ½"""
        logger.info("æµ‹è¯•å¹¶å‘æŠ“å–æ€§èƒ½...")
        
        platforms = ['amazon', 'tiktok']
        concurrent_results = {}
        
        for platform in platforms:
            logger.info(f"æµ‹è¯• {platform} å¹¶å‘æŠ“å–...")
            
            scraper = MockScraper(platform)
            worker_counts = [2, 5, 10, 15]
            concurrent_performance = {}
            
            for worker_count in worker_counts:
                logger.info(f"  å¹¶å‘æ•°: {worker_count}")
                
                def scraping_worker(worker_id, page_count):
                    """æŠ“å–å·¥ä½œçº¿ç¨‹"""
                    worker_times = []
                    successful = 0
                    failed = 0
                    
                    for i in range(page_count):
                        page_num = worker_id * page_count + i + 1
                        result = scraper.scrape_page(page_num)
                        worker_times.append(result['response_time'])
                        
                        if result['success']:
                            successful += 1
                        else:
                            failed += 1
                    
                    return {
                        'worker_id': worker_id,
                        'times': worker_times,
                        'successful': successful,
                        'failed': failed
                    }
                
                # æ‰§è¡Œå¹¶å‘æµ‹è¯•
                pages_per_worker = 10
                start_time = time.time()
                
                with ThreadPoolExecutor(max_workers=worker_count) as executor:
                    futures = [
                        executor.submit(scraping_worker, i, pages_per_worker)
                        for i in range(worker_count)
                    ]
                    
                    worker_results = []
                    for future in as_completed(futures):
                        try:
                            result = future.result()
                            worker_results.append(result)
                        except Exception as e:
                            logger.error(f"å¹¶å‘å·¥ä½œå‡ºé”™: {e}")
                
                total_time = time.time() - start_time
                total_requests = worker_count * pages_per_worker
                
                # æ±‡æ€»ç»“æœ
                all_times = []
                total_successful = 0
                total_failed = 0
                
                for worker_result in worker_results:
                    all_times.extend(worker_result['times'])
                    total_successful += worker_result['successful']
                    total_failed += worker_result['failed']
                
                throughput = total_requests / total_time
                success_rate = (total_successful / total_requests) * 100
                
                concurrent_performance[f'workers_{worker_count}'] = {
                    'worker_count': worker_count,
                    'total_requests': total_requests,
                    'total_time': round(total_time, 2),
                    'throughput_requests_per_minute': round(throughput * 60, 2),
                    'successful': total_successful,
                    'failed': total_failed,
                    'success_rate': round(success_rate, 2),
                    'avg_response_time': round(statistics.mean(all_times), 2),
                    'target_met': success_rate >= 90 and throughput >= 5  # å¹¶å‘ç›®æ ‡ç¨ä½
                }
                
                logger.info(f"    {worker_count} workers: {throughput:.1f} req/min, æˆåŠŸç‡ {success_rate:.1f}%")
            
            concurrent_results[platform] = concurrent_performance
        
        self.test_results['concurrent_scraping'] = concurrent_results
        return concurrent_results
    
    def test_error_handling_and_retry(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        logger.info("æµ‹è¯•é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶...")
        
        platforms = ['amazon', 'tiktok']
        error_handling_results = {}
        
        for platform in platforms:
            logger.info(f"æµ‹è¯• {platform} é”™è¯¯å¤„ç†...")
            
            scraper = MockScraper(platform)
            retry_tests = {}
            
            # æµ‹è¯•ä¸åŒé‡è¯•æ¬¡æ•°çš„æ•ˆæœ
            for max_retries in [1, 2, 3, 5]:
                logger.info(f"  æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")
                
                successful = 0
                failed = 0
                total_attempts = 0
                retry_counts = []
                
                # æµ‹è¯•50æ¬¡æŠ“å–
                for page_num in range(1, 51):
                    success, result = scraper.scrape_with_retry(page_num, max_retries)
                    
                    if success:
                        successful += 1
                    else:
                        failed += 1
                    
                    total_attempts += result.get('attempts', 1)
                    # æ¨¡æ‹Ÿé‡è¯•æ¬¡æ•°
                    retry_counts.append(random.randint(0, max_retries))
                
                final_success_rate = (successful / 50) * 100
                avg_retries = statistics.mean(retry_counts)
                
                retry_tests[f'max_retries_{max_retries}'] = {
                    'max_retries': max_retries,
                    'final_success_rate': round(final_success_rate, 2),
                    'avg_retries_attempted': round(avg_retries, 2),
                    'successful_final': successful,
                    'failed_final': failed,
                    'improvement_from_single': round(final_success_rate - 95, 2) if platform == 'amazon' else round(final_success_rate - 93, 2)
                }
                
                logger.info(f"    æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%, å¹³å‡é‡è¯•: {avg_retries:.1f}")
            
            error_handling_results[platform] = retry_tests
        
        self.test_results['error_handling_retry'] = error_handling_results
        return error_handling_results
    
    def test_data_quality_metrics(self):
        """æµ‹è¯•æ•°æ®è´¨é‡æŒ‡æ ‡"""
        logger.info("æµ‹è¯•æ•°æ®è´¨é‡æŒ‡æ ‡...")
        
        platforms = ['amazon', 'tiktok']
        quality_results = {}
        
        for platform in platforms:
            logger.info(f"æµ‹è¯• {platform} æ•°æ®è´¨é‡...")
            
            scraper = MockScraper(platform)
            
            # æ”¶é›†100é¡µæ•°æ®ç”¨äºè´¨é‡åˆ†æ
            all_products = []
            for page_num in range(1, 101):
                result = scraper.scrape_page(page_num)
                if result['success']:
                    all_products.extend(result['data']['products'])
            
            # åˆ†ææ•°æ®è´¨é‡
            quality_metrics = self._analyze_data_quality(all_products, platform)
            quality_results[platform] = quality_metrics
            
            logger.info(f"  æ•°æ®å®Œæ•´æ€§: {quality_metrics['completeness_score']:.1f}%")
            logger.info(f"  æ•°æ®ä¸€è‡´æ€§: {quality_metrics['consistency_score']:.1f}%")
            logger.info(f"  é‡å¤ç‡: {quality_metrics['duplicate_rate']:.2f}%")
        
        self.test_results['data_quality'] = quality_results
        return quality_results
    
    def _analyze_data_quality(self, products: List[Dict], platform: str) -> Dict[str, Any]:
        """åˆ†ææ•°æ®è´¨é‡"""
        if not products:
            return {
                'total_products': 0,
                'completeness_score': 0,
                'consistency_score': 0,
                'duplicate_rate': 0,
                'data_freshness_score': 0
            }
        
        total_products = len(products)
        
        # å®Œæ•´æ€§æ£€æŸ¥
        required_fields = ['id', 'name', 'price', 'platform']
        complete_products = 0
        for product in products:
            if all(field in product and product[field] for field in required_fields):
                complete_products += 1
        
        completeness_score = (complete_products / total_products) * 100
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        consistent_products = 0
        for product in products:
            # æ£€æŸ¥ä»·æ ¼æ ¼å¼
            price_valid = isinstance(product.get('price'), (int, float)) and product.get('price', 0) > 0
            # æ£€æŸ¥å¹³å°å­—æ®µ
            platform_valid = product.get('platform') == platform
            # æ£€æŸ¥è¯„åˆ†èŒƒå›´
            rating_valid = isinstance(product.get('rating'), (int, float)) and 0 <= product.get('rating', 0) <= 5
            
            if price_valid and platform_valid and rating_valid:
                consistent_products += 1
        
        consistency_score = (consistent_products / total_products) * 100
        
        # é‡å¤ç‡æ£€æŸ¥
        ids = [p.get('id') for p in products if p.get('id')]
        unique_ids = set(ids)
        duplicate_rate = ((len(ids) - len(unique_ids)) / len(ids)) * 100 if ids else 0
        
        # æ•°æ®æ–°é²œåº¦ (æ¨¡æ‹Ÿ)
        freshness_score = random.uniform(85, 98)  # 85-98%
        
        return {
            'total_products': total_products,
            'complete_products': complete_products,
            'completeness_score': round(completeness_score, 1),
            'consistent_products': consistent_products,
            'consistency_score': round(consistency_score, 1),
            'unique_products': len(unique_ids),
            'duplicate_rate': round(duplicate_rate, 2),
            'data_freshness_score': round(freshness_score, 1),
            'target_met': completeness_score >= 90 and consistency_score >= 85 and duplicate_rate < 5
        }
    
    def test_resource_usage(self):
        """æµ‹è¯•èµ„æºä½¿ç”¨æƒ…å†µ"""
        logger.info("æµ‹è¯•èµ„æºä½¿ç”¨æƒ…å†µ...")
        
        platforms = ['amazon', 'tiktok']
        resource_results = {}
        
        import psutil
        process = psutil.Process()
        
        for platform in platforms:
            logger.info(f"æµ‹è¯• {platform} èµ„æºä½¿ç”¨...")
            
            scraper = MockScraper(platform)
            
            # ç›‘æ§å¼€å§‹æ—¶çš„èµ„æºä½¿ç”¨
            initial_cpu = process.cpu_percent()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ‰§è¡Œå¤§é‡æŠ“å–æµ‹è¯•èµ„æºæ¶ˆè€—
            start_time = time.time()
            successful_count = 0
            failed_count = 0
            
            for page_num in range(1, 201):  # 200é¡µ
                result = scraper.scrape_page(page_num)
                if result['success']:
                    successful_count += 1
                else:
                    failed_count += 1
            
            # ç›‘æ§ç»“æŸæ—¶çš„èµ„æºä½¿ç”¨
            end_time = time.time()
            final_cpu = process.cpu_percent()
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            test_duration = end_time - start_time
            memory_increase = final_memory - initial_memory
            cpu_avg = (initial_cpu + final_cpu) / 2
            
            resource_results[platform] = {
                'test_duration': round(test_duration, 2),
                'total_requests': 200,
                'successful_requests': successful_count,
                'failed_requests': failed_count,
                'initial_memory_mb': round(initial_memory, 2),
                'final_memory_mb': round(final_memory, 2),
                'memory_increase_mb': round(memory_increase, 2),
                'avg_cpu_percent': round(cpu_avg, 2),
                'requests_per_second': round(200 / test_duration, 2),
                'target_met': memory_increase < 50 and cpu_avg < 50  # å†…å­˜å¢é•¿<50MB, CPU<50%
            }
            
            logger.info(f"  æµ‹è¯•æ—¶é•¿: {test_duration:.1f}s")
            logger.info(f"  å†…å­˜å¢é•¿: {memory_increase:.1f}MB")
            logger.info(f"  å¹³å‡CPU: {cpu_avg:.1f}%")
        
        self.test_results['resource_usage'] = resource_results
        return resource_results
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æŠ“å–æ€§èƒ½æµ‹è¯•"""
        logger.info("å¼€å§‹æ•°æ®æŠ“å–æ€§èƒ½æµ‹è¯•...")
        
        try:
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.test_single_page_performance()
            self.test_batch_processing_performance()
            self.test_concurrent_scraping()
            self.test_error_handling_and_retry()
            self.test_data_quality_metrics()
            self.test_resource_usage()
            
            logger.info("æ•°æ®æŠ“å–æ€§èƒ½æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        return {
            'test_type': 'scraper_performance',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'platforms_tested': ['amazon', 'tiktok'],
            'test_results': self.test_results,
            'summary': self._generate_summary()
        }
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'overall_success_rate': 0,
            'overall_status': 'unknown',
            'platform_scores': {}
        }
        
        # åˆ†æå„å¹³å°è¡¨ç°
        platforms = ['amazon', 'tiktok']
        platform_scores = {}
        
        for platform in platforms:
            platform_score = 0
            platform_total = 0
            
            # æ±‡æ€»å„æµ‹è¯•ç»“æœ
            for test_category, results in self.test_results.items():
                if platform in results:
                    platform_total += 1
                    
                    if test_category == 'single_page_performance':
                        if results[platform].get('target_met', False):
                            platform_score += 1
                    elif test_category == 'batch_processing_performance':
                        # æ£€æŸ¥æœ€å¤§æ‰¹é‡çš„æˆåŠŸç‡
                        batch_50 = results[platform].get('batch_50', {})
                        if batch_50.get('success_rate', 0) >= 90:
                            platform_score += 1
                    elif test_category == 'concurrent_scraping':
                        # æ£€æŸ¥æœ€ä¼˜å¹¶å‘é…ç½®
                        workers_10 = results[platform].get('workers_10', {})
                        if workers_10.get('target_met', False):
                            platform_score += 1
                    elif test_category == 'data_quality':
                        if results[platform].get('target_met', False):
                            platform_score += 1
                    elif test_category == 'resource_usage':
                        if results[platform].get('target_met', False):
                            platform_score += 1
            
            platform_scores[platform] = round((platform_score / platform_total) * 100, 1) if platform_total > 0 else 0
        
        summary['platform_scores'] = platform_scores
        
        # è®¡ç®—æ€»ä½“æˆåŠŸç‡
        success_rates = []
        if 'single_page_performance' in self.test_results:
            for platform, result in self.test_results['single_page_performance'].items():
                success_rates.append(result.get('success_rate', 0))
        
        if success_rates:
            summary['overall_success_rate'] = round(statistics.mean(success_rates), 1)
        
        # è®¡ç®—æ•´ä½“çŠ¶æ€
        passed_platforms = sum(1 for score in platform_scores.values() if score >= 80)
        
        if passed_platforms == len(platforms):
            summary['overall_status'] = 'passed'
        elif passed_platforms > 0:
            summary['overall_status'] = 'partial'
        else:
            summary['overall_status'] = 'failed'
        
        return summary


def run_scraper_performance_tests():
    """è¿è¡ŒæŠ“å–æ€§èƒ½æµ‹è¯•çš„ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ•°æ®æŠ“å–æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    tester = ScrapingPerformanceTest()
    
    try:
        # è¿è¡Œæµ‹è¯•
        tester.run_all_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_report()
        
        # è¾“å‡ºç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        summary = report['summary']
        print(f"   æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']}%")
        print(f"   Amazonæ€§èƒ½: {summary['platform_scores'].get('amazon', 0)}/100")
        print(f"   TikTokæ€§èƒ½: {summary['platform_scores'].get('tiktok', 0)}/100")
        print(f"   æ•´ä½“çŠ¶æ€: {summary['overall_status']}")
        
        # è¯¦ç»†ç»“æœ
        print("\nğŸ“ˆ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, results in report['test_results'].items():
            print(f"\n{test_name}:")
            if isinstance(results, dict):
                for platform, platform_results in results.items():
                    print(f"  {platform}:")
                    if isinstance(platform_results, dict):
                        for key, value in platform_results.items():
                            if key.endswith('_met') or isinstance(value, bool):
                                status = "âœ…" if value else "âŒ"
                                print(f"    {status} {key}: {value}")
                            else:
                                print(f"    {key}: {value}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = Path("tests/scraper_performance_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    run_scraper_performance_tests()