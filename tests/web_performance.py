#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘é¡µåº”ç”¨æ€§èƒ½æµ‹è¯•æ¨¡å—

æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
1. é¡µé¢åŠ è½½é€Ÿåº¦
2. å›¾è¡¨æ¸²æŸ“æ€§èƒ½
3. ç§»åŠ¨ç«¯å“åº”é€Ÿåº¦
4. å†…å­˜ä½¿ç”¨æƒ…å†µ

æµ‹è¯•æŒ‡æ ‡ï¼š
- é¡µé¢åŠ è½½: < 2s
- å›¾è¡¨æ¸²æŸ“: < 1s
- é¦–æ¬¡å†…å®¹ç»˜åˆ¶: < 1.5s
- äº¤äº’å‡†å¤‡æ—¶é—´: < 3s
"""

import time
import json
import statistics
from typing import Dict, List, Any
import subprocess
import sys
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebPerformanceTest:
    """ç½‘é¡µåº”ç”¨æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self, dashboard_path: str = "fashion-dashboard"):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.dashboard_path = Path(dashboard_path)
        self.test_results = {}
        self.lighthouse_metrics = {}
        
    def setup_dashboard(self):
        """è®¾ç½®å¹¶å¯åŠ¨ä»ªè¡¨æ¿åº”ç”¨"""
        logger.info("è®¾ç½®ä»ªè¡¨æ¿åº”ç”¨...")
        
        if not self.dashboard_path.exists():
            raise FileNotFoundError(f"ä»ªè¡¨æ¿ç›®å½•ä¸å­˜åœ¨: {self.dashboard_path}")
        
        # æ£€æŸ¥package.json
        package_json = self.dashboard_path / "package.json"
        if not package_json.exists():
            raise FileNotFoundError(f"package.jsonä¸å­˜åœ¨: {package_json}")
        
        logger.info("ä»ªè¡¨æ¿è®¾ç½®å®Œæˆ")
    
    def test_build_performance(self):
        """æµ‹è¯•æ„å»ºæ€§èƒ½"""
        logger.info("æµ‹è¯•æ„å»ºæ€§èƒ½...")
        
        start_time = time.time()
        
        try:
            # è¿è¡Œæ„å»ºå‘½ä»¤
            result = subprocess.run(
                ["npm", "run", "build"],
                cwd=self.dashboard_path,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            build_time = time.time() - start_time
            
            if result.returncode != 0:
                logger.error(f"æ„å»ºå¤±è´¥: {result.stderr}")
                return {
                    'success': False,
                    'build_time': build_time,
                    'error': result.stderr
                }
            
            # æ£€æŸ¥æ„å»ºç»“æœ
            dist_path = self.dashboard_path / "dist"
            if not dist_path.exists():
                return {
                    'success': False,
                    'build_time': build_time,
                    'error': "distç›®å½•æœªç”Ÿæˆ"
                }
            
            # è·å–æ„å»ºæ–‡ä»¶å¤§å°
            total_size = self._get_directory_size(dist_path)
            
            build_performance = {
                'success': True,
                'build_time': round(build_time, 2),
                'total_size_mb': round(total_size / 1024 / 1024, 2),
                'target_met': build_time < 60,  # æ„å»ºæ—¶é—´<60s
                'dist_files': len(list(dist_path.rglob("*"))) if dist_path.exists() else 0
            }
            
            logger.info(f"æ„å»ºå®Œæˆ: {build_time:.1f}s, å¤§å°: {total_size/1024/1024:.1f}MB")
            
        except subprocess.TimeoutExpired:
            build_performance = {
                'success': False,
                'build_time': 300,
                'error': "æ„å»ºè¶…æ—¶(5åˆ†é’Ÿ)"
            }
        except Exception as e:
            build_performance = {
                'success': False,
                'build_time': time.time() - start_time,
                'error': str(e)
            }
        
        self.test_results['build_performance'] = build_performance
        return build_performance
    
    def _get_directory_size(self, directory: Path) -> int:
        """è·å–ç›®å½•æ€»å¤§å°"""
        total_size = 0
        try:
            for file_path in directory.rglob('*'):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        except Exception as e:
            logger.warning(f"è®¡ç®—ç›®å½•å¤§å°æ—¶å‡ºé”™: {e}")
        return total_size
    
    def test_lighthouse_performance(self, url: str = "http://localhost:5173"):
        """ä½¿ç”¨Lighthouseæµ‹è¯•æ€§èƒ½æŒ‡æ ‡"""
        logger.info("è¿è¡ŒLighthouseæ€§èƒ½æµ‹è¯•...")
        
        lighthouse_script = """
        const puppeteer = require('puppeteer');
        const lighthouse = require('lighthouse');
        
        (async () => {
            const browser = await puppeteer.launch();
            const { lhr } = await lighthouse(url, {
                port: new URL(browser.wsEndpoint()).port,
                output: 'json',
                logLevel: 'info',
                onlyCategories: ['performance']
            });
            
            console.log(JSON.stringify(lhr));
            await browser.close();
        })();
        """
        
        try:
            # æ£€æŸ¥Lighthouseæ˜¯å¦å¯ç”¨
            result = subprocess.run(
                ["which", "lighthouse"],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                logger.warning("Lighthouseæœªå®‰è£…ï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")
                return {
                    'available': False,
                    'note': 'Lighthouseæœªå®‰è£…ï¼Œéœ€è¦: npm install -g lighthouse'
                }
            
            # è¿è¡ŒLighthouseæµ‹è¯•
            result = subprocess.run(
                ["lighthouse", url, "--output=json", "--quiet"],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode == 0:
                lhr_data = json.loads(result.stdout)
                lighthouse_metrics = self._extract_lighthouse_metrics(lhr_data)
            else:
                lighthouse_metrics = {
                    'available': False,
                    'error': result.stderr
                }
                
        except Exception as e:
            logger.warning(f"Lighthouseæµ‹è¯•å¤±è´¥: {e}")
            lighthouse_metrics = {
                'available': False,
                'error': str(e)
            }
        
        self.test_results['lighthouse_performance'] = lighthouse_metrics
        return lighthouse_metrics
    
    def _extract_lighthouse_metrics(self, lhr_data: Dict) -> Dict[str, Any]:
        """æå–Lighthouseå…³é”®æŒ‡æ ‡"""
        try:
            categories = lhr_data.get('categories', {})
            audits = lhr_data.get('audits', {})
            
            # æå–æ ¸å¿ƒWebæŒ‡æ ‡
            metrics = {
                'performance_score': round(categories.get('performance', {}).get('score', 0) * 100, 1),
                'first-contentful-paint': {
                    'displayValue': audits.get('first-contentful-paint', {}).get('displayValue', 'N/A'),
                    'numericValue': round(audits.get('first-contentful-paint', {}).get('numericValue', 0) / 1000, 2)
                },
                'largest-contentful-paint': {
                    'displayValue': audits.get('largest-contentful-paint', {}).get('displayValue', 'N/A'),
                    'numericValue': round(audits.get('largest-contentful-paint', {}).get('numericValue', 0) / 1000, 2)
                },
                'speed-index': {
                    'displayValue': audits.get('speed-index', {}).get('displayValue', 'N/A'),
                    'numericValue': round(audits.get('speed-index', {}).get('numericValue', 0) / 1000, 2)
                },
                'cumulative-layout-shift': {
                    'displayValue': audits.get('cumulative-layout-shift', {}).get('displayValue', 'N/A'),
                    'numericValue': round(audits.get('cumulative-layout-shift', {}).get('numericValue', 0), 3)
                },
                'total-blocking-time': {
                    'displayValue': audits.get('total-blocking-time', {}).get('displayValue', 'N/A'),
                    'numericValue': round(audits.get('total-blocking-time', {}).get('numericValue', 0) / 1000, 2)
                },
                'available': True
            }
            
            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦è¾¾æˆ
            metrics['targets_met'] = {
                'fcp_under_1_5s': metrics['first-contentful-paint']['numericValue'] < 1.5,
                'lcp_under_2_5s': metrics['largest-contentful-paint']['numericValue'] < 2.5,
                'cls_under_0_1': metrics['cumulative-layout-shift']['numericValue'] < 0.1,
                'tbt_under_200ms': metrics['total-blocking-time']['numericValue'] < 0.2
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"æå–LighthouseæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            return {
                'available': False,
                'error': str(e)
            }
    
    def test_bundle_analysis(self):
        """åˆ†ææ‰“åŒ…æ–‡ä»¶å¤§å°"""
        logger.info("åˆ†ææ‰“åŒ…æ–‡ä»¶...")
        
        dist_path = self.dashboard_path / "dist"
        if not dist_path.exists():
            return {
                'available': False,
                'error': 'distç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ„å»º'
            }
        
        bundle_analysis = {
            'available': True,
            'total_size_mb': 0,
            'js_files': [],
            'css_files': [],
            'asset_files': [],
            'largest_files': []
        }
        
        try:
            all_files = list(dist_path.rglob("*"))
            
            for file_path in all_files:
                if file_path.is_file():
                    size = file_path.stat().st_size
                    relative_path = file_path.relative_to(dist_path)
                    
                    file_info = {
                        'path': str(relative_path),
                        'size_kb': round(size / 1024, 2)
                    }
                    
                    if file_path.suffix == '.js':
                        bundle_analysis['js_files'].append(file_info)
                    elif file_path.suffix == '.css':
                        bundle_analysis['css_files'].append(file_info)
                    else:
                        bundle_analysis['asset_files'].append(file_info)
            
            # è®¡ç®—æ€»å¤§å°
            bundle_analysis['total_size_mb'] = round(
                sum(f['size_kb'] for f in 
                    bundle_analysis['js_files'] + 
                    bundle_analysis['css_files'] + 
                    bundle_analysis['asset_files']) / 1024, 2
            )
            
            # æ‰¾å‡ºæœ€å¤§çš„æ–‡ä»¶
            all_files_sorted = sorted(
                bundle_analysis['js_files'] + bundle_analysis['css_files'] + bundle_analysis['asset_files'],
                key=lambda x: x['size_kb'],
                reverse=True
            )[:10]
            
            bundle_analysis['largest_files'] = all_files_sorted
            
            # æ£€æŸ¥ç›®æ ‡ (JSæ–‡ä»¶ < 500KB, CSSæ–‡ä»¶ < 100KB)
            js_sizes = [f['size_kb'] for f in bundle_analysis['js_files']]
            css_sizes = [f['size_kb'] for f in bundle_analysis['css_files']]
            
            bundle_analysis['targets_met'] = {
                'js_under_500kb': all(size < 500 for size in js_sizes) if js_sizes else True,
                'css_under_100kb': all(size < 100 for size in css_sizes) if css_sizes else True,
                'total_under_2mb': bundle_analysis['total_size_mb'] < 2
            }
            
            logger.info(f"æ‰“åŒ…åˆ†æå®Œæˆ: æ€»å¤§å° {bundle_analysis['total_size_mb']}MB")
            
        except Exception as e:
            bundle_analysis = {
                'available': False,
                'error': str(e)
            }
        
        self.test_results['bundle_analysis'] = bundle_analysis
        return bundle_analysis
    
    def test_react_component_performance(self):
        """æµ‹è¯•Reactç»„ä»¶æ€§èƒ½"""
        logger.info("æµ‹è¯•Reactç»„ä»¶æ€§èƒ½...")
        
        # åˆ†æä¸»è¦ç»„ä»¶æ–‡ä»¶
        src_path = self.dashboard_path / "src"
        if not src_path.exists():
            return {
                'available': False,
                'error': 'srcç›®å½•ä¸å­˜åœ¨'
            }
        
        component_analysis = {
            'available': True,
            'total_components': 0,
            'components': [],
            'potential_issues': []
        }
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰Reactç»„ä»¶
            component_files = list(src_path.rglob("*.tsx")) + list(src_path.rglob("*.jsx"))
            component_analysis['total_components'] = len(component_files)
            
            for component_file in component_files:
                try:
                    with open(component_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç®€å•åˆ†æç»„ä»¶ç‰¹å¾
                    analysis = self._analyze_component_file(component_file, content)
                    component_analysis['components'].append(analysis)
                    
                except Exception as e:
                    logger.warning(f"åˆ†æç»„ä»¶æ–‡ä»¶ {component_file} æ—¶å‡ºé”™: {e}")
            
            # è¯†åˆ«æ½œåœ¨é—®é¢˜
            issues = self._identify_performance_issues(component_analysis['components'])
            component_analysis['potential_issues'] = issues
            
            component_analysis['targets_met'] = {
                'no_large_components': all(c['lines'] < 500 for c in component_analysis['components']),
                'no_deep_nesting': all(c['max_depth'] < 10 for c in component_analysis['components']),
                'reasonable_imports': all(c['import_count'] < 20 for c in component_analysis['components'])
            }
            
            logger.info(f"ç»„ä»¶åˆ†æå®Œæˆ: {component_analysis['total_components']} ä¸ªç»„ä»¶")
            
        except Exception as e:
            component_analysis = {
                'available': False,
                'error': str(e)
            }
        
        self.test_results['react_component_performance'] = component_analysis
        return component_analysis
    
    def _analyze_component_file(self, file_path: Path, content: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªç»„ä»¶æ–‡ä»¶"""
        lines = content.split('\n')
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        line_count = len(lines)
        
        # ç®€å•ç»Ÿè®¡
        jsx_elements = content.count('<') + content.count('</')
        useState_hooks = content.count('useState')
        useEffect_hooks = content.count('useEffect')
        useMemo_hooks = content.count('useMemo')
        useCallback_hooks = content.count('useCallback')
        
        # å¯¼å…¥ç»Ÿè®¡
        import_lines = [line for line in lines if line.strip().startswith('import')]
        import_count = len(import_lines)
        
        # æœ€å¤§åµŒå¥—æ·±åº¦ä¼°è®¡
        max_depth = 0
        current_depth = 0
        for line in lines:
            current_depth += line.count('<') - line.count('</>')
            max_depth = max(max_depth, current_depth)
        
        return {
            'name': file_path.stem,
            'path': str(file_path.relative_to(src_path)),
            'lines': line_count,
            'jsx_elements': jsx_elements,
            'hooks': {
                'useState': useState_hooks,
                'useEffect': useEffect_hooks,
                'useMemo': useMemo_hooks,
                'useCallback': useCallback_hooks
            },
            'import_count': import_count,
            'max_depth': max_depth,
            'complexity_score': self._calculate_complexity_score(line_count, jsx_elements, useState_hooks, useEffect_hooks)
        }
    
    def _calculate_complexity_score(self, lines: int, jsx_elements: int, use_state: int, use_effect: int) -> int:
        """è®¡ç®—ç»„ä»¶å¤æ‚åº¦åˆ†æ•°"""
        # ç®€åŒ–çš„å¤æ‚åº¦è®¡ç®—
        return lines // 10 + jsx_elements // 5 + use_state + use_effect * 2
    
    def _identify_performance_issues(self, components: List[Dict]) -> List[str]:
        """è¯†åˆ«æ€§èƒ½é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥å¤§æ–‡ä»¶
        large_components = [c for c in components if c['lines'] > 300]
        if large_components:
            issues.append(f"å‘ç° {len(large_components)} ä¸ªå¤§æ–‡ä»¶ç»„ä»¶ (>300è¡Œ)")
        
        # æ£€æŸ¥æ·±åº¦åµŒå¥—
        deep_components = [c for c in components if c['max_depth'] > 8]
        if deep_components:
            issues.append(f"å‘ç° {len(deep_components)} ä¸ªæ·±åº¦åµŒå¥—ç»„ä»¶ (>8å±‚)")
        
        # æ£€æŸ¥è¿‡å¤šå¯¼å…¥
        heavy_imports = [c for c in components if c['import_count'] > 15]
        if heavy_imports:
            issues.append(f"å‘ç° {len(heavy_imports)} ä¸ªå¯¼å…¥è¿‡å¤šçš„ç»„ä»¶ (>15ä¸ªå¯¼å…¥)")
        
        # æ£€æŸ¥è¿‡åº¦ä½¿ç”¨useEffect
        heavy_effects = [c for c in components if c['hooks']['useEffect'] > 5]
        if heavy_effects:
            issues.append(f"å‘ç° {len(heavy_effects)} ä¸ªè¿‡åº¦ä½¿ç”¨useEffectçš„ç»„ä»¶ (>5ä¸ª)")
        
        return issues
    
    def test_mobile_responsiveness(self):
        """æµ‹è¯•ç§»åŠ¨ç«¯å“åº”æ€§"""
        logger.info("æµ‹è¯•ç§»åŠ¨ç«¯å“åº”æ€§...")
        
        # æ£€æŸ¥å“åº”å¼è®¾è®¡å®ç°
        src_path = self.dashboard_path / "src"
        if not src_path.exists():
            return {
                'available': False,
                'error': 'srcç›®å½•ä¸å­˜åœ¨'
            }
        
        responsiveness_analysis = {
            'available': True,
            'has_responsive_hooks': False,
            'has_media_queries': False,
            'has_mobile_components': False,
            'touch_friendly_elements': 0,
            'mobile_specific_files': []
        }
        
        try:
            # æ£€æŸ¥hookå®ç°
            hooks_path = src_path / "hooks"
            if hooks_path.exists():
                use_mobile_file = hooks_path / "use-mobile.tsx"
                if use_mobile_file.exists():
                    responsiveness_analysis['has_responsive_hooks'] = True
                    
                    with open(use_mobile_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # æ£€æŸ¥ç§»åŠ¨ç«¯æ£€æµ‹é€»è¾‘
                        if 'window.innerWidth' in content or 'ResizeObserver' in content:
                            responsiveness_analysis['mobile_detection'] = True
            
            # æ£€æŸ¥CSSåª’ä½“æŸ¥è¯¢
            css_files = list(src_path.rglob("*.css")) + list(self.dashboard_path.rglob("*.css"))
            media_queries_count = 0
            
            for css_file in css_files:
                try:
                    with open(css_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        media_queries_count += content.count('@media')
                except:
                    continue
            
            responsiveness_analysis['has_media_queries'] = media_queries_count > 0
            responsiveness_analysis['media_queries_count'] = media_queries_count
            
            # æ£€æŸ¥ç§»åŠ¨ç«¯ç‰¹å®šç»„ä»¶
            mobile_files = [f for f in src_path.rglob("*") 
                           if f.is_file() and 'mobile' in f.name.lower()]
            responsiveness_analysis['mobile_specific_files'] = [str(f) for f in mobile_files]
            responsiveness_analysis['has_mobile_components'] = len(mobile_files) > 0
            
            # æ£€æŸ¥ç§»åŠ¨ç«¯å‹å¥½å…ƒç´ 
            tsx_files = list(src_path.rglob("*.tsx"))
            touch_friendly_count = 0
            
            for tsx_file in tsx_files:
                try:
                    with open(tsx_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # æ£€æŸ¥å¸¸è§ç§»åŠ¨ç«¯äº¤äº’å…ƒç´ 
                        touch_friendly_count += content.count('button')
                        touch_friendly_count += content.count('onClick')
                        touch_friendly_count += content.count('touch')
                except:
                    continue
            
            responsiveness_analysis['touch_friendly_elements'] = touch_friendly_count
            
            # æ£€æŸ¥ç›®æ ‡è¾¾æˆ
            responsiveness_analysis['targets_met'] = {
                'has_responsive_hooks': responsiveness_analysis['has_responsive_hooks'],
                'has_media_queries': responsiveness_analysis['has_media_queries'],
                'has_mobile_optimization': (
                    responsiveness_analysis['has_responsive_hooks'] or 
                    responsiveness_analysis['has_media_queries']
                )
            }
            
            logger.info(f"å“åº”å¼åˆ†æå®Œæˆ: åª’ä½“æŸ¥è¯¢ {media_queries_count} ä¸ª")
            
        except Exception as e:
            responsiveness_analysis = {
                'available': False,
                'error': str(e)
            }
        
        self.test_results['mobile_responsiveness'] = responsiveness_analysis
        return responsiveness_analysis
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰ç½‘é¡µæ€§èƒ½æµ‹è¯•"""
        logger.info("å¼€å§‹ç½‘é¡µåº”ç”¨æ€§èƒ½æµ‹è¯•...")
        
        try:
            # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
            self.setup_dashboard()
            
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.test_build_performance()
            self.test_bundle_analysis()
            self.test_react_component_performance()
            self.test_mobile_responsiveness()
            
            # Lighthouseæµ‹è¯• (å¯é€‰)
            self.test_lighthouse_performance()
            
            logger.info("ç½‘é¡µåº”ç”¨æ€§èƒ½æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        return {
            'test_type': 'web_performance',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'framework': 'React + Vite',
            'test_results': self.test_results,
            'summary': self._generate_summary()
        }
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'overall_status': 'unknown',
            'performance_score': 0
        }
        
        # æ£€æŸ¥å„é¡¹æµ‹è¯•æ˜¯å¦è¾¾æ ‡
        test_checks = {
            'build_performance': lambda r: r.get('target_met', False),
            'bundle_analysis': lambda r: r.get('targets_met', {}).get('total_under_2mb', False),
            'react_component_performance': lambda r: r.get('targets_met', {}).get('no_large_components', False),
            'mobile_responsiveness': lambda r: r.get('targets_met', {}).get('has_mobile_optimization', False),
            'lighthouse_performance': lambda r: r.get('performance_score', 0) >= 80
        }
        
        for test_name, check_func in test_checks.items():
            if test_name in self.test_results:
                summary['total_tests'] += 1
                try:
                    if check_func(self.test_results[test_name]):
                        summary['passed_tests'] += 1
                    else:
                        summary['failed_tests'] += 1
                except Exception:
                    summary['failed_tests'] += 1
        
        # è®¡ç®—æ€§èƒ½åˆ†æ•°
        if summary['total_tests'] > 0:
            summary['performance_score'] = round((summary['passed_tests'] / summary['total_tests']) * 100, 1)
        
        if summary['passed_tests'] == summary['total_tests']:
            summary['overall_status'] = 'passed'
        elif summary['passed_tests'] > 0:
            summary['overall_status'] = 'partial'
        else:
            summary['overall_status'] = 'failed'
        
        return summary


def run_web_performance_tests():
    """è¿è¡Œç½‘é¡µæ€§èƒ½æµ‹è¯•çš„ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç½‘é¡µåº”ç”¨æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    tester = WebPerformanceTest()
    
    try:
        # è¿è¡Œæµ‹è¯•
        tester.run_all_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_report()
        
        # è¾“å‡ºç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        summary = report['summary']
        print(f"   æ€§èƒ½åˆ†æ•°: {summary['performance_score']}/100")
        print(f"   æ€»æµ‹è¯•é¡¹: {summary['total_tests']}")
        print(f"   é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"   æ•´ä½“çŠ¶æ€: {summary['overall_status']}")
        
        # è¯¦ç»†ç»“æœ
        print("\nğŸ“ˆ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, results in report['test_results'].items():
            print(f"\n{test_name}:")
            if isinstance(results, dict):
                for key, value in results.items():
                    if key in ['target_met', 'targets_met'] or isinstance(value, bool):
                        status = "âœ…" if value else "âŒ"
                        print(f"   {status} {key}: {value}")
                    else:
                        print(f"   {key}: {value}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = Path("tests/web_performance_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    run_web_performance_tests()