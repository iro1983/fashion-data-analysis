#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿå‹åŠ›æµ‹è¯•æ¨¡å—

æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
1. è¿ç»­24å°æ—¶è¿è¡Œç¨³å®šæ€§æµ‹è¯•
2. é«˜å¹¶å‘è´Ÿè½½æµ‹è¯•
3. æ•°æ®åº“å‹åŠ›æµ‹è¯•
4. å†…å­˜æ³„æ¼æ£€æµ‹

æµ‹è¯•æŒ‡æ ‡ï¼š
- ç³»ç»Ÿç¨³å®šæ€§: 99%+
- å†…å­˜æ³„æ¼: < 5MB/hour
- å“åº”æ—¶é—´é€€åŒ–: < 10%
- é”™è¯¯ç‡: < 1%
"""

import time
import json
import threading
import statistics
import gc
import psutil
import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Callable
from pathlib import Path
import logging
import signal
import sys
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class LoadTestMetrics:
    """å‹åŠ›æµ‹è¯•æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: float
    active_threads: int
    memory_usage_mb: float
    cpu_percent: float
    response_time: float
    success_count: int
    error_count: int
    database_connections: int
    queue_size: int

class SystemStressTest:
    """ç³»ç»Ÿå‹åŠ›æµ‹è¯•ç±»"""
    
    def __init__(self, test_db_path: str = "tests/stress_test.db"):
        """åˆå§‹åŒ–å‹åŠ›æµ‹è¯•"""
        self.test_db_path = Path(test_db_path)
        self.test_db_path.parent.mkdir(exist_ok=True)
        
        self.test_results = {}
        self.metrics_log = []
        self.stop_flag = False
        self.lock = threading.Lock()
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        self._init_database()
        
        # æ€§èƒ½ç›‘æ§
        self.process = psutil.Process()
        
    def _init_database(self):
        """åˆå§‹åŒ–æµ‹è¯•æ•°æ®åº“"""
        try:
            # åˆ é™¤ç°æœ‰æµ‹è¯•æ•°æ®åº“
            if self.test_db_path.exists():
                self.test_db_path.unlink()
            
            # åˆ›å»ºæ–°çš„æµ‹è¯•æ•°æ®åº“
            conn = sqlite3.connect(str(self.test_db_path))
            cursor = conn.cursor()
            
            # åˆ›å»ºæµ‹è¯•è¡¨
            cursor.execute("""
                CREATE TABLE stress_test_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    thread_id INTEGER,
                    operation_type TEXT,
                    data_size INTEGER,
                    response_time REAL,
                    timestamp REAL,
                    success BOOLEAN
                )
            """)
            
            cursor.execute("CREATE INDEX idx_timestamp ON stress_test_data(timestamp)")
            cursor.execute("CREATE INDEX idx_thread ON stress_test_data(thread_id)")
            
            conn.commit()
            conn.close()
            
            logger.info("æµ‹è¯•æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def stress_test_database_operations(self, duration_hours: int = 2, max_threads: int = 20):
        """æ•°æ®åº“æ“ä½œå‹åŠ›æµ‹è¯•"""
        logger.info(f"å¼€å§‹æ•°æ®åº“å‹åŠ›æµ‹è¯• (æŒç»­ {duration_hours} å°æ—¶, æœ€å¤š {max_threads} çº¿ç¨‹)...")
        
        test_start_time = time.time()
        end_time = test_start_time + (duration_hours * 3600)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_operations': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'response_times': [],
            'threads_created': 0,
            'errors': []
        }
        
        def database_worker(worker_id: int, stop_flag_ref):
            """æ•°æ®åº“æ“ä½œå·¥ä½œçº¿ç¨‹"""
            operation_count = 0
            worker_start_time = time.time()
            
            while not stop_flag_ref() and time.time() < end_time:
                try:
                    operation_start = time.time()
                    
                    # æ‰§è¡Œæ•°æ®åº“æ“ä½œ
                    success = self._perform_database_operation(worker_id, operation_count)
                    
                    operation_end = time.time()
                    response_time = operation_end - operation_start
                    
                    # è®°å½•æ“ä½œç»“æœ
                    with self.lock:
                        stats['total_operations'] += 1
                        stats['response_times'].append(response_time)
                        
                        if success:
                            stats['successful_operations'] += 1
                        else:
                            stats['failed_operations'] += 1
                            stats['errors'].append(f"Worker {worker_id} operation {operation_count} failed")
                    
                    operation_count += 1
                    
                    # çŸ­æš‚ä¼‘æ¯
                    time.sleep(random.uniform(0.01, 0.1))
                    
                except Exception as e:
                    with self.lock:
                        stats['failed_operations'] += 1
                        stats['errors'].append(f"Worker {worker_id} error: {str(e)}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                if stop_flag_ref():
                    break
            
            return operation_count
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        threads = []
        thread_count = 0
        
        while time.time() < end_time:
            if thread_count < max_threads:
                # åˆ›å»ºæ–°çº¿ç¨‹
                thread = threading.Thread(
                    target=database_worker,
                    args=(thread_count, lambda: self.stop_flag),
                    daemon=True
                )
                thread.start()
                threads.append(thread)
                
                with self.lock:
                    stats['threads_created'] += 1
                
                thread_count += 1
                
                # éšæœºå»¶è¿Ÿå¯åŠ¨
                time.sleep(random.uniform(0.1, 1.0))
            
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰çº¿ç¨‹æ­»äº¡
                alive_threads = [t for t in threads if t.is_alive()]
                if len(alive_threads) < max_threads * 0.8:  # å¦‚æœå­˜æ´»çº¿ç¨‹å°‘äº80%ï¼Œåˆ›å»ºæ–°çº¿ç¨‹
                    time.sleep(1)
                    continue
                else:
                    time.sleep(5)  # å‡å°‘æ£€æŸ¥é¢‘ç‡
            
            # æ£€æŸ¥æ•´ä½“æµ‹è¯•æ—¶é—´
            if time.time() >= end_time:
                self.stop_flag = True
                break
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join(timeout=5)
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        test_duration = time.time() - test_start_time
        
        response_times = stats['response_times']
        avg_response_time = statistics.mean(response_times) if response_times else 0
        p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times) if response_times else 0
        
        success_rate = (stats['successful_operations'] / stats['total_operations'] * 100) if stats['total_operations'] > 0 else 0
        
        database_stress_result = {
            'test_duration_hours': round(test_duration / 3600, 2),
            'total_operations': stats['total_operations'],
            'successful_operations': stats['successful_operations'],
            'failed_operations': stats['failed_operations'],
            'success_rate': round(success_rate, 2),
            'avg_response_time': round(avg_response_time, 4),
            'p95_response_time': round(p95_response_time, 4),
            'max_response_time': round(max(response_times), 4) if response_times else 0,
            'threads_created': stats['threads_created'],
            'operations_per_second': round(stats['total_operations'] / test_duration, 2),
            'target_met': success_rate >= 99 and avg_response_time < 0.1,
            'errors_sample': stats['errors'][:10]  # åªä¿ç•™å‰10ä¸ªé”™è¯¯
        }
        
        logger.info(f"æ•°æ®åº“å‹åŠ›æµ‹è¯•å®Œæˆ: {stats['total_operations']} æ¬¡æ“ä½œ, æˆåŠŸç‡ {success_rate:.1f}%")
        
        self.test_results['database_stress'] = database_stress_result
        return database_stress_result
    
    def _perform_database_operation(self, thread_id: int, operation_count: int) -> bool:
        """æ‰§è¡Œå•ä¸ªæ•°æ®åº“æ“ä½œ"""
        try:
            conn = sqlite3.connect(str(self.test_db_path), timeout=5)
            cursor = conn.cursor()
            
            # éšæœºé€‰æ‹©æ“ä½œç±»å‹
            operation_type = random.choice(['INSERT', 'SELECT', 'UPDATE', 'DELETE'])
            
            if operation_type == 'INSERT':
                cursor.execute("""
                    INSERT INTO stress_test_data 
                    (thread_id, operation_type, data_size, response_time, timestamp, success)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    thread_id,
                    'INSERT',
                    random.randint(100, 10000),
                    random.uniform(0.001, 0.01),
                    time.time(),
                    True
                ))
            
            elif operation_type == 'SELECT':
                cursor.execute("""
                    SELECT COUNT(*) FROM stress_test_data 
                    WHERE thread_id = ? AND timestamp > ?
                """, (thread_id, time.time() - 3600))
                result = cursor.fetchone()
            
            elif operation_type == 'UPDATE':
                cursor.execute("""
                    UPDATE stress_test_data 
                    SET response_time = ? 
                    WHERE thread_id = ? AND id = (
                        SELECT id FROM stress_test_data 
                        WHERE thread_id = ? 
                        ORDER BY timestamp DESC 
                        LIMIT 1
                    )
                """, (random.uniform(0.001, 0.01), thread_id, thread_id))
            
            elif operation_type == 'DELETE':
                cursor.execute("""
                    DELETE FROM stress_test_data 
                    WHERE thread_id = ? AND timestamp < ?
                """, (thread_id, time.time() - 7200))  # åˆ é™¤2å°æ—¶å‰çš„è®°å½•
            
            conn.commit()
            conn.close()
            
            return True
            
        except Exception as e:
            logger.debug(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            return False
    
    def memory_leak_detection(self, duration_hours: int = 2):
        """å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•"""
        logger.info(f"å¼€å§‹å†…å­˜æ³„æ¼æ£€æµ‹ (æŒç»­ {duration_hours} å°æ—¶)...")
        
        test_start_time = time.time()
        end_time = test_start_time + (duration_hours * 3600)
        
        # è®°å½•å†…å­˜ä½¿ç”¨å†å²
        memory_history = []
        gc_history = []
        
        def memory_monitor(stop_flag_ref):
            """å†…å­˜ç›‘æ§çº¿ç¨‹"""
            while not stop_flag_ref() and time.time() < end_time:
                # è·å–å½“å‰å†…å­˜ä½¿ç”¨
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                gc_count = len(gc.get_objects())
                
                # è®°å½•æŒ‡æ ‡
                metric = LoadTestMetrics(
                    timestamp=time.time(),
                    active_threads=threading.active_count(),
                    memory_usage_mb=memory_mb,
                    cpu_percent=self.process.cpu_percent(),
                    response_time=0,  # ç›‘æ§çº¿ç¨‹ä¸æµ‹é‡å“åº”æ—¶é—´
                    success_count=0,
                    error_count=0,
                    database_connections=0,
                    queue_size=0
                )
                
                with self.lock:
                    self.metrics_log.append(metric)
                    memory_history.append(memory_mb)
                    gc_history.append(gc_count)
                
                # é™åˆ¶æ—¥å¿—å¤§å°
                if len(self.metrics_log) > 1000:
                    with self.lock:
                        self.metrics_log = self.metrics_log[-500:]
                
                time.sleep(30)  # æ¯30ç§’è®°å½•ä¸€æ¬¡
        
        # å¯åŠ¨å†…å­˜ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(
            target=memory_monitor,
            args=(lambda: self.stop_flag,),
            daemon=True
        )
        monitor_thread.start()
        
        # æ‰§è¡Œå‹åŠ›æ“ä½œ
        self._run_memory_stress_operations(end_time)
        
        # åœæ­¢ç›‘æ§
        self.stop_flag = True
        monitor_thread.join()
        
        # åˆ†æå†…å­˜ä½¿ç”¨è¶‹åŠ¿
        if len(memory_history) >= 2:
            initial_memory = memory_history[0]
            final_memory = memory_history[-1]
            memory_increase = final_memory - initial_memory
            memory_leak_rate = memory_increase / (duration_hours * 3600 / 30)  # MB per check interval
            
            # çº¿æ€§å›å½’åˆ†æå†…å­˜è¶‹åŠ¿
            memory_trend = self._calculate_trend(memory_history)
        else:
            memory_increase = 0
            memory_leak_rate = 0
            memory_trend = 0
        
        memory_leak_result = {
            'test_duration_hours': duration_hours,
            'initial_memory_mb': round(memory_history[0], 2) if memory_history else 0,
            'final_memory_mb': round(memory_history[-1], 2) if memory_history else 0,
            'memory_increase_mb': round(memory_increase, 2),
            'memory_leak_rate_mb_per_hour': round(memory_leak_rate * 30, 2),  # è½¬æ¢ä¸ºæ¯å°æ—¶
            'memory_trend_slope': round(memory_trend, 6),
            'peak_memory_mb': round(max(memory_history), 2) if memory_history else 0,
            'min_memory_mb': round(min(memory_history), 2) if memory_history else 0,
            'memory_stability': 'stable' if abs(memory_trend) < 0.001 else 'increasing' if memory_trend > 0 else 'decreasing',
            'target_met': memory_leak_rate < 0.001  # å†…å­˜æ³„æ¼ < 0.001MB per check (~5MB/hour)
        }
        
        logger.info(f"å†…å­˜æ³„æ¼æ£€æµ‹å®Œæˆ: å†…å­˜å¢é•¿ {memory_increase:.1f}MB, è¶‹åŠ¿ {memory_trend:.6f}")
        
        self.test_results['memory_leak_detection'] = memory_leak_result
        return memory_leak_result
    
    def _run_memory_stress_operations(self, end_time: float):
        """è¿è¡Œå†…å­˜å‹åŠ›æ“ä½œ"""
        def memory_stress_worker():
            """å†…å­˜å‹åŠ›å·¥ä½œçº¿ç¨‹"""
            large_objects = []
            
            while not self.stop_flag and time.time() < end_time:
                try:
                    # åˆ›å»ºå¤§é‡å¯¹è±¡
                    for _ in range(100):
                        large_objects.append({
                            'data': [random.random() for _ in range(1000)],
                            'timestamp': time.time(),
                            'id': random.randint(1, 1000000)
                        })
                    
                    # å®šæœŸæ¸…ç†
                    if len(large_objects) > 1000:
                        large_objects = large_objects[-500:]  # ä¿ç•™ååŠéƒ¨åˆ†
                    
                    time.sleep(0.1)
                    
                except Exception as e:
                    logger.debug(f"å†…å­˜å‹åŠ›æ“ä½œå‡ºé”™: {e}")
        
        # å¯åŠ¨å¤šä¸ªå†…å­˜å‹åŠ›çº¿ç¨‹
        threads = []
        for i in range(5):
            thread = threading.Thread(target=memory_stress_worker, daemon=True)
            thread.start()
            threads.append(thread)
        
        # ç­‰å¾…ç»“æŸæ—¶é—´
        while time.time() < end_time and not self.stop_flag:
            time.sleep(1)
        
        # æ¸…ç†çº¿ç¨‹
        for thread in threads:
            thread.join(timeout=1)
    
    def _calculate_trend(self, values: List[float]) -> float:
        """è®¡ç®—æ•°å€¼åºåˆ—çš„è¶‹åŠ¿ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰"""
        if len(values) < 2:
            return 0
        
        n = len(values)
        x = list(range(n))
        
        # è®¡ç®—çº¿æ€§å›å½’
        sum_x = sum(x)
        sum_y = sum(values)
        sum_xy = sum(x[i] * values[i] for i in range(n))
        sum_x2 = sum(xi * xi for xi in x)
        
        if n * sum_x2 - sum_x * sum_x == 0:
            return 0
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        return slope
    
    def concurrent_user_simulation(self, duration_hours: int = 1, max_concurrent_users: int = 50):
        """å¹¶å‘ç”¨æˆ·æ¨¡æ‹Ÿæµ‹è¯•"""
        logger.info(f"å¼€å§‹å¹¶å‘ç”¨æˆ·æ¨¡æ‹Ÿ (æŒç»­ {duration_hours} å°æ—¶, æœ€å¤š {max_concurrent_users} ç”¨æˆ·)...")
        
        test_start_time = time.time()
        end_time = test_start_time + (duration_hours * 3600)
        
        # ç”¨æˆ·æ¨¡æ‹Ÿç»Ÿè®¡
        user_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'response_times': [],
            'concurrent_users': 0,
            'peak_concurrent_users': 0
        }
        
        def simulate_user_session(user_id: int, stop_flag_ref):
            """æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯"""
            session_start = time.time()
            
            while not stop_flag_ref() and time.time() < end_time:
                try:
                    # æ¨¡æ‹Ÿç”¨æˆ·æ“ä½œ
                    operation_start = time.time()
                    
                    # éšæœºé€‰æ‹©æ“ä½œç±»å‹
                    operation = random.choice(['browse', 'search', 'filter', 'details'])
                    
                    # æ¨¡æ‹Ÿä¸åŒæ“ä½œçš„å“åº”æ—¶é—´
                    if operation == 'browse':
                        time.sleep(random.uniform(0.5, 2.0))  # æµè§ˆé¡µé¢
                    elif operation == 'search':
                        time.sleep(random.uniform(1.0, 3.0))  # æœç´¢
                    elif operation == 'filter':
                        time.sleep(random.uniform(0.2, 1.0))  # ç­›é€‰
                    elif operation == 'details':
                        time.sleep(random.uniform(2.0, 5.0))  # æŸ¥çœ‹è¯¦æƒ…
                    
                    operation_end = time.time()
                    response_time = operation_end - operation_start
                    
                    # è®°å½•è¯·æ±‚ç»“æœ
                    success = random.random() > 0.02  # 98%æˆåŠŸç‡
                    
                    with self.lock:
                        user_stats['total_requests'] += 1
                        user_stats['response_times'].append(response_time)
                        user_stats['concurrent_users'] = threading.active_count() - 1  # å‡å»ç›‘æ§çº¿ç¨‹
                        user_stats['peak_concurrent_users'] = max(user_stats['peak_concurrent_users'], user_stats['concurrent_users'])
                        
                        if success:
                            user_stats['successful_requests'] += 1
                        else:
                            user_stats['failed_requests'] += 1
                    
                    # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
                    think_time = random.uniform(1.0, 5.0)
                    time.sleep(think_time)
                    
                except Exception as e:
                    with self.lock:
                        user_stats['failed_requests'] += 1
                
                # æ£€æŸ¥ä¼šè¯æ˜¯å¦åº”è¯¥ç»“æŸ
                session_duration = time.time() - session_start
                if session_duration > random.uniform(300, 1800):  # 5-30åˆ†é’Ÿçš„ä¼šè¯
                    break
        
        # å¯åŠ¨ç”¨æˆ·ä¼šè¯
        active_sessions = []
        session_id = 0
        
        while time.time() < end_time and not self.stop_flag:
            # åŠ¨æ€è°ƒæ•´å¹¶å‘ç”¨æˆ·æ•°
            target_users = min(
                max_concurrent_users,
                max(10, int((time.time() - test_start_time) / 300) * 5)  # æ¯5åˆ†é’Ÿå¢åŠ 5ä¸ªç”¨æˆ·
            )
            
            # åˆ›å»ºæ–°ä¼šè¯
            while len(active_sessions) < target_users and time.time() < end_time:
                session = threading.Thread(
                    target=simulate_user_session,
                    args=(session_id, lambda: self.stop_flag),
                    daemon=True
                )
                session.start()
                active_sessions.append(session)
                session_id += 1
                
                time.sleep(random.uniform(0.1, 1.0))  # éšæœºå»¶è¿Ÿå¯åŠ¨
            
            # æ¸…ç†å·²å®Œæˆä¼šè¯
            active_sessions = [s for s in active_sessions if s.is_alive()]
            
            time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
        
        # ç­‰å¾…æ‰€æœ‰ä¼šè¯å®Œæˆ
        for session in active_sessions:
            session.join(timeout=5)
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        test_duration = time.time() - test_start_time
        response_times = user_stats['response_times']
        
        concurrent_user_result = {
            'test_duration_hours': round(test_duration / 3600, 2),
            'total_requests': user_stats['total_requests'],
            'successful_requests': user_stats['successful_requests'],
            'failed_requests': user_stats['failed_requests'],
            'success_rate': round((user_stats['successful_requests'] / user_stats['total_requests'] * 100), 2) if user_stats['total_requests'] > 0 else 0,
            'peak_concurrent_users': user_stats['peak_concurrent_users'],
            'avg_response_time': round(statistics.mean(response_times), 2) if response_times else 0,
            'p95_response_time': round(statistics.quantiles(response_times, n=20)[18], 2) if len(response_times) > 20 else 0,
            'max_response_time': round(max(response_times), 2) if response_times else 0,
            'requests_per_second': round(user_stats['total_requests'] / test_duration, 2),
            'target_met': user_stats['peak_concurrent_users'] >= max_concurrent_users * 0.8
        }
        
        logger.info(f"å¹¶å‘ç”¨æˆ·æµ‹è¯•å®Œæˆ: {user_stats['peak_concurrent_users']} å³°å€¼ç”¨æˆ·, æˆåŠŸç‡ {concurrent_user_result['success_rate']:.1f}%")
        
        self.test_results['concurrent_user_simulation'] = concurrent_user_result
        return concurrent_user_result
    
    def long_running_stability_test(self, duration_hours: int = 24):
        """é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•"""
        logger.info(f"å¼€å§‹é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯• (æŒç»­ {duration_hours} å°æ—¶)...")
        
        test_start_time = time.time()
        end_time = test_start_time + (duration_hours * 3600)
        
        stability_stats = {
            'test_start_time': test_start_time,
            'operations_per_hour': [],
            'error_rates': [],
            'performance_degradation': [],
            'system_errors': [],
            'resource_warnings': []
        }
        
        def stability_monitor(stop_flag_ref):
            """ç¨³å®šæ€§ç›‘æ§çº¿ç¨‹"""
            last_check_time = time.time()
            hourly_stats = []
            
            while not stop_flag_ref() and time.time() < end_time:
                current_time = time.time()
                
                # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
                if current_time - last_check_time >= 3600:
                    hour_duration = current_time - last_check_time
                    
                    # è®¡ç®—æ¯å°æ—¶ç»Ÿè®¡
                    with self.lock:
                        metrics_in_hour = [m for m in self.metrics_log if m.timestamp >= last_check_time]
                        
                        if metrics_in_hour:
                            avg_response_time = statistics.mean([m.response_time for m in metrics_in_hour if m.response_time > 0])
                            total_requests = len(metrics_in_hour)
                            error_count = sum(1 for m in metrics_in_hour if m.error_count > 0)
                            error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
                            
                            hourly_stats.append({
                                'hour': len(stability_stats['operations_per_hour']) + 1,
                                'total_requests': total_requests,
                                'avg_response_time': avg_response_time,
                                'error_rate': error_rate,
                                'timestamp': current_time
                            })
                    
                    stability_stats['operations_per_hour'].append(total_requests)
                    stability_stats['error_rates'].append(error_rate)
                    
                    last_check_time = current_time
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        
        # å¯åŠ¨ç¨³å®šæ€§ç›‘æ§
        monitor_thread = threading.Thread(
            target=stability_monitor,
            args=(lambda: self.stop_flag,),
            daemon=True
        )
        monitor_thread.start()
        
        # è¿è¡ŒåŸºç¡€è´Ÿè½½
        baseline_start = time.time()
        while time.time() < end_time and not self.stop_flag:
            # æ‰§è¡ŒåŸºç¡€æ“ä½œä¿æŒç³»ç»Ÿæ´»è·ƒ
            try:
                self._perform_database_operation(0, 0)
                time.sleep(1)
            except:
                pass
        
        # åœæ­¢ç›‘æ§
        self.stop_flag = True
        monitor_thread.join()
        
        # åˆ†æç¨³å®šæ€§æ•°æ®
        if stability_stats['operations_per_hour']:
            avg_ops_per_hour = statistics.mean(stability_stats['operations_per_hour'])
            max_hourly_ops = max(stability_stats['operations_per_hour'])
            min_hourly_ops = min(stability_stats['operations_per_hour'])
            
            # è®¡ç®—æ€§èƒ½é€€åŒ–
            if len(stability_stats['operations_per_hour']) >= 2:
                performance_degradation = ((max_hourly_ops - min_hourly_ops) / max_hourly_ops * 100)
            else:
                performance_degradation = 0
        else:
            avg_ops_per_hour = 0
            performance_degradation = 0
        
        stability_result = {
            'test_duration_hours': duration_hours,
            'actual_duration_hours': round((time.time() - test_start_time) / 3600, 2),
            'avg_operations_per_hour': round(avg_ops_per_hour, 2),
            'max_hourly_operations': max_hourly_ops if stability_stats['operations_per_hour'] else 0,
            'min_hourly_operations': min_hourly_ops if stability_stats['operations_per_hour'] else 0,
            'performance_degradation_percent': round(performance_degradation, 2),
            'avg_error_rate': round(statistics.mean(stability_stats['error_rates']), 2) if stability_stats['error_rates'] else 0,
            'max_error_rate': round(max(stability_stats['error_rates']), 2) if stability_stats['error_rates'] else 0,
            'stability_score': self._calculate_stability_score(stability_stats),
            'target_met': performance_degradation < 10 and max(stability_stats['error_rates']) < 5 if stability_stats['error_rates'] else True
        }
        
        logger.info(f"ç¨³å®šæ€§æµ‹è¯•å®Œæˆ: æ€§èƒ½é€€åŒ– {performance_degradation:.1f}%")
        
        self.test_results['long_running_stability'] = stability_result
        return stability_result
    
    def _calculate_stability_score(self, stability_stats: Dict) -> float:
        """è®¡ç®—ç¨³å®šæ€§åˆ†æ•°"""
        score = 100.0
        
        # é”™è¯¯ç‡æ‰£åˆ†
        max_error_rate = max(stability_stats['error_rates']) if stability_stats['error_rates'] else 0
        score -= max_error_rate * 2  # æ¯ä¸ªç™¾åˆ†ç‚¹æ‰£2åˆ†
        
        # æ€§èƒ½é€€åŒ–æ‰£åˆ†
        if stability_stats['operations_per_hour']:
            max_ops = max(stability_stats['operations_per_hour'])
            min_ops = min(stability_stats['operations_per_hour'])
            if max_ops > 0:
                performance_drop = ((max_ops - min_ops) / max_ops) * 100
                score -= performance_drop * 0.5  # æ€§èƒ½é€€åŒ–æ¯ç™¾åˆ†ç‚¹æ‰£0.5åˆ†
        
        return max(0, round(score, 1))
    
    def run_all_tests(self, short_test: bool = False):
        """è¿è¡Œæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
        if short_test:
            logger.info("å¼€å§‹çŸ­æ—¶é—´å‹åŠ›æµ‹è¯• (ç”¨äºå¼€å‘/è°ƒè¯•)...")
        else:
            logger.info("å¼€å§‹å®Œæ•´å‹åŠ›æµ‹è¯•...")
        
        try:
            # æ ¹æ®æµ‹è¯•æ¨¡å¼è°ƒæ•´å‚æ•°
            if short_test:
                db_duration = 0.5  # 30åˆ†é’Ÿ
                memory_duration = 0.5  # 30åˆ†é’Ÿ
                user_duration = 0.25  # 15åˆ†é’Ÿ
                stability_duration = 1  # 1å°æ—¶
                max_threads = 10
                max_users = 20
            else:
                db_duration = 4  # 4å°æ—¶
                memory_duration = 2  # 2å°æ—¶
                user_duration = 1  # 1å°æ—¶
                stability_duration = 6  # 6å°æ—¶ (æ€»æµ‹è¯•æ—¶é—´è€ƒè™‘é™åˆ¶)
                max_threads = 20
                max_users = 50
            
            # è®¾ç½®ä¿¡å·å¤„ç†å™¨ç”¨äºä¼˜é›…åœæ­¢
            def signal_handler(signum, frame):
                logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…åœæ­¢æµ‹è¯•...")
                self.stop_flag = True
            
            signal.signal(signal.SIGINT, signal_handler)
            signal.signal(signal.SIGTERM, signal_handler)
            
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.stress_test_database_operations(db_duration, max_threads)
            self.memory_leak_detection(memory_duration)
            self.concurrent_user_simulation(user_duration, max_users)
            self.long_running_stability_test(stability_duration)
            
            logger.info("å‹åŠ›æµ‹è¯•å®Œæˆ")
            
        except KeyboardInterrupt:
            logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            self.stop_flag = True
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            self.stop_flag = True
            raise
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š"""
        return {
            'test_type': 'load_stress_test',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_results': self.test_results,
            'metrics_log_size': len(self.metrics_log),
            'summary': self._generate_summary()
        }
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            'total_tests': len(self.test_results),
            'passed_tests': 0,
            'failed_tests': 0,
            'overall_status': 'unknown',
            'stability_score': 0,
            'performance_score': 0
        }
        
        # æ£€æŸ¥å„é¡¹æµ‹è¯•æ˜¯å¦è¾¾æ ‡
        target_checks = {
            'database_stress': lambda r: r.get('target_met', False),
            'memory_leak_detection': lambda r: r.get('target_met', False),
            'concurrent_user_simulation': lambda r: r.get('target_met', False),
            'long_running_stability': lambda r: r.get('target_met', False)
        }
        
        passed_count = 0
        for test_name, check_func in target_checks.items():
            if test_name in self.test_results:
                try:
                    if check_func(self.test_results[test_name]):
                        passed_count += 1
                except Exception:
                    pass
        
        summary['passed_tests'] = passed_count
        summary['failed_tests'] = summary['total_tests'] - passed_count
        
        # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°
        if 'long_running_stability' in self.test_results:
            summary['stability_score'] = self.test_results['long_running_stability'].get('stability_score', 0)
        
        # è®¡ç®—æ€§èƒ½åˆ†æ•°
        if passed_count == summary['total_tests']:
            summary['overall_status'] = 'passed'
        elif passed_count > 0:
            summary['overall_status'] = 'partial'
        else:
            summary['overall_status'] = 'failed'
        
        return summary
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        try:
            if self.test_db_path.exists():
                self.test_db_path.unlink()
            logger.info("å‹åŠ›æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¸…ç†ç¯å¢ƒæ—¶å‡ºé”™: {e}")


def run_load_tests(short_test: bool = False):
    """è¿è¡Œå‹åŠ›æµ‹è¯•çš„ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç³»ç»Ÿå‹åŠ›æµ‹è¯•")
    print("=" * 60)
    
    tester = SystemStressTest()
    
    try:
        # è¿è¡Œæµ‹è¯•
        tester.run_all_tests(short_test=short_test)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_report()
        
        # è¾“å‡ºç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        summary = report['summary']
        print(f"   ç¨³å®šæ€§åˆ†æ•°: {summary['stability_score']}/100")
        print(f"   æ€»æµ‹è¯•é¡¹: {summary['total_tests']}")
        print(f"   é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"   æ•´ä½“çŠ¶æ€: {summary['overall_status']}")
        
        # è¯¦ç»†ç»“æœ
        print("\nğŸ“ˆ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, results in report['test_results'].items():
            print(f"\n{test_name}:")
            if isinstance(results, dict):
                for key, value in results.items():
                    if key.endswith('_met') or isinstance(value, bool):
                        status = "âœ…" if value else "âŒ"
                        print(f"   {status} {key}: {value}")
                    elif isinstance(value, (int, float)) and key != 'errors_sample':
                        print(f"   {key}: {value}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = Path("tests/load_test_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        raise
    finally:
        tester.cleanup()


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦è¿è¡ŒçŸ­æµ‹è¯•
    short_test = len(sys.argv) > 1 and sys.argv[1] == '--short'
    run_load_tests(short_test=short_test)