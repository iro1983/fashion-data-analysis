# ç®¡ç†ç»´æŠ¤æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»ç³»ç»Ÿçš„å®‰è£…ã€é…ç½®ã€ç›‘æ§å’Œç»´æŠ¤æ“ä½œï¼Œé€‚ç”¨äºç³»ç»Ÿç®¡ç†å‘˜å’Œè¿ç»´äººå‘˜ã€‚

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿå®‰è£…ä¸éƒ¨ç½²](#ç³»ç»Ÿå®‰è£…ä¸éƒ¨ç½²)
- [é…ç½®ç®¡ç†](#é…ç½®ç®¡ç†)
- [æ•°æ®åº“ç®¡ç†](#æ•°æ®åº“ç®¡ç†)
- [ç›‘æ§ä¸å‘Šè­¦](#ç›‘æ§ä¸å‘Šè­¦)
- [æ•°æ®æŠ“å–ç®¡ç†](#æ•°æ®æŠ“å–ç®¡ç†)
- [ç³»ç»Ÿä¼˜åŒ–](#ç³»ç»Ÿä¼˜åŒ–)
- [å¤‡ä»½ä¸æ¢å¤](#å¤‡ä»½ä¸æ¢å¤)
- [å®‰å…¨è®¾ç½®](#å®‰å…¨è®¾ç½®)

## ç³»ç»Ÿå®‰è£…ä¸éƒ¨ç½²

### ç¯å¢ƒå‡†å¤‡

#### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**ï¼šUbuntu 18.04+ / CentOS 7+ / Windows 10+ / macOS 10.15+
- **CPU**ï¼šè‡³å°‘2æ ¸ï¼Œæ¨è4æ ¸+
- **å†…å­˜**ï¼šæœ€å°‘4GBï¼Œæ¨è8GB+
- **å­˜å‚¨**ï¼šæœ€å°‘10GBå¯ç”¨ç©ºé—´ï¼Œæ¨è50GB+
- **ç½‘ç»œ**ï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥ï¼Œå»ºè®®å¸¦å®½10Mbps+

#### ä¾èµ–è½¯ä»¶å®‰è£…

**Ubuntu/Debianç³»ç»Ÿ**ï¼š
```bash
# æ›´æ–°ç³»ç»ŸåŒ…
sudo apt update && sudo apt upgrade -y

# å®‰è£…Pythonå’Œpip
sudo apt install python3 python3-pip python3-venv -y

# å®‰è£…Node.js (å‰ç«¯ä»ªè¡¨æ¿éœ€è¦)
curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -
sudo apt install nodejs -y

# å®‰è£…SQLite
sudo apt install sqlite3 -y

# å®‰è£…å…¶ä»–å¿…è¦å·¥å…·
sudo apt install git curl wget unzip -y
```

**CentOS/RHELç³»ç»Ÿ**ï¼š
```bash
# å®‰è£…EPELä»“åº“
sudo yum install epel-release -y

# å®‰è£…Pythonå’Œpip
sudo yum install python3 python3-pip -y

# å®‰è£…Node.js
curl -fsSL https://rpm.nodesource.com/setup_16.x | sudo bash -
sudo yum install nodejs -y

# å®‰è£…SQLite
sudo yum install sqlite -y
```

**macOSç³»ç»Ÿ**ï¼š
```bash
# å®‰è£…Homebrewï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# å®‰è£…ä¾èµ–
brew install python3 node sqlite
```

### éƒ¨ç½²æ­¥éª¤

#### 1. é¡¹ç›®æ–‡ä»¶éƒ¨ç½²

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
sudo mkdir -p /opt/tiktok-amazon-system
cd /opt/tiktok-amazon-system

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶ï¼ˆå‡è®¾ä»æºç ä»“åº“è·å–ï¼‰
git clone <repository-url> .
# æˆ–è€…
# wget <download-url> && unzip <archive-file>

# è®¾ç½®ç›®å½•æƒé™
sudo chown -R $USER:$USER /opt/tiktok-amazon-system
chmod +x run.sh
```

#### 2. Pythonç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows

# å®‰è£…ä¾èµ–
pip install --upgrade pip
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python main.py --help
```

#### 3. å‰ç«¯ä»ªè¡¨æ¿éƒ¨ç½²

```bash
# è¿›å…¥å‰ç«¯ç›®å½•
cd fashion-dashboard

# å®‰è£…ä¾èµ–
npm install

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬
npm run build

# ä½¿ç”¨é™æ€æ–‡ä»¶æœåŠ¡å™¨éƒ¨ç½²
npm install -g serve
serve -s dist -l 3000
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### ä½¿ç”¨systemdæœåŠ¡ï¼ˆLinuxï¼‰

åˆ›å»ºæœåŠ¡æ–‡ä»¶ `/etc/systemd/system/tiktok-amazon-scraper.service`ï¼š

```ini
[Unit]
Description=TikTok Amazon Scraper Service
After=network.target

[Service]
Type=simple
User=scraper
Group=scraper
WorkingDirectory=/opt/tiktok-amazon-system
Environment=PATH=/opt/tiktok-amazon-system/venv/bin
ExecStart=/opt/tiktok-amazon-system/venv/bin/python main.py daemon
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

å¯åŠ¨æœåŠ¡ï¼š
```bash
sudo systemctl daemon-reload
sudo systemctl enable tiktok-amazon-scraper
sudo systemctl start tiktok-amazon-scraper
sudo systemctl status tiktok-amazon-scraper
```

#### ä½¿ç”¨Dockeréƒ¨ç½²

åˆ›å»º `Dockerfile`ï¼š

```dockerfile
FROM python:3.9-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    sqlite3 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# å®‰è£…Pythonä¾èµ–
RUN pip install -r requirements.txt

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "main.py", "daemon"]
```

æ„å»ºå’Œè¿è¡Œï¼š
```bash
# æ„å»ºé•œåƒ
docker build -t tiktok-amazon-scraper .

# è¿è¡Œå®¹å™¨
docker run -d \
  --name scraper-service \
  -p 8000:8000 \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/logs:/app/logs \
  tiktok-amazon-scraper
```

## é…ç½®ç®¡ç†

### é…ç½®æ–‡ä»¶ç»“æ„

ç³»ç»Ÿä½¿ç”¨YAMLæ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼Œä¸»è¦é…ç½®æ–‡ä»¶ä¸º `config/config.yaml`ï¼š

```yaml
# å®Œæ•´é…ç½®ç¤ºä¾‹
database:
  type: sqlite
  path: data/scraping.db
  backup_enabled: true
  backup_interval: "24h"
  connection_pool_size: 10

scraping:
  amazon:
    enabled: true
    max_concurrent: 3
    request_delay: 1.0
    timeout: 30
    user_agent: "Mozilla/5.0..."
    categories:
      - "T-Shirt"
      - "Hoodie"
      - "Sweatshirt"
    keywords:
      - "print"
      - "graphic"
      - "design"
    proxy:
      enabled: false
      url: ""
      username: ""
      password: ""
  
  tiktok:
    enabled: true
    max_concurrent: 2
    request_delay: 2.0
    timeout: 30
    categories:
      - "æœè£…"
      - "æ—¶å°š"
      - "æ½®æµ"
    keywords:
      - "å°èŠ±"
      - "Tæ¤"
      - "å«è¡£"

retry:
  max_retries: 3
  backoff_factor: 2
  retry_delay: 5
  max_retry_delay: 300

monitoring:
  log_level: INFO
  performance_tracking: true
  alert_thresholds:
    failure_rate: 0.3
    avg_response_time: 30
    memory_usage: 80
    disk_usage: 90
  notification:
    email:
      enabled: false
      smtp_server: ""
      smtp_port: 587
      username: ""
      password: ""
      recipients: []

advanced:
  deduplication:
    enabled: true
    strategy: "product_id"
    cache_ttl: 3600
  
  data_validation:
    enabled: true
    strict_mode: false
    required_fields:
      - "title"
      - "price"
      - "url"
  
  output:
    format: "json"
    compress: false
    max_file_size: "100MB"
    export_path: "exports/"

security:
  api_rate_limit: 1000
  request_signature: true
  encryption_key: ""
  access_log: true
```

### é…ç½®ç®¡ç†å‘½ä»¤

#### æŸ¥çœ‹å½“å‰é…ç½®

```bash
# æŸ¥çœ‹æ‰€æœ‰é…ç½®
python main.py config show

# æŸ¥çœ‹ç‰¹å®šæ¨¡å—é…ç½®
python main.py config show scraping.amazon

# æŸ¥çœ‹ç‰¹å®šé…ç½®é¡¹
python main.py config get scraping.amazon.max_concurrent
```

#### ä¿®æ”¹é…ç½®

```bash
# è®¾ç½®é…ç½®é¡¹
python main.py config set scraping.amazon.max_concurrent 5
python main.py config set monitoring.log_level DEBUG

# æ‰¹é‡è®¾ç½®
python main.py config set-batch config/quick_settings.yaml
```

#### é…ç½®éªŒè¯

```bash
# éªŒè¯é…ç½®æ–‡ä»¶è¯­æ³•
python main.py config validate

# æµ‹è¯•è¿æ¥é…ç½®
python main.py config test-connections

# ç”Ÿæˆé…ç½®å·®å¼‚æŠ¥å‘Š
python main.py config diff config/backup_config.yaml
```

### ç¯å¢ƒå˜é‡é…ç½®

æŸäº›é…ç½®ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼š

```bash
# æ•°æ®åº“é…ç½®
export SCRAPER_DB_PATH="/custom/path/scraping.db"
export SCRAPER_DB_BACKUP_ENABLED="true"

# ä»£ç†é…ç½®
export SCRAPER_PROXY_URL="http://proxy.example.com:8080"
export SCRAPER_PROXY_USERNAME="user"
export SCRAPER_PROXY_PASSWORD="pass"

# é€šçŸ¥é…ç½®
export SCRAPER_EMAIL_SMTP_SERVER="smtp.example.com"
export SCRAPER_EMAIL_USERNAME="notification@example.com"
export SCRAPER_EMAIL_PASSWORD="password"
```

## æ•°æ®åº“ç®¡ç†

### æ•°æ®åº“ç»“æ„

ç³»ç»Ÿä½¿ç”¨SQLiteæ•°æ®åº“ï¼Œä¸»è¦è¡¨ç»“æ„ï¼š

#### äº§å“è¡¨ (products)
```sql
CREATE TABLE products (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    platform VARCHAR(50) NOT NULL,           -- 'amazon' æˆ– 'tiktok'
    product_id VARCHAR(255) UNIQUE NOT NULL, -- å¹³å°äº§å“ID
    title TEXT NOT NULL,                      -- äº§å“æ ‡é¢˜
    brand VARCHAR(255),                       -- å“ç‰Œ
    price DECIMAL(10,2),                      -- ä»·æ ¼
    currency VARCHAR(3) DEFAULT 'USD',        -- è´§å¸
    category VARCHAR(255),                    -- åˆ†ç±»
    subcategory VARCHAR(255),                 -- å­åˆ†ç±»
    rating DECIMAL(3,2),                      -- è¯„åˆ†
    review_count INTEGER DEFAULT 0,           -- è¯„ä»·æ•°é‡
    sales_rank INTEGER,                       -- é”€é‡æ’å
    availability VARCHAR(50),                 -- åº“å­˜çŠ¶æ€
    url TEXT,                                 -- äº§å“é“¾æ¥
    image_url TEXT,                           -- å›¾ç‰‡é“¾æ¥
    description TEXT,                         -- æè¿°
    features TEXT,                            -- ç‰¹æ€§JSON
    specifications TEXT,                      -- è§„æ ¼JSON
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    last_scraped DATETIME,                    -- æœ€åæŠ“å–æ—¶é—´
    
    INDEX idx_platform (platform),
    INDEX idx_category (category),
    INDEX idx_price (price),
    INDEX idx_platform_id (platform, product_id),
    INDEX idx_created (created_at),
    INDEX idx_last_scraped (last_scraped)
);
```

#### æŠ“å–ä»»åŠ¡è¡¨ (scraping_tasks)
```sql
CREATE TABLE scraping_tasks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    task_id VARCHAR(255) UNIQUE NOT NULL,
    platform VARCHAR(50) NOT NULL,
    category VARCHAR(255),
    keywords TEXT,                            -- å…³é”®è¯JSONæ•°ç»„
    status VARCHAR(50) DEFAULT 'pending',     -- 'pending', 'running', 'completed', 'failed'
    start_time DATETIME,
    end_time DATETIME,
    products_found INTEGER DEFAULT 0,
    products_new INTEGER DEFAULT 0,
    products_updated INTEGER DEFAULT 0,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_platform (platform),
    INDEX idx_status (status),
    INDEX idx_created (created_at),
    INDEX idx_platform_status (platform, status)
);
```

#### ç³»ç»Ÿæ—¥å¿—è¡¨ (system_logs)
```sql
CREATE TABLE system_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    level VARCHAR(20) NOT NULL,               -- 'DEBUG', 'INFO', 'WARNING', 'ERROR'
    module VARCHAR(100) NOT NULL,             -- æ¨¡å—åç§°
    message TEXT NOT NULL,                    -- æ—¥å¿—æ¶ˆæ¯
    details TEXT,                             -- è¯¦ç»†ä¿¡æ¯JSON
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    task_id VARCHAR(255),                     -- å…³è”ä»»åŠ¡ID
    
    INDEX idx_level (level),
    INDEX idx_module (module),
    INDEX idx_timestamp (timestamp),
    INDEX idx_task_id (task_id)
);
```

### æ•°æ®åº“æ“ä½œ

#### æ—¥å¸¸ç»´æŠ¤å‘½ä»¤

```bash
# æ•°æ®åº“çŠ¶æ€æ£€æŸ¥
python main.py db status

# æ•°æ®åº“ä¿¡æ¯ç»Ÿè®¡
python main.py db stats

# æ•°æ®æ¸…ç†
python main.py db cleanup --older-than 30    # æ¸…ç†30å¤©å‰çš„æ•°æ®
python main.py db cleanup --empty-categories # æ¸…ç†ç©ºåˆ†ç±»
python main.py db cleanup --duplicates       # æ¸…ç†é‡å¤æ•°æ®

# æ•°æ®åº“ä¼˜åŒ–
python main.py db optimize                   # é‡å»ºç´¢å¼•ï¼Œå‹ç¼©æ•°æ®åº“
python main.py db vacuum                     # æ¸…ç†ç¢ç‰‡ç©ºé—´
```

#### å¤‡ä»½ä¸æ¢å¤

```bash
# æ‰‹åŠ¨å¤‡ä»½
python main.py db backup --path /backup/scraping_$(date +%Y%m%d).db

# è‡ªåŠ¨å¤‡ä»½è®¾ç½®
python main.py config set database.backup_enabled true
python main.py config set database.backup_interval "24h"
python main.py config set database.backup_path "/backup/"

# å¤‡ä»½æ¢å¤
python main.py db restore --path /backup/scraping_20231114.db
```

#### æ•°æ®å¯¼å…¥å¯¼å‡º

```bash
# å¯¼å‡ºæ•°æ®
python export_data.py --format json --output products.json
python export_data.py --format csv --output products.csv --platform amazon

# å¯¼å…¥æ•°æ®
python import_data.py --file products_import.json --merge   # åˆå¹¶æ¨¡å¼
python import_data.py --file products_import.json --replace # æ›¿æ¢æ¨¡å¼
```

### æ€§èƒ½ä¼˜åŒ–

#### æŸ¥è¯¢ä¼˜åŒ–

```sql
-- åˆ†ææŸ¥è¯¢æ€§èƒ½
EXPLAIN QUERY PLAN SELECT * FROM products WHERE platform = 'amazon' AND price BETWEEN 50 AND 100;
EXPLAIN QUERY PLAN SELECT * FROM products WHERE category = 'T-Shirt' ORDER BY created_at DESC LIMIT 100;

-- åˆ›å»ºå¤åˆç´¢å¼•ï¼ˆå¦‚æœæŸ¥è¯¢é¢‘ç¹ï¼‰
CREATE INDEX idx_platform_category_price ON products(platform, category, price);
CREATE INDEX idx_platform_created ON products(platform, created_at);

-- æ¸…ç†è¿‡æœŸç»Ÿè®¡ä¿¡æ¯
ANALYZE products;
ANALYZE scraping_tasks;
ANALYZE system_logs;
```

#### æ•°æ®åº“ç»´æŠ¤è„šæœ¬

åˆ›å»ºè‡ªåŠ¨ç»´æŠ¤è„šæœ¬ `scripts/db_maintenance.sh`ï¼š

```bash
#!/bin/bash

# æ•°æ®åº“ç»´æŠ¤è„šæœ¬
DB_PATH="/opt/tiktok-amazon-system/data/scraping.db"
BACKUP_PATH="/backup/scraping"
LOG_FILE="/var/log/scraper-db-maintenance.log"

log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log_message "å¼€å§‹æ•°æ®åº“ç»´æŠ¤"

# 1. å¤‡ä»½æ•°æ®åº“
log_message "åˆ›å»ºæ•°æ®åº“å¤‡ä»½"
cp "$DB_PATH" "$BACKUP_PATH/scraping_$(date +%Y%m%d_%H%M%S).db"

# 2. æ¸…ç†æ—§æ•°æ®ï¼ˆä¿ç•™30å¤©ï¼‰
log_message "æ¸…ç†30å¤©å‰çš„æ•°æ®"
sqlite3 "$DB_PATH" "DELETE FROM products WHERE created_at < datetime('now', '-30 days');"
sqlite3 "$DB_PATH" "DELETE FROM system_logs WHERE timestamp < datetime('now', '-30 days');"

# 3. æ¸…ç†ç©ºåˆ†ç±»äº§å“
log_message "æ¸…ç†ç©ºåˆ†ç±»äº§å“"
sqlite3 "$DB_PATH" "DELETE FROM products WHERE category IS NULL OR category = '';"

# 4. é‡å»ºç´¢å¼•
log_message "é‡å»ºæ•°æ®åº“ç´¢å¼•"
sqlite3 "$DB_PATH" "REINDEX;"

# 5. å‹ç¼©æ•°æ®åº“
log_message "å‹ç¼©æ•°æ®åº“"
sqlite3 "$DB_PATH" "VACUUM;"

# 6. åˆ†ææ•°æ®åº“ç»Ÿè®¡
log_message "æ›´æ–°æ•°æ®åº“ç»Ÿè®¡"
sqlite3 "$DB_PATH" "ANALYZE;"

# 7. æ¸…ç†å¤‡ä»½æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
find "$BACKUP_PATH" -name "scraping_*.db" -mtime +7 -delete

log_message "æ•°æ®åº“ç»´æŠ¤å®Œæˆ"
```

è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼š
```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ æ¯æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œç»´æŠ¤
0 2 * * * /opt/tiktok-amazon-system/scripts/db_maintenance.sh
```

## ç›‘æ§ä¸å‘Šè­¦

### æ—¥å¿—é…ç½®

#### æ—¥å¿—çº§åˆ«è®¾ç½®

```yaml
# config/config.yaml
monitoring:
  log_level: INFO                    # DEBUG, INFO, WARNING, ERROR
  log_format: "json"                 # "text" æˆ– "json"
  log_rotation:
    max_size: "100MB"               # å•ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°
    backup_count: 10                # ä¿ç•™çš„æ—¥å¿—æ–‡ä»¶æ•°é‡
    compress: true                   # æ˜¯å¦å‹ç¼©å†å²æ—¥å¿—
  
  # æ–‡ä»¶è·¯å¾„é…ç½®
  log_files:
    main: "logs/coordinator.log"
    scraping: "logs/scraping.log"
    database: "logs/database.log"
    errors: "logs/errors.log"
```

#### æ—¥å¿—æŸ¥çœ‹å‘½ä»¤

```bash
# å®æ—¶æŸ¥çœ‹ä¸»è¦æ—¥å¿—
tail -f logs/coordinator.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f logs/errors.log

# æœç´¢ç‰¹å®šé”™è¯¯
grep "ERROR" logs/coordinator.log | tail -20

# æŒ‰æ—¶é—´èŒƒå›´æŸ¥çœ‹æ—¥å¿—
grep "2025-11-14 10:" logs/coordinator.log

# æŸ¥çœ‹ç‰¹å®šæ¨¡å—æ—¥å¿—
grep "AmazonScraper" logs/scraping.log
```

### æ€§èƒ½ç›‘æ§

#### ç³»ç»ŸæŒ‡æ ‡ç›‘æ§

```bash
# CPUå’Œå†…å­˜ä½¿ç”¨ç‡
python main.py monitor system --metrics cpu,memory,disk

# æŠ“å–æ€§èƒ½ç»Ÿè®¡
python main.py monitor scraping --period 24h

# æ•°æ®åº“æ€§èƒ½åˆ†æ
python main.py monitor database --slow-queries

# ç½‘ç»œè¯·æ±‚ç»Ÿè®¡
python main.py monitor network --by-platform
```

#### å®æ—¶ç›‘æ§è„šæœ¬

åˆ›å»ºç›‘æ§è„šæœ¬ `scripts/monitor.sh`ï¼š

```bash
#!/bin/bash

# ç³»ç»Ÿç›‘æ§è„šæœ¬
MONITOR_LOG="/var/log/scraper-monitor.log"
ALERT_EMAIL="admin@example.com"

check_system_health() {
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    local mem_usage=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
    local disk_usage=$(df -h / | awk 'NR==2 {print $5}' | cut -d'%' -f1)
    
    echo "[$(date)] CPU: ${cpu_usage}%, Memory: ${mem_usage}%, Disk: ${disk_usage}%" >> "$MONITOR_LOG"
    
    # æ£€æŸ¥å‘Šè­¦é˜ˆå€¼
    if (( $(echo "$cpu_usage > 80" | bc -l) )); then
        send_alert "CPUä½¿ç”¨ç‡è¿‡é«˜: ${cpu_usage}%"
    fi
    
    if (( $(echo "$mem_usage > 85" | bc -l) )); then
        send_alert "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: ${mem_usage}%"
    fi
    
    if (( disk_usage > 90 )); then
        send_alert "ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: ${disk_usage}%"
    fi
}

check_scraper_status() {
    local running_tasks=$(python main.py status --json | jq '.running_tasks')
    local failed_tasks=$(python main.py status --json | jq '.failed_tasks')
    local success_rate=$(python main.py status --json | jq '.success_rate')
    
    if (( $(echo "$success_rate < 0.8" | bc -l) )); then
        send_alert "æŠ“å–æˆåŠŸç‡è¿‡ä½: $success_rate"
    fi
    
    if (( failed_tasks > 5 )); then
        send_alert "å¤±è´¥ä»»åŠ¡æ•°é‡è¿‡å¤š: $failed_tasks"
    fi
}

send_alert() {
    local message="$1"
    echo "[ALERT] $message" >> "$MONITOR_LOG"
    
    # å‘é€é‚®ä»¶å‘Šè­¦ï¼ˆéœ€è¦é…ç½®é‚®ä»¶ç³»ç»Ÿï¼‰
    # echo "$message" | mail -s "Scraper Alert" "$ALERT_EMAIL"
    
    # å‘é€åˆ°æ—¥å¿—ç³»ç»Ÿ
    logger -t scraper-monitor "$message"
}

# æ‰§è¡Œæ£€æŸ¥
check_system_health
check_scraper_status
```

### å‘Šè­¦é…ç½®

#### é‚®ä»¶å‘Šè­¦è®¾ç½®

```yaml
# config/config.yaml
monitoring:
  notification:
    email:
      enabled: true
      smtp_server: "smtp.gmail.com"
      smtp_port: 587
      username: "your-email@gmail.com"
      password: "your-app-password"  # ä½¿ç”¨åº”ç”¨ä¸“ç”¨å¯†ç 
      use_tls: true
      recipients:
        - "admin@example.com"
        - "ops-team@example.com"
      
    # å‘Šè­¦è§„åˆ™
    alert_rules:
      - name: "high_failure_rate"
        condition: "scraping.success_rate < 0.8"
        threshold: 3                 # è¿ç»­3æ¬¡æ£€æŸ¥éƒ½è§¦å‘æ‰å‘é€
        interval: "5m"               # æ£€æŸ¥é—´éš”
        message: "æŠ“å–ä»»åŠ¡å¤±è´¥ç‡è¶…è¿‡80%"
      
      - name: "system_resources"
        condition: "system.cpu_usage > 80 OR system.memory_usage > 85"
        threshold: 2
        interval: "1m"
        message: "ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡è¿‡é«˜"
      
      - name: "database_issues"
        condition: "database.connection_errors > 0"
        threshold: 1
        interval: "30s"
        message: "æ•°æ®åº“è¿æ¥å‡ºç°é—®é¢˜"
```

#### å‘Šè­¦æµ‹è¯•

```bash
# æµ‹è¯•é‚®ä»¶å‘Šè­¦
python main.py alert test --type email --message "è¿™æ˜¯ä¸€æ¡æµ‹è¯•å‘Šè­¦æ¶ˆæ¯"

# æµ‹è¯•æ‰€æœ‰å‘Šè­¦è§„åˆ™
python main.py alert test --all

# æ‰‹åŠ¨è§¦å‘å‘Šè­¦æ£€æŸ¥
python main.py alert check --now
```

## æ•°æ®æŠ“å–ç®¡ç†

### ä»»åŠ¡è°ƒåº¦

#### å®šæ—¶æŠ“å–è®¾ç½®

```bash
# æ·»åŠ å®šæ—¶ä»»åŠ¡
python main.py schedule add --name "amazon-morning" \
    --platform amazon \
    --cron "0 8 * * *" \
    --category "T-Shirt,Hoodie" \
    --keywords "print,graphic"

python main.py schedule add --name "tiktok-evening" \
    --platform tiktok \
    --cron "0 18 * * *" \
    --category "æœè£…,æ—¶å°š"

# æŸ¥çœ‹å®šæ—¶ä»»åŠ¡
python main.py schedule list

# åˆ é™¤å®šæ—¶ä»»åŠ¡
python main.py schedule remove amazon-morning

# å¯ç”¨/ç¦ç”¨ä»»åŠ¡
python main.py schedule enable amazon-morning
python main.py schedule disable tiktok-evening
```

#### æ‰‹åŠ¨ä»»åŠ¡æ§åˆ¶

```bash
# å¼€å§‹æ–°çš„æŠ“å–ä»»åŠ¡
python main.py scrape start --platform amazon --async

# æŸ¥çœ‹è¿è¡Œä¸­çš„ä»»åŠ¡
python main.py task list --status running

# åœæ­¢ä»»åŠ¡
python main.py task stop <task-id>

# é‡å¯å¤±è´¥çš„ä»»åŠ¡
python main.py task restart --platform amazon --since "2025-11-14 10:00"
```

### æŠ“å–ä¼˜åŒ–

#### å¹¶å‘æ§åˆ¶

```yaml
# æ€§èƒ½è°ƒä¼˜é…ç½®
scraping:
  amazon:
    # åŸºç¡€å¹¶å‘è®¾ç½®
    max_concurrent: 3
    request_delay: 1.0
    
    # é«˜çº§ä¼˜åŒ–è®¾ç½®
    connection_pool_size: 10
    timeout: 30
    retry_count: 3
    
    # å¹³å°ç‰¹å®šä¼˜åŒ–
    rate_limit:
      requests_per_minute: 60
      burst_limit: 10
    
    # ä»£ç†è®¾ç½®ï¼ˆå¯é€‰ï¼‰
    proxy:
      enabled: false
      rotation: false
      pool_size: 5
```

#### æ•°æ®è´¨é‡æ§åˆ¶

```yaml
# æ•°æ®éªŒè¯é…ç½®
advanced:
  data_validation:
    enabled: true
    strict_mode: false
    
    # å¿…å¡«å­—æ®µéªŒè¯
    required_fields:
      - "title"
      - "price" 
      - "url"
      - "platform"
    
    # æ•°æ®æ¸…æ´—è§„åˆ™
    cleaning_rules:
      - field: "title"
        rules:
          - "remove_html_tags"
          - "trim_whitespace"
          - "normalize_case"
      
      - field: "price"
        rules:
          - "extract_numeric"
          - "validate_currency"
          - "range_check:0,10000"
    
    # é‡å¤æ•°æ®æ£€æµ‹
    deduplication:
      enabled: true
      strategy: "product_id"    # product_id, url, title, fingerprint
      confidence_threshold: 0.8
      
  # è´¨é‡è¯„åˆ†ç³»ç»Ÿ
  quality_scoring:
    enabled: true
    weights:
      completeness: 0.3        # æ•°æ®å®Œæ•´åº¦æƒé‡
      accuracy: 0.4            # æ•°æ®å‡†ç¡®åº¦æƒé‡
      freshness: 0.3           # æ•°æ®æ–°é²œåº¦æƒé‡
```

### é”™è¯¯å¤„ç†

#### å¸¸è§é”™è¯¯å¤„ç†

```bash
# æŸ¥çœ‹å¤±è´¥ä»»åŠ¡è¯¦æƒ…
python main.py task details <failed-task-id> --errors

# åˆ†æé”™è¯¯æ¨¡å¼
python main.py error analysis --period 7d --by-type

# æ¸…ç†å¤±è´¥ä»»åŠ¡
python main.py task cleanup --status failed --older-than 7d

# é”™è¯¯ç»Ÿè®¡æŠ¥å‘Š
python main.py report error-trends --format html --output error_report.html
```

#### é”™è¯¯æ¢å¤ç­–ç•¥

```yaml
# config/config.yaml
retry:
  # åŸºç¡€é‡è¯•é…ç½®
  max_retries: 3
  backoff_factor: 2
  retry_delay: 5
  max_retry_delay: 300
  
  # æŒ‰é”™è¯¯ç±»å‹å®šåˆ¶ç­–ç•¥
  error_strategies:
    "ConnectionError":
      max_retries: 5
      backoff_factor: 3
      retry_delay: 10
    
    "TimeoutError":
      max_retries: 3
      backoff_factor: 1.5
      retry_delay: 5
    
    "RateLimitError":
      max_retries: 2
      backoff_factor: 1
      retry_delay: 60
    
    "ValidationError":
      max_retries: 0         # ä¸é‡è¯•éªŒè¯é”™è¯¯
      skip_error: true

# æ•…éšœè½¬ç§»è®¾ç½®
failover:
  enabled: true
  proxy_rotation: true
  fallback_platforms:
    - "backup_amazon_endpoint"
    - "backup_tiktok_endpoint"
```

## ç³»ç»Ÿä¼˜åŒ–

### æ€§èƒ½è°ƒä¼˜

#### ç³»ç»Ÿçº§ä¼˜åŒ–

```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
python main.py system resource-check

# ä¼˜åŒ–ç³»ç»Ÿè®¾ç½®
# Linuxç³»ç»Ÿ
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
sysctl -p

# æ•°æ®åº“ä¼˜åŒ–è®¾ç½®
python main.py config set database.optimization_enabled true
python main.py config set database.auto_vacuum true
python main.py config set database.cache_size 10000
```

#### åº”ç”¨çº§ä¼˜åŒ–

```yaml
# æ€§èƒ½è°ƒä¼˜é…ç½®
performance:
  # å†…å­˜ç®¡ç†
  memory:
    max_heap_size: "2GB"
    gc_threshold: 700
    object_cache_size: 1000
  
  # ç¼“å­˜é…ç½®
  cache:
    enabled: true
    backend: "redis"              # "memory", "redis", "disk"
    ttl: 3600                     # ç¼“å­˜ç”Ÿå­˜æ—¶é—´
    max_size: "500MB"
    
    # Redisé…ç½®ï¼ˆå¦‚æœä½¿ç”¨Redisï¼‰
    redis:
      host: "localhost"
      port: 6379
      password: ""
      db: 0
  
  # è¿æ¥æ± é…ç½®
  connection_pool:
    enabled: true
    max_connections: 20
    min_connections: 5
    idle_timeout: 300
    connection_timeout: 30
```

### å®¹é‡è§„åˆ’

#### å­˜å‚¨å®¹é‡è®¡ç®—

```bash
# ä¼°ç®—å­˜å‚¨éœ€æ±‚
python main.py capacity estimate \
    --daily-products 1000 \
    --retention-days 90 \
    --products-per-category 100

# è¾“å‡ºç¤ºä¾‹:
# ä¼°ç®—ç»“æœ:
# - æ¯æ—¥æ–°å¢äº§å“: 1000ä¸ª
# - ä¿ç•™å¤©æ•°: 90å¤©
# - é¢„è®¡æ•°æ®åº“å¤§å°: 2.3GB
# - é¢„è®¡æ—¥å¿—æ–‡ä»¶å¤§å°: 500MB
# - æ€»å­˜å‚¨éœ€æ±‚: 2.8GB
```

#### æ‰©å®¹å»ºè®®

```bash
# æ€§èƒ½åŸºå‡†æµ‹è¯•
python main.py benchmark --duration 1h --load normal

# æ‰©å®¹æ–¹æ¡ˆå»ºè®®
python main.py scaling recommendation \
    --current-throughput 1000 \
    --target-throughput 5000
```

## å¤‡ä»½ä¸æ¢å¤

### è‡ªåŠ¨åŒ–å¤‡ä»½

#### å¤‡ä»½ç­–ç•¥é…ç½®

```yaml
# backupç­–ç•¥é…ç½®
backup:
  enabled: true
  schedule:
    # å®Œæ•´å¤‡ä»½
    full_backup:
      enabled: true
      cron: "0 2 * * 0"         # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
      retention: 4               # ä¿ç•™4å‘¨
    
    # å¢é‡å¤‡ä»½  
    incremental_backup:
      enabled: true
      cron: "0 2 * * 1-6"       # æ¯å¤©å‡Œæ™¨2ç‚¹ï¼ˆå‘¨æ—¥é™¤å¤–ï¼‰
      retention: 7               # ä¿ç•™7å¤©
  
  # å¤‡ä»½ä½ç½®
  locations:
    local:
      path: "/backup/scraper/"
      compression: true
      encryption: false
    
    remote:
      enabled: false
      type: "s3"                 # "s3", "ftp", "sftp"
      bucket: "scraper-backups"
      region: "us-east-1"
      access_key: "${AWS_ACCESS_KEY_ID}"
      secret_key: "${AWS_SECRET_ACCESS_KEY}"
  
  # å¤‡ä»½å†…å®¹
  include:
    - "data/scraping.db"
    - "logs/"
    - "config/"
    - "exports/"
  
  exclude:
    - "*.tmp"
    - "cache/"
    - "temp/"
```

#### å¤‡ä»½æ‰§è¡Œå‘½ä»¤

```bash
# æ‰‹åŠ¨æ‰§è¡Œå®Œæ•´å¤‡ä»½
python main.py backup create --type full --comment "æ‰‹åŠ¨å¤‡ä»½-$(date +%Y%m%d)"

# æ‰‹åŠ¨æ‰§è¡Œå¢é‡å¤‡ä»½
python main.py backup create --type incremental

# æŸ¥çœ‹å¤‡ä»½å†å²
python main.py backup list --count 10

# éªŒè¯å¤‡ä»½å®Œæ•´æ€§
python main.py backup verify <backup-id>

# æ¸…ç†è¿‡æœŸå¤‡ä»½
python main.py backup cleanup --older-than 30d
```

### ç¾éš¾æ¢å¤

#### æ¢å¤æµç¨‹

```bash
# 1. åœæ­¢æœåŠ¡
sudo systemctl stop tiktok-amazon-scraper

# 2. å¤‡ä»½å½“å‰æ•°æ®
cp data/scraping.db data/scraping.db.emergency_backup

# 3. ä»å¤‡ä»½æ¢å¤
python main.py backup restore <backup-id> --target data/scraping.db

# 4. éªŒè¯æ•°æ®å®Œæ•´æ€§
python main.py db verify --fix-errors

# 5. é‡å¯æœåŠ¡
sudo systemctl start tiktok-amazon-scraper

# 6. éªŒè¯æœåŠ¡çŠ¶æ€
python main.py health-check
```

#### ç¾éš¾æ¢å¤è®¡åˆ’

åˆ›å»ºç¾éš¾æ¢å¤æ–‡æ¡£ `disaster_recovery.md`ï¼š

```markdown
# ç¾éš¾æ¢å¤è®¡åˆ’

## æ¢å¤ç›®æ ‡æ—¶é—´ (RTO): 2å°æ—¶
## æ¢å¤ç‚¹ç›®æ ‡ (RPO): 24å°æ—¶

## æ¢å¤æ­¥éª¤

### 1. æœåŠ¡ä¸­æ–­ (< 5åˆ†é’Ÿ)
- åœæ­¢æ‰€æœ‰ç›¸å…³æœåŠ¡
- ä¿æŠ¤ç°åœºæ•°æ®
- é€šçŸ¥ç›¸å…³äººå‘˜

### 2. ç¯å¢ƒé‡å»º (30åˆ†é’Ÿ)
- é‡æ–°éƒ¨ç½²ç³»ç»Ÿç¯å¢ƒ
- æ¢å¤é…ç½®æ–‡ä»¶
- æ¢å¤æ•°æ®åº“å¤‡ä»½

### 3. æœåŠ¡æ¢å¤ (60åˆ†é’Ÿ)  
- å¯åŠ¨æ‰€æœ‰æœåŠ¡ç»„ä»¶
- éªŒè¯æ•°æ®å®Œæ•´æ€§
- æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•

### 4. ä¸šåŠ¡éªŒè¯ (25åˆ†é’Ÿ)
- éªŒè¯æ ¸å¿ƒåŠŸèƒ½
- ç›‘æ§ç³»ç»Ÿæ€§èƒ½
- ç”¨æˆ·éªŒæ”¶æµ‹è¯•

## å…³é”®è”ç³»äºº
- ç³»ç»Ÿç®¡ç†å‘˜: admin@example.com
- æŠ€æœ¯æ”¯æŒ: support@example.com  
- ä¸šåŠ¡è´Ÿè´£äºº: business@example.com

## èµ„æºæ¸…å•
- å¤‡ç”¨æœåŠ¡å™¨: backup-server-01
- å¤‡ä»½å­˜å‚¨: /backup/scraper/
- æ¢å¤æ–‡æ¡£: /docs/disaster_recovery.md
```

## å®‰å…¨è®¾ç½®

### è®¿é—®æ§åˆ¶

#### é…ç½®æ–‡ä»¶å®‰å…¨

```bash
# è®¾ç½®é…ç½®æ–‡ä»¶æƒé™
chmod 600 config/config.yaml
chown scraper:scraper config/config.yaml

# åŠ å¯†æ•æ„Ÿä¿¡æ¯
python main.py config encrypt-secrets --output config/secrets.enc

# éªŒè¯é…ç½®æ–‡ä»¶å®Œæ•´æ€§
python main.py config verify-integrity
```

#### APIå®‰å…¨

```yaml
# å®‰å…¨é…ç½®
security:
  # APIè®¿é—®æ§åˆ¶
  api:
    rate_limit:
      enabled: true
      requests_per_minute: 100
      burst_limit: 20
    
    authentication:
      enabled: true
      method: "token"           # "token", "oauth", "basic"
      token_expiry: "24h"
    
    cors:
      enabled: true
      allowed_origins:
        - "https://dashboard.example.com"
        - "https://app.example.com"
      allowed_methods:
        - "GET"
        - "POST"
      allowed_headers:
        - "Content-Type"
        - "Authorization"
  
  # æ•°æ®åŠ å¯†
  encryption:
    enabled: true
    algorithm: "AES-256-GCM"
    key_rotation: "90d"        # å¯†é’¥è½®æ¢å‘¨æœŸ
  
  # å®¡è®¡æ—¥å¿—
  audit:
    enabled: true
    log_access: true
    log_changes: true
    retention: "1y"
```

#### ç½‘ç»œå®‰å…¨

```bash
# é…ç½®é˜²ç«å¢™è§„åˆ™
sudo ufw allow 22/tcp      # SSH
sudo ufw allow 8000/tcp    # APIæœåŠ¡
sudo ufw deny 3000/tcp     # æ‹’ç»ç›´æ¥è®¿é—®å‰ç«¯
sudo ufw enable

# ä½¿ç”¨åå‘ä»£ç† (Nginx)
cat > /etc/nginx/sites-available/scraper << EOF
server {
    listen 80;
    server_name scraper.example.com;
    
    location /api/ {
        proxy_pass http://localhost:8000/;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
        
        # é™æµ
        limit_req zone=api burst=10 nodelay;
    }
    
    location / {
        root /opt/tiktok-amazon-system/fashion-dashboard/dist;
        try_files \$uri \$uri/ /index.html;
    }
}
EOF

sudo ln -s /etc/nginx/sites-available/scraper /etc/nginx/sites-enabled/
sudo nginx -t && sudo systemctl reload nginx
```

### åˆè§„æ€§ç®¡ç†

#### æ•°æ®åˆè§„æ£€æŸ¥

```bash
# æ•°æ®åˆè§„æ‰«æ
python main.py compliance scan --platform amazon --scope privacy

# ç”Ÿæˆåˆè§„æŠ¥å‘Š
python main.py compliance report --format pdf --output compliance_report.pdf

# è‡ªåŠ¨åŒ–åˆè§„æ£€æŸ¥
python main.py compliance monitor --alert-email admin@example.com
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£æ¶µç›–äº†TikTok & Amazonæœè£…æ•°æ®ç³»ç»Ÿçš„å®Œæ•´ç®¡ç†ç»´æŠ¤æ“ä½œï¼ŒåŒ…æ‹¬ï¼š

- **ç³»ç»Ÿéƒ¨ç½²**ï¼šä»å®‰è£…åˆ°ç”Ÿäº§çš„å®Œæ•´æµç¨‹
- **é…ç½®ç®¡ç†**ï¼šçµæ´»çš„YAMLé…ç½®ç³»ç»Ÿ
- **æ•°æ®åº“ç®¡ç†**ï¼šæ€§èƒ½ä¼˜åŒ–å’Œç»´æŠ¤
- **ç›‘æ§å‘Šè­¦**ï¼šå…¨é¢çš„ç³»ç»Ÿç›‘æ§æ–¹æ¡ˆ
- **å¤‡ä»½æ¢å¤**ï¼šå¯é çš„ç¾éš¾æ¢å¤è®¡åˆ’
- **å®‰å…¨è®¾ç½®**ï¼šç”Ÿäº§ç¯å¢ƒå®‰å…¨é…ç½®

éµå¾ªæœ¬æŒ‡å—å¯ä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šã€å®‰å…¨ã€é«˜æ•ˆåœ°è¿è¡Œã€‚å¦‚éœ€æ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼Œè¯·å‚è€ƒAPIå‚è€ƒæ–‡æ¡£æˆ–è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿã€‚