# APIå‚è€ƒæ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†TikTok & Amazonæœè£…æ•°æ®ç³»ç»Ÿçš„ç¨‹åºåŒ–è®¿é—®æ¥å£ï¼ŒåŒ…æ‹¬REST APIã€å‘½ä»¤è¡Œæ¥å£å’ŒSDKä½¿ç”¨è¯´æ˜ã€‚

## ğŸ“‹ ç›®å½•

- [APIæ¦‚è¿°](#apiæ¦‚è¿°)
- [REST APIæ¥å£](#rest-apiæ¥å£)
- [å‘½ä»¤è¡Œæ¥å£](#å‘½ä»¤è¡Œæ¥å£)
- [Python SDK](#python-sdk)
- [æ•°æ®æ¨¡å‹](#æ•°æ®æ¨¡å‹)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [è®¤è¯æˆæƒ](#è®¤è¯æˆæƒ)
- [é™æµç­–ç•¥](#é™æµç­–ç•¥)
- [SDKç¤ºä¾‹](#sdkç¤ºä¾‹)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## APIæ¦‚è¿°

### æ¥å£æ¶æ„

ç³»ç»Ÿæä¾›ä¸‰ç§APIè®¿é—®æ–¹å¼ï¼š

1. **REST API**ï¼šæ ‡å‡†çš„HTTPæ¥å£ï¼Œé€‚åˆWebåº”ç”¨é›†æˆ
2. **CLIæ¥å£**ï¼šå‘½ä»¤è¡Œæ¥å£ï¼Œé€‚åˆè„šæœ¬å’Œè‡ªåŠ¨åŒ–ä»»åŠ¡
3. **Python SDK**ï¼šç¨‹åºåº“ï¼Œé€‚åˆå¤æ‚çš„æ•°æ®å¤„ç†å’Œåˆ†æ

### åŸºç¡€URL

```
ç”Ÿäº§ç¯å¢ƒ: https://api.scraper-system.com/v1
å¼€å‘ç¯å¢ƒ: http://localhost:8000/api/v1
```

### æ•°æ®æ ¼å¼

- **è¯·æ±‚æ ¼å¼**ï¼šJSON
- **å“åº”æ ¼å¼**ï¼šJSON
- **å­—ç¬¦ç¼–ç **ï¼šUTF-8
- **æ—¥æœŸæ ¼å¼**ï¼šISO 8601 (2025-11-14T10:30:00Z)

### ç‰ˆæœ¬æ§åˆ¶

APIä½¿ç”¨URLç‰ˆæœ¬æ§åˆ¶ï¼Œå½“å‰ç‰ˆæœ¬ä¸ºv1ã€‚å‘åå…¼å®¹çš„æ›´æ”¹ä¼šåœ¨åŒä¸€ç‰ˆæœ¬ä¸­å‘å¸ƒï¼Œç ´åæ€§æ›´æ”¹ä¼šå‘å¸ƒæ–°ç‰ˆæœ¬ã€‚

## REST APIæ¥å£

### è®¤è¯

æ‰€æœ‰APIè¯·æ±‚éœ€è¦åœ¨Headerä¸­åŒ…å«è®¤è¯ä¿¡æ¯ï¼š

```bash
Authorization: Bearer <your-api-token>
Content-Type: application/json
```

### åŸºç¡€å“åº”æ ¼å¼

#### æˆåŠŸå“åº”
```json
{
  "success": true,
  "data": {
    // å“åº”æ•°æ®
  },
  "meta": {
    "timestamp": "2025-11-14T10:30:00Z",
    "request_id": "req_123456789",
    "version": "v1"
  }
}
```

#### é”™è¯¯å“åº”
```json
{
  "success": false,
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "è¯·æ±‚å‚æ•°æ— æ•ˆ",
    "details": {
      "field": "platform",
      "issue": "å¿…é¡»æ˜¯'amazon'æˆ–'tiktok'"
    }
  },
  "meta": {
    "timestamp": "2025-11-14T10:30:00Z",
    "request_id": "req_123456789"
  }
}
```

### äº§å“ç›¸å…³æ¥å£

#### è·å–äº§å“åˆ—è¡¨

```http
GET /api/v1/products
```

**æŸ¥è¯¢å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | æè¿° |
|-----|------|------|--------|------|
| platform | string | å¦ | all | å¹³å°ç­›é€‰: `amazon`, `tiktok`, `all` |
| category | string | å¦ | - | åˆ†ç±»ç­›é€‰ |
| price_min | number | å¦ | - | æœ€ä½ä»·æ ¼ |
| price_max | number | å¦ | - | æœ€é«˜ä»·æ ¼ |
| brand | string | å¦ | - | å“ç‰Œç­›é€‰ |
| limit | integer | å¦ | 50 | è¿”å›æ•°é‡é™åˆ¶ (1-1000) |
| offset | integer | å¦ | 0 | åç§»é‡ |
| sort | string | å¦ | created_at | æ’åºå­—æ®µ: `created_at`, `price`, `rating` |
| order | string | å¦ | desc | æ’åºæ–¹å‘: `asc`, `desc` |

**è¯·æ±‚ç¤ºä¾‹**ï¼š
```bash
curl -X GET "http://localhost:8000/api/v1/products?platform=amazon&category=T-Shirt&price_min=20&price_max=100&limit=10" \
  -H "Authorization: Bearer your-api-token"
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "products": [
      {
        "id": "prod_12345",
        "platform": "amazon",
        "product_id": "B08N5WRWNW",
        "title": "Classic Cotton T-Shirt",
        "brand": "BrandName",
        "price": 29.99,
        "currency": "USD",
        "category": "T-Shirt",
        "rating": 4.5,
        "review_count": 1250,
        "sales_rank": 15,
        "availability": "In Stock",
        "url": "https://amazon.com/dp/B08N5WRWNW",
        "image_url": "https://m.media-amazon.com/images/I/...",
        "created_at": "2025-11-14T10:30:00Z",
        "updated_at": "2025-11-14T10:30:00Z"
      }
    ],
    "pagination": {
      "total": 1250,
      "limit": 10,
      "offset": 0,
      "has_more": true
    }
  }
}
```

#### è·å–å•ä¸ªäº§å“è¯¦æƒ…

```http
GET /api/v1/products/{product_id}
```

**è·¯å¾„å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…å¡« | æè¿° |
|-----|------|------|------|
| product_id | string | æ˜¯ | äº§å“ID |

**è¯·æ±‚ç¤ºä¾‹**ï¼š
```bash
curl -X GET "http://localhost:8000/api/v1/products/prod_12345" \
  -H "Authorization: Bearer your-api-token"
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "product": {
      "id": "prod_12345",
      "platform": "amazon",
      "product_id": "B08N5WRWNW",
      "title": "Classic Cotton T-Shirt",
      "brand": "BrandName",
      "price": 29.99,
      "currency": "USD",
      "category": "T-Shirt",
      "subcategory": "Basic Tees",
      "rating": 4.5,
      "review_count": 1250,
      "sales_rank": 15,
      "availability": "In Stock",
      "url": "https://amazon.com/dp/B08N5WRWNW",
      "image_url": "https://m.media-amazon.com/images/I/...",
      "description": "High quality cotton t-shirt...",
      "features": ["100% Cotton", "Machine Washable", "Available in Multiple Colors"],
      "specifications": {
        "material": "100% Cotton",
        "care": "Machine wash cold",
        "origin": "Made in USA"
      },
      "created_at": "2025-11-14T10:30:00Z",
      "updated_at": "2025-11-14T10:30:00Z",
      "last_scraped": "2025-11-14T10:25:00Z"
    }
  }
}
```

#### æœç´¢äº§å“

```http
GET /api/v1/products/search
```

**æŸ¥è¯¢å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…å¡« | æè¿° |
|-----|------|------|------|
| q | string | æ˜¯ | æœç´¢å…³é”®è¯ |
| platform | string | å¦ | å¹³å°ç­›é€‰ |
| category | string | å¦ | åˆ†ç±»ç­›é€‰ |
| price_range | string | å¦ | ä»·æ ¼èŒƒå›´ï¼Œå¦‚ "20-100" |
| sort_by | string | å¦ | æ’åºæ–¹å¼: `relevance`, `price_low`, `price_high`, `rating` |
| limit | integer | å¦ | è¿”å›æ•°é‡ (1-100) |

**è¯·æ±‚ç¤ºä¾‹**ï¼š
```bash
curl -X GET "http://localhost:8000/api/v1/products/search?q=graphic%20tee&platform=amazon&sort_by=price_low&limit=20" \
  -H "Authorization: Bearer your-api-token"
```

### ç»Ÿè®¡å’Œåˆ†ææ¥å£

#### è·å–æ•°æ®æ¦‚è§ˆ

```http
GET /api/v1/analytics/overview
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "summary": {
      "total_products": 15640,
      "amazon_products": 8932,
      "tiktok_products": 6708,
      "today_new_products": 156,
      "categories_count": 28,
      "brands_count": 456
    },
    "trends": {
      "products_trend": [
        {"date": "2025-11-14", "count": 156},
        {"date": "2025-11-13", "count": 142}
      ],
      "platform_distribution": [
        {"platform": "amazon", "count": 8932, "percentage": 57.1},
        {"platform": "tiktok", "count": 6708, "percentage": 42.9}
      ]
    },
    "top_categories": [
      {"category": "T-Shirt", "count": 3456, "trend": "+5%"},
      {"category": "Hoodie", "count": 2134, "trend": "+3%"}
    ],
    "price_analysis": {
      "avg_price": 45.67,
      "median_price": 39.99,
      "min_price": 9.99,
      "max_price": 299.99
    }
  }
}
```

#### è·å–çƒ­é—¨äº§å“

```http
GET /api/v1/analytics/trending
```

**æŸ¥è¯¢å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…å¡« | æè¿° |
|-----|------|------|------|
| period | string | å¦ | æ—¶é—´å‘¨æœŸ: `1d`, `7d`, `30d`, `90d` |
| platform | string | å¦ | å¹³å°ç­›é€‰ |
| metric | string | å¦ | æ’åºæŒ‡æ ‡: `sales_rank`, `rating`, `review_count` |

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "trending_products": [
      {
        "id": "prod_12345",
        "title": "Best Selling Hoodie 2025",
        "platform": "amazon",
        "price": 59.99,
        "rating": 4.8,
        "review_count": 2341,
        "sales_rank": 3,
        "trend_score": 95,
        "trend_direction": "up",
        "growth_rate": "+15%"
      }
    ],
    "trending_categories": [
      {"category": "T-Shirt", "growth": "+12%", "new_products": 45}
    ]
  }
}
```

#### ä»·æ ¼åˆ†æ

```http
GET /api/v1/analytics/price-analysis
```

**æŸ¥è¯¢å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…å¡« | æè¿° |
|-----|------|------|------|
| category | string | å¦ | äº§å“åˆ†ç±» |
| platform | string | å¦ | å¹³å° |
| period | string | å¦ | åˆ†æå‘¨æœŸ: `7d`, `30d`, `90d` |

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "price_statistics": {
      "count": 1234,
      "mean": 45.67,
      "median": 39.99,
      "std_dev": 18.23,
      "min": 9.99,
      "max": 299.99,
      "percentiles": {
        "25": 29.99,
        "50": 39.99,
        "75": 54.99,
        "90": 79.99,
        "95": 99.99
      }
    },
    "price_distribution": [
      {"range": "0-20", "count": 123, "percentage": 10.0},
      {"range": "20-40", "count": 456, "percentage": 37.0}
    ],
    "price_trends": [
      {"date": "2025-11-14", "avg_price": 45.67},
      {"date": "2025-11-13", "avg_price": 45.12}
    ]
  }
}
```

### æŠ“å–ä»»åŠ¡æ¥å£

#### å¯åŠ¨æŠ“å–ä»»åŠ¡

```http
POST /api/v1/scraping/tasks
```

**è¯·æ±‚ä½“**ï¼š
```json
{
  "platform": "amazon",
  "categories": ["T-Shirt", "Hoodie"],
  "keywords": ["print", "graphic"],
  "options": {
    "max_products": 1000,
    "priority": "high",
    "schedule_time": "2025-11-14T12:00:00Z"
  }
}
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "task_id": "task_98765",
    "status": "pending",
    "estimated_completion": "2025-11-14T12:15:00Z",
    "queue_position": 1,
    "estimated_products": 800
  }
}
```

#### è·å–ä»»åŠ¡çŠ¶æ€

```http
GET /api/v1/scraping/tasks/{task_id}
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "task": {
      "task_id": "task_98765",
      "platform": "amazon",
      "status": "running",
      "progress": 65,
      "products_found": 520,
      "products_processed": 338,
      "start_time": "2025-11-14T12:00:00Z",
      "estimated_completion": "2025-11-14T12:15:00Z",
      "errors": [],
      "warnings": []
    }
  }
}
```

#### åˆ—å‡ºä»»åŠ¡

```http
GET /api/v1/scraping/tasks
```

**æŸ¥è¯¢å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…å¡« | æè¿° |
|-----|------|------|------|
| status | string | å¦ | çŠ¶æ€ç­›é€‰: `pending`, `running`, `completed`, `failed` |
| platform | string | å¦ | å¹³å°ç­›é€‰ |
| limit | integer | å¦ | è¿”å›æ•°é‡ |
| offset | integer | å¦ | åç§»é‡ |

### æ•°æ®å¯¼å‡ºæ¥å£

#### å¯¼å‡ºæ•°æ®

```http
POST /api/v1/export
```

**è¯·æ±‚ä½“**ï¼š
```json
{
  "format": "json",  // "json", "csv", "excel", "xml"
  "filters": {
    "platform": "amazon",
    "category": "T-Shirt",
    "created_after": "2025-11-01T00:00:00Z"
  },
  "fields": [
    "id", "title", "price", "rating", "review_count"
  ],
  "options": {
    "include_images": false,
    "compress": true
  }
}
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "export_id": "exp_abcdef",
    "status": "processing",
    "download_url": null,
    "estimated_completion": "2025-11-14T12:05:00Z",
    "file_size": "15.2MB",
    "record_count": 12345
  }
}
```

#### è·å–å¯¼å‡ºçŠ¶æ€

```http
GET /api/v1/export/{export_id}
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "export": {
      "export_id": "exp_abcdef",
      "status": "completed",
      "download_url": "https://api.scraper-system.com/v1/exports/exp_abcdef/download",
      "expires_at": "2025-11-21T12:00:00Z",
      "file_size": "15.2MB",
      "record_count": 12345,
      "created_at": "2025-11-14T12:00:00Z",
      "completed_at": "2025-11-14T12:04:23Z"
    }
  }
}
```

#### ä¸‹è½½å¯¼å‡ºæ–‡ä»¶

```http
GET /api/v1/exports/{export_id}/download
```

**å“åº”**ï¼šç›´æ¥è¿”å›æ–‡ä»¶æµ

### ç³»ç»Ÿç®¡ç†æ¥å£

#### è·å–ç³»ç»ŸçŠ¶æ€

```http
GET /api/v1/system/status
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "status": "healthy",
    "version": "1.0.0",
    "uptime": "7d 12h 34m",
    "components": {
      "database": {"status": "healthy", "response_time": "5ms"},
      "scraper": {"status": "healthy", "active_tasks": 2},
      "api": {"status": "healthy", "response_time": "12ms"}
    },
    "resources": {
      "cpu_usage": 45.2,
      "memory_usage": 67.8,
      "disk_usage": 23.1,
      "network_io": {
        "bytes_sent": 1024000,
        "bytes_received": 2048000
      }
    },
    "recent_errors": []
  }
}
```

#### è·å–é…ç½®

```http
GET /api/v1/system/config
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "data": {
    "config": {
      "scraping": {
        "amazon": {
          "enabled": true,
          "max_concurrent": 3,
          "request_delay": 1.0
        },
        "tiktok": {
          "enabled": true,
          "max_concurrent": 2,
          "request_delay": 2.0
        }
      },
      "database": {
        "backup_enabled": true,
        "backup_interval": "24h"
      },
      "monitoring": {
        "log_level": "INFO",
        "performance_tracking": true
      }
    }
  }
}
```

#### æ›´æ–°é…ç½®

```http
PUT /api/v1/system/config
```

**è¯·æ±‚ä½“**ï¼š
```json
{
  "scraping": {
    "amazon": {
      "max_concurrent": 5,
      "request_delay": 0.5
    }
  }
}
```

## å‘½ä»¤è¡Œæ¥å£

### åŸºæœ¬å‘½ä»¤

#### æŸ¥çœ‹å¸®åŠ©

```bash
# ä¸»å¸®åŠ©
python main.py --help

# å­å‘½ä»¤å¸®åŠ©
python main.py scrape --help
python main.py config --help
python main.py status --help
```

### æŠ“å–ç›¸å…³å‘½ä»¤

#### å¯åŠ¨æŠ“å–

```bash
# æŠ“å–Amazonæ•°æ®
python main.py scrape --platform amazon

# æŠ“å–TikTokæ•°æ®
python main.py scrape --platform tiktok

# åŒæ—¶æŠ“å–ä¸¤ä¸ªå¹³å°
python main.py scrape --platform all

# è‡ªå®šä¹‰å‚æ•°
python main.py scrape --platform amazon --category "T-Shirt" --keywords "print,graphic" --max-products 100

# å¼‚æ­¥æ‰§è¡Œ
python main.py scrape --platform all --async --output-file results.json
```

#### æŠ“å–é€‰é¡¹

| é€‰é¡¹ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| `--platform` | æŒ‡å®šå¹³å° | `amazon`, `tiktok`, `all` |
| `--category` | äº§å“åˆ†ç±» | `"T-Shirt,Hoodie"` |
| `--keywords` | æœç´¢å…³é”®è¯ | `"print,graphic"` |
| `--max-products` | æœ€å¤§äº§å“æ•° | `1000` |
| `--async` | å¼‚æ­¥æ‰§è¡Œ | - |
| `--output-file` | è¾“å‡ºæ–‡ä»¶ | `results.json` |
| `--timeout` | è¶…æ—¶æ—¶é—´ | `300` |

### çŠ¶æ€ç›‘æ§å‘½ä»¤

#### æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€

```bash
# åŸºæœ¬çŠ¶æ€
python main.py status

# è¯¦ç»†çŠ¶æ€
python main.py status --verbose

# ç‰¹å®šç»„ä»¶çŠ¶æ€
python main.py status --component database
python main.py status --component scraper
python main.py status --component api
```

#### å®æ—¶ç›‘æ§

```bash
# å®æ—¶æ—¥å¿—
python main.py monitor --log

# æ€§èƒ½ç›‘æ§
python main.py monitor --performance --interval 10

# èµ„æºç›‘æ§
python main.py monitor --resources
```

### æ•°æ®æŸ¥è¯¢å‘½ä»¤

#### äº§å“æŸ¥è¯¢

```bash
# åŸºæœ¬æŸ¥è¯¢
python main.py query --platform amazon --category "T-Shirt"

# ä»·æ ¼èŒƒå›´æŸ¥è¯¢
python main.py query --price-min 20 --price-max 100

# é«˜çº§æŸ¥è¯¢
python main.py query --brand "Nike" --rating-min 4.0 --sort-by price --order asc

# è¾“å‡ºæ ¼å¼
python main.py query --format json --output products.json
```

#### ç»Ÿè®¡æŸ¥è¯¢

```bash
# åŸºç¡€ç»Ÿè®¡
python main.py stats --platform amazon

# åˆ†ç±»ç»Ÿè®¡
python main.py stats --by-category --period 30d

# ä»·æ ¼ç»Ÿè®¡
python main.py stats --price-analysis --platform all

# è¶‹åŠ¿åˆ†æ
python main.py stats --trends --period 7d --output trends.json
```

### é…ç½®ç®¡ç†å‘½ä»¤

#### æŸ¥çœ‹é…ç½®

```bash
# æŸ¥çœ‹æ‰€æœ‰é…ç½®
python main.py config show

# æŸ¥çœ‹ç‰¹å®šæ¨¡å—
python main.py config show scraping.amazon

# æŸ¥çœ‹ç‰¹å®šå‚æ•°
python main.py config get scraping.amazon.max_concurrent
```

#### ä¿®æ”¹é…ç½®

```bash
# è®¾ç½®å•ä¸ªå‚æ•°
python main.py config set scraping.amazon.max_concurrent 5

# æ‰¹é‡è®¾ç½®
python main.py config set-batch config/quick_settings.yaml

# é‡ç½®é…ç½®
python main.py config reset --module scraping.amazon
```

### æ•°æ®åº“ç®¡ç†å‘½ä»¤

#### æ•°æ®åº“æ“ä½œ

```bash
# åˆå§‹åŒ–æ•°æ®åº“
python main.py db init

# å¤‡ä»½æ•°æ®åº“
python main.py db backup --path /backup/scraping_$(date +%Y%m%d).db

# æ¢å¤æ•°æ®åº“
python main.py db restore --path /backup/scraping_20251114.db

# æ¸…ç†æ•°æ®
python main.py db cleanup --older-than 30d

# æ•°æ®åº“ä¼˜åŒ–
python main.py db optimize
```

#### æ•°æ®å¯¼å‡º

```bash
# å¯¼å‡ºJSON
python main.py export --format json --output products.json

# å¯¼å‡ºCSV
python main.py export --format csv --output products.csv --platform amazon

# è‡ªå®šä¹‰å­—æ®µ
python main.py export --fields "id,title,price,rating" --output custom_export.xlsx
```

## Python SDK

### å®‰è£…SDK

```bash
pip install tiktok-amazon-scraper-sdk
```

### åŸºç¡€ä½¿ç”¨

#### åˆå§‹åŒ–å®¢æˆ·ç«¯

```python
from scraper_sdk import ScraperClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = ScraperClient(
    base_url="http://localhost:8000/api/v1",
    api_key="your-api-key"
)
```

#### äº§å“æŸ¥è¯¢

```python
# è·å–äº§å“åˆ—è¡¨
products = client.products.list(
    platform="amazon",
    category="T-Shirt",
    limit=50,
    sort="price"
)

for product in products:
    print(f"{product.title}: ${product.price}")

# è·å–å•ä¸ªäº§å“
product = client.products.get("prod_12345")
print(f"Product: {product.title}")

# æœç´¢äº§å“
search_results = client.products.search(
    q="graphic tee",
    platform="amazon",
    sort_by="price_low",
    limit=20
)
```

#### æ•°æ®åˆ†æ

```python
# è·å–æ¦‚è§ˆç»Ÿè®¡
overview = client.analytics.overview()
print(f"Total products: {overview.total_products}")

# è·å–è¶‹åŠ¿æ•°æ®
trending = client.analytics.trending(
    period="7d",
    platform="amazon"
)

# ä»·æ ¼åˆ†æ
price_analysis = client.analytics.price_analysis(
    category="T-Shirt",
    period="30d"
)
```

#### æŠ“å–ä»»åŠ¡ç®¡ç†

```python
# å¯åŠ¨æŠ“å–ä»»åŠ¡
task = client.scraping.start_task(
    platform="amazon",
    categories=["T-Shirt", "Hoodie"],
    keywords=["print", "graphic"],
    max_products=1000
)

print(f"Task ID: {task.task_id}")

# è½®è¯¢ä»»åŠ¡çŠ¶æ€
import time
while task.status in ["pending", "running"]:
    task = client.scraping.get_task(task.task_id)
    print(f"Progress: {task.progress}%")
    time.sleep(5)

print(f"Task completed. Found {task.products_found} products")
```

### é«˜çº§åŠŸèƒ½

#### æ‰¹é‡æ“ä½œ

```python
from scraper_sdk import BatchClient

# æ‰¹é‡æŸ¥è¯¢
batch_client = BatchClient(client)

# æ‰¹é‡äº§å“æŸ¥è¯¢
product_queries = [
    {"platform": "amazon", "category": "T-Shirt"},
    {"platform": "amazon", "category": "Hoodie"},
    {"platform": "tiktok", "category": "æœè£…"}
]

results = batch_client.products.batch_query(product_queries)
for result in results:
    print(f"Query {result.query}: {len(result.products)} products")
```

#### å¼‚æ­¥æ“ä½œ

```python
import asyncio
from scraper_sdk import AsyncScraperClient

async def async_example():
    client = AsyncScraperClient(
        base_url="http://localhost:8000/api/v1",
        api_key="your-api-key"
    )
    
    # å¼‚æ­¥æŸ¥è¯¢å¤šä¸ªå¹³å°
    tasks = [
        client.products.list(platform="amazon", limit=100),
        client.products.list(platform="tiktok", limit=100)
    ]
    
    results = await asyncio.gather(*tasks)
    amazon_products, tiktok_products = results
    
    print(f"Amazon: {len(amazon_products)} products")
    print(f"TikTok: {len(tiktok_products)} products")

# è¿è¡Œå¼‚æ­¥å‡½æ•°
asyncio.run(async_example())
```

#### æµå¼æ•°æ®å¤„ç†

```python
from scraper_sdk import StreamingClient

def process_products():
    client = StreamingClient(
        base_url="http://localhost:8000/api/v1",
        api_key="your-api-key"
    )
    
    # æµå¼å¤„ç†å¤§é‡æ•°æ®
    count = 0
    for product in client.products.stream(platform="amazon"):
        # å®æ—¶å¤„ç†æ¯ä¸ªäº§å“
        process_product(product)
        count += 1
        
        if count % 1000 == 0:
            print(f"Processed {count} products")
    
    print(f"Total processed: {count} products")

process_products()
```

## æ•°æ®æ¨¡å‹

### äº§å“æ¨¡å‹ (Product)

```python
from dataclasses import dataclass
from typing import Optional, List
from datetime import datetime

@dataclass
class Product:
    """äº§å“æ•°æ®æ¨¡å‹"""
    id: str
    platform: str  # 'amazon' or 'tiktok'
    product_id: str  # å¹³å°äº§å“ID
    title: str
    brand: Optional[str] = None
    price: Optional[float] = None
    currency: str = "USD"
    category: Optional[str] = None
    subcategory: Optional[str] = None
    rating: Optional[float] = None
    review_count: int = 0
    sales_rank: Optional[int] = None
    availability: Optional[str] = None
    url: Optional[str] = None
    image_url: Optional[str] = None
    description: Optional[str] = None
    features: Optional[List[str]] = None
    specifications: Optional[dict] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    last_scraped: Optional[datetime] = None
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return self.__dict__
    
    @classmethod
    def from_dict(cls, data: dict) -> 'Product':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**data)
```

### ä»»åŠ¡æ¨¡å‹ (Task)

```python
from enum import Enum
from dataclasses import dataclass
from typing import Optional, List
from datetime import datetime

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class ScrapingTask:
    """æŠ“å–ä»»åŠ¡æ¨¡å‹"""
    task_id: str
    platform: str
    status: TaskStatus
    categories: Optional[List[str]] = None
    keywords: Optional[List[str]] = None
    options: Optional[dict] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    progress: int = 0
    products_found: int = 0
    products_processed: int = 0
    errors: Optional[List[str]] = None
    warnings: Optional[List[str]] = None
    created_at: Optional[datetime] = None
    
    @property
    def is_completed(self) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ"""
        return self.status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]
    
    @property
    def duration(self) -> Optional[int]:
        """è·å–ä»»åŠ¡æ‰§è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰"""
        if self.start_time and self.end_time:
            return int((self.end_time - self.start_time).total_seconds())
        return None
```

### åˆ†ææ¨¡å‹ (Analytics)

```python
from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime

@dataclass
class PlatformStats:
    """å¹³å°ç»Ÿè®¡"""
    platform: str
    total_products: int
    avg_price: float
    top_categories: List[str]
    growth_rate: Optional[float] = None

@dataclass
class PriceStats:
    """ä»·æ ¼ç»Ÿè®¡"""
    count: int
    mean: float
    median: float
    std_dev: float
    min_price: float
    max_price: float
    percentiles: Dict[str, float]

@dataclass
class TrendData:
    """è¶‹åŠ¿æ•°æ®"""
    date: datetime
    value: float
    platform: Optional[str] = None
    category: Optional[str] = None

@dataclass
class AnalyticsOverview:
    """åˆ†ææ¦‚è§ˆ"""
    total_products: int
    amazon_products: int
    tiktok_products: int
    today_new_products: int
    categories_count: int
    brands_count: int
    platform_stats: List[PlatformStats]
    price_stats: PriceStats
    top_categories: List[Dict]
    recent_trends: List[TrendData]
```

## é”™è¯¯å¤„ç†

### é”™è¯¯ç å®šä¹‰

| é”™è¯¯ç  | HTTPçŠ¶æ€ | æè¿° |
|--------|----------|------|
| `VALIDATION_ERROR` | 400 | è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥ |
| `AUTHENTICATION_ERROR` | 401 | è®¤è¯å¤±è´¥ |
| `AUTHORIZATION_ERROR` | 403 | æƒé™ä¸è¶³ |
| `NOT_FOUND` | 404 | èµ„æºä¸å­˜åœ¨ |
| `RATE_LIMIT_EXCEEDED` | 429 | è¯·æ±‚é¢‘ç‡è¶…é™ |
| `INTERNAL_ERROR` | 500 | æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ |
| `SERVICE_UNAVAILABLE` | 503 | æœåŠ¡ä¸å¯ç”¨ |
| `TIMEOUT_ERROR` | 504 | è¯·æ±‚è¶…æ—¶ |

### å¼‚å¸¸å¤„ç†ç¤ºä¾‹

#### Python SDKå¼‚å¸¸å¤„ç†

```python
from scraper_sdk import ScraperClient, APIError, ValidationError, RateLimitError

client = ScraperClient(base_url="http://localhost:8000/api/v1", api_key="your-key")

try:
    products = client.products.list(platform="invalid_platform")
except ValidationError as e:
    print(f"å‚æ•°éªŒè¯é”™è¯¯: {e.message}")
    print(f"é”™è¯¯è¯¦æƒ…: {e.details}")
except RateLimitError as e:
    print(f"é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {e.retry_after} ç§’")
    time.sleep(e.retry_after)
    # é‡è¯•è¯·æ±‚
    products = client.products.list(platform="amazon")
except APIError as e:
    print(f"APIé”™è¯¯: {e.code} - {e.message}")
    # è®°å½•é”™è¯¯æ—¥å¿—
    logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

#### REST APIé”™è¯¯å¤„ç†

```python
import requests
from requests.exceptions import RequestException

def api_request(url, headers=None, timeout=30):
    """APIè¯·æ±‚å°è£…"""
    try:
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.Timeout:
        raise APIError("REQUEST_TIMEOUT", "è¯·æ±‚è¶…æ—¶")
    except requests.exceptions.ConnectionError:
        raise APIError("CONNECTION_ERROR", "ç½‘ç»œè¿æ¥å¤±è´¥")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429:
            raise RateLimitError("é¢‘ç‡é™åˆ¶")
        elif e.response.status_code == 401:
            raise APIError("AUTHENTICATION_ERROR", "è®¤è¯å¤±è´¥")
        else:
            raise APIError("HTTP_ERROR", f"HTTPé”™è¯¯: {e.response.status_code}")
    except RequestException as e:
        raise APIError("REQUEST_ERROR", f"è¯·æ±‚é”™è¯¯: {e}")
```

### é‡è¯•æœºåˆ¶

#### è‡ªåŠ¨é‡è¯•

```python
from scraper_sdk import RetryConfig, ScraperClient

# é…ç½®é‡è¯•ç­–ç•¥
retry_config = RetryConfig(
    max_retries=3,
    backoff_factor=2.0,
    max_retry_delay=60,
    retryable_status_codes=[429, 500, 502, 503, 504]
)

client = ScraperClient(
    base_url="http://localhost:8000/api/v1",
    api_key="your-key",
    retry_config=retry_config
)

# å¯ç”¨é‡è¯•çš„è¯·æ±‚
products = client.products.list(platform="amazon")  # è‡ªåŠ¨é‡è¯•å¤±è´¥è¯·æ±‚
```

#### æ‰‹åŠ¨é‡è¯•

```python
import time
from scraper_sdk import RateLimitError

def retry_with_backoff(func, max_retries=3, base_delay=1):
    """æŒ‡æ•°é€€é¿é‡è¯•"""
    for attempt in range(max_retries):
        try:
            return func()
        except RateLimitError as e:
            if attempt == max_retries - 1:
                raise
            delay = base_delay * (2 ** attempt)
            print(f"é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {delay} ç§’åé‡è¯•...")
            time.sleep(delay)
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(base_delay)

# ä½¿ç”¨é‡è¯•å‡½æ•°
products = retry_with_backoff(
    lambda: client.products.list(platform="amazon"),
    max_retries=3
)
```

## è®¤è¯æˆæƒ

### API Keyè®¤è¯

```python
# é€šè¿‡headerä¼ é€’
headers = {
    "Authorization": "Bearer your-api-key",
    "Content-Type": "application/json"
}

# é€šè¿‡å‚æ•°ä¼ é€’
client = ScraperClient(
    base_url="http://localhost:8000/api/v1",
    api_key="your-api-key"
)
```

### OAuthè®¤è¯

```python
from scraper_sdk import OAuthClient

# OAuthæµç¨‹
oauth_client = OAuthClient(
    client_id="your-client-id",
    client_secret="your-client-secret",
    redirect_uri="http://localhost:8000/callback"
)

# è·å–æˆæƒURL
auth_url = oauth_client.get_authorization_url(
    scope="read write",
    state="random-state-string"
)

# å¤„ç†å›è°ƒ
def handle_oauth_callback(code, state):
    tokens = oauth_client.exchange_code(code)
    access_token = tokens["access_token"]
    
    # ä½¿ç”¨è®¿é—®ä»¤ç‰Œåˆå§‹åŒ–å®¢æˆ·ç«¯
    client = ScraperClient(
        base_url="http://localhost:8000/api/v1",
        access_token=access_token
    )
    
    return client
```

### æƒé™ç®¡ç†

#### æƒé™çº§åˆ«

| æƒé™ | æè¿° | å…è®¸çš„æ“ä½œ |
|------|------|-----------|
| `read` | åªè¯»æƒé™ | æŸ¥è¯¢äº§å“ã€ç»Ÿè®¡æ•°æ® |
| `write` | è¯»å†™æƒé™ | å¯åŠ¨æŠ“å–ä»»åŠ¡ã€å¯¼å‡ºæ•°æ® |
| `admin` | ç®¡ç†æƒé™ | ä¿®æ”¹é…ç½®ã€ç³»ç»Ÿç®¡ç† |
| `billing` | è®¡è´¹æƒé™ | æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡ã€ç®¡ç†è®¢é˜… |

#### æƒé™éªŒè¯

```python
# æ£€æŸ¥æƒé™
def check_permission(client, required_permission):
    user_permissions = client.auth.get_permissions()
    if required_permission not in user_permissions:
        raise PermissionError(f"éœ€è¦ {required_permission} æƒé™")

# ä½¿ç”¨æƒé™æ£€æŸ¥
try:
    check_permission(client, "write")
    task = client.scraping.start_task(platform="amazon")
except PermissionError as e:
    print(f"æƒé™ä¸è¶³: {e}")
```

## é™æµç­–ç•¥

### é™æµè§„åˆ™

| ç”¨æˆ·ç±»å‹ | æ¯åˆ†é’Ÿè¯·æ±‚æ•° | æ¯å°æ—¶è¯·æ±‚æ•° | å¹¶å‘è¿æ¥æ•° |
|----------|-------------|-------------|-----------|
| å…è´¹ç”¨æˆ· | 60 | 1000 | 3 |
| ä»˜è´¹ç”¨æˆ· | 300 | 10000 | 10 |
| ä¼ä¸šç”¨æˆ· | 1000 | 50000 | 50 |

### é™æµå¤´ä¿¡æ¯

```http
HTTP/1.1 200 OK
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1642694400
X-RateLimit-Retry-After: 45
```

### é™æµå¤„ç†

```python
from scraper_sdk import RateLimitError

def handle_rate_limit(response):
    """å¤„ç†é™æµå“åº”"""
    if response.status_code == 429:
        retry_after = int(response.headers.get('X-RateLimit-Retry-After', 60))
        remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
        reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
        
        raise RateLimitError(
            message="è¯·æ±‚é¢‘ç‡è¶…é™",
            retry_after=retry_after,
            remaining=remaining,
            reset_time=datetime.fromtimestamp(reset_time)
        )

# ä½¿ç”¨é™æµå¤„ç†
try:
    result = api_request("http://localhost:8000/api/v1/products")
except RateLimitError as e:
    print(f"é™æµé‡è¯•ï¼Œç­‰å¾… {e.retry_after} ç§’")
    time.sleep(e.retry_after)
    result = api_request("http://localhost:8000/api/v1/products")
```

## SDKç¤ºä¾‹

### å®Œæ•´ç¤ºä¾‹ï¼šç”µå•†ç«å“åˆ†æ

```python
import asyncio
from datetime import datetime, timedelta
from scraper_sdk import AsyncScraperClient
import pandas as pd
import matplotlib.pyplot as plt

class CompetitorAnalyzer:
    """ç«å“åˆ†æå™¨"""
    
    def __init__(self, api_key: str):
        self.client = AsyncScraperClient(
            base_url="http://localhost:8000/api/v1",
            api_key=api_key
        )
    
    async def analyze_market_trends(self, category: str = "T-Shirt"):
        """åˆ†æå¸‚åœºè¶‹åŠ¿"""
        print(f"åˆ†æ {category} ç±»åˆ«çš„å¸‚åœºè¶‹åŠ¿...")
        
        # å¹¶è¡Œè·å–æ•°æ®
        tasks = [
            self.client.products.list(
                platform="amazon",
                category=category,
                limit=1000,
                sort="sales_rank"
            ),
            self.client.products.list(
                platform="tiktok",
                category=category,
                limit=1000,
                sort="rating"
            )
        ]
        
        amazon_products, tiktok_products = await asyncio.gather(*tasks)
        
        # æ•°æ®åˆ†æ
        analysis = self._analyze_products(amazon_products, tiktok_products, category)
        
        return analysis
    
    def _analyze_products(self, amazon_products, tiktok_products, category):
        """åˆ†æäº§å“æ•°æ®"""
        analysis = {
            "category": category,
            "timestamp": datetime.now(),
            "amazon": {
                "count": len(amazon_products),
                "avg_price": sum(p.price for p in amazon_products if p.price) / len(amazon_products) if amazon_products else 0,
                "top_brands": self._get_top_brands(amazon_products, 5),
                "price_range": self._get_price_range(amazon_products)
            },
            "tiktok": {
                "count": len(tiktok_products),
                "avg_rating": sum(p.rating for p in tiktok_products if p.rating) / len([p for p in tiktok_products if p.rating]) if tiktok_products else 0,
                "top_brands": self._get_top_brands(tiktok_products, 5),
                "engagement": self._calculate_engagement(tiktok_products)
            },
            "comparison": self._compare_platforms(amazon_products, tiktok_products)
        }
        
        return analysis
    
    def _get_top_brands(self, products, limit=5):
        """è·å–çƒ­é—¨å“ç‰Œ"""
        brand_counts = {}
        for product in products:
            if product.brand:
                brand_counts[product.brand] = brand_counts.get(product.brand, 0) + 1
        
        return sorted(brand_counts.items(), key=lambda x: x[1], reverse=True)[:limit]
    
    def _get_price_range(self, products):
        """è·å–ä»·æ ¼èŒƒå›´"""
        prices = [p.price for p in products if p.price]
        if prices:
            return {
                "min": min(prices),
                "max": max(prices),
                "median": sorted(prices)[len(prices)//2]
            }
        return None
    
    def _calculate_engagement(self, products):
        """è®¡ç®—å‚ä¸åº¦æŒ‡æ ‡"""
        ratings = [p.rating for p in products if p.rating]
        review_counts = [p.review_count for p in products]
        
        return {
            "avg_rating": sum(ratings) / len(ratings) if ratings else 0,
            "total_reviews": sum(review_counts),
            "engagement_score": sum(r * rc for r, rc in zip(ratings, review_counts)) / len(products) if products else 0
        }
    
    def _compare_platforms(self, amazon_products, tiktok_products):
        """å¹³å°å¯¹æ¯”åˆ†æ"""
        return {
            "price_difference": self._calculate_price_diff(amazon_products, tiktok_products),
            "quality_comparison": self._compare_quality(amazon_products, tiktok_products),
            "market_overlap": self._calculate_overlap(amazon_products, tiktok_products)
        }
    
    def _calculate_price_diff(self, amazon_products, tiktok_products):
        """è®¡ç®—ä»·æ ¼å·®å¼‚"""
        amazon_prices = [p.price for p in amazon_products if p.price]
        tiktok_prices = [p.price for p in tiktok_products if p.price]
        
        if amazon_prices and tiktok_prices:
            avg_amazon = sum(amazon_prices) / len(amazon_prices)
            avg_tiktok = sum(tiktok_prices) / len(tiktok_prices)
            
            return {
                "amazon_avg": avg_amazon,
                "tiktok_avg": avg_tiktok,
                "difference_percent": ((avg_tiktok - avg_amazon) / avg_amazon) * 100
            }
        return None
    
    def _compare_quality(self, amazon_products, tiktok_products):
        """è´¨é‡å¯¹æ¯”"""
        amazon_ratings = [p.rating for p in amazon_products if p.rating]
        tiktok_ratings = [p.rating for p in tiktok_products if p.rating]
        
        return {
            "amazon_avg_rating": sum(amazon_ratings) / len(amazon_ratings) if amazon_ratings else 0,
            "tiktok_avg_rating": sum(tiktok_ratings) / len(tiktok_ratings) if tiktok_ratings else 0
        }
    
    def _calculate_overlap(self, amazon_products, tiktok_products):
        """è®¡ç®—å¸‚åœºé‡å åº¦"""
        # åŸºäºå“ç‰Œå’Œä»·æ ¼èŒƒå›´çš„ç®€å•é‡å åº¦è®¡ç®—
        amazon_brands = set(p.brand for p in amazon_products if p.brand)
        tiktok_brands = set(p.brand for p in tiktok_products if p.brand)
        
        common_brands = amazon_brands & tiktok_brands
        total_brands = amazon_brands | tiktok_brands
        
        return {
            "common_brands": len(common_brands),
            "total_brands": len(total_brands),
            "overlap_percentage": (len(common_brands) / len(total_brands)) * 100 if total_brands else 0
        }
    
    async def generate_report(self, analysis):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = f"""
# {analysis['category']} ç«å“åˆ†ææŠ¥å‘Š

## æ‰§è¡Œæ—¶é—´
{analysis['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}

## å¹³å°å¯¹æ¯”

### Amazon
- äº§å“æ•°é‡: {analysis['amazon']['count']}
- å¹³å‡ä»·æ ¼: ${analysis['amazon']['avg_price']:.2f}
- ä»·æ ¼èŒƒå›´: ${analysis['amazon']['price_range']['min']:.2f} - ${analysis['amazon']['price_range']['max']:.2f}
- çƒ­é—¨å“ç‰Œ: {', '.join([f"{brand}({count})" for brand, count in analysis['amazon']['top_brands'][:3]])}

### TikTok
- äº§å“æ•°é‡: {analysis['tiktok']['count']}
- å¹³å‡è¯„åˆ†: {analysis['tiktok']['avg_rating']:.2f}
- å‚ä¸åº¦è¯„åˆ†: {analysis['tiktok']['engagement']['engagement_score']:.2f}
- çƒ­é—¨å“ç‰Œ: {', '.join([f"{brand}({count})" for brand, count in analysis['tiktok']['top_brands'][:3]])}

## å¸‚åœºç«äº‰åˆ†æ

### ä»·æ ¼å¯¹æ¯”
{f"Amazonå¹³å‡ä»·æ ¼æ¯”TikTokä½ {abs(analysis['comparison']['price_difference']['difference_percent']):.1f}%" if analysis['comparison']['price_difference'] else "ä»·æ ¼æ•°æ®ä¸è¶³"}

### è´¨é‡å¯¹æ¯”
- Amazonå¹³å‡è¯„åˆ†: {analysis['comparison']['quality_comparison']['amazon_avg_rating']:.2f}
- TikTokå¹³å‡è¯„åˆ†: {analysis['comparison']['quality_comparison']['tiktok_avg_rating']:.2f}

### å“ç‰Œé‡å åº¦
- å…±åŒå“ç‰Œæ•°é‡: {analysis['comparison']['market_overlap']['common_brands']}
- æ€»å“ç‰Œæ•°é‡: {analysis['comparison']['market_overlap']['total_brands']}
- é‡å ç‡: {analysis['comparison']['market_overlap']['overlap_percentage']:.1f}%

## å»ºè®®
1. å…³æ³¨ä»·æ ¼æ•æ„Ÿçš„å¸‚åœºæœºä¼š
2. é‡ç‚¹å…³æ³¨é«˜è¯„åˆ†äº§å“ç‰¹å¾
3. è€ƒè™‘å“ç‰Œåˆä½œç­–ç•¥
4. ç›‘æ§çƒ­é—¨äº§å“è¶‹åŠ¿
"""
        return report
    
    def export_to_excel(self, analysis, filename="competitor_analysis.xlsx"):
        """å¯¼å‡ºåˆ†æç»“æœåˆ°Excel"""
        with pd.ExcelWriter(filename) as writer:
            # Amazonæ•°æ®
            amazon_data = []
            for product in analysis.get('amazon_products', []):
                amazon_data.append({
                    'title': product.title,
                    'brand': product.brand,
                    'price': product.price,
                    'rating': product.rating,
                    'review_count': product.review_count
                })
            
            if amazon_data:
                pd.DataFrame(amazon_data).to_excel(writer, sheet_name='Amazon Products', index=False)
            
            # TikTokæ•°æ®
            tiktok_data = []
            for product in analysis.get('tiktok_products', []):
                tiktok_data.append({
                    'title': product.title,
                    'brand': product.brand,
                    'rating': product.rating,
                    'review_count': product.review_count,
                    'engagement_score': (product.rating or 0) * product.review_count
                })
            
            if tiktok_data:
                pd.DataFrame(tiktok_data).to_excel(writer, sheet_name='TikTok Products', index=False)
            
            # åˆ†ææ‘˜è¦
            summary_data = [
                ['Amazonäº§å“æ•°é‡', analysis['amazon']['count']],
                ['Amazonå¹³å‡ä»·æ ¼', f"${analysis['amazon']['avg_price']:.2f}"],
                ['TikTokäº§å“æ•°é‡', analysis['tiktok']['count']],
                ['TikTokå¹³å‡è¯„åˆ†', f"{analysis['tiktok']['avg_rating']:.2f}"],
                ['å“ç‰Œé‡å ç‡', f"{analysis['comparison']['market_overlap']['overlap_percentage']:.1f}%"]
            ]
            
            pd.DataFrame(summary_data, columns=['æŒ‡æ ‡', 'å€¼']).to_excel(
                writer, sheet_name='Analysis Summary', index=False
            )

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    analyzer = CompetitorAnalyzer(api_key="your-api-key")
    
    # åˆ†æå¸‚åœºè¶‹åŠ¿
    analysis = await analyzer.analyze_market_trends("T-Shirt")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = await analyzer.generate_report(analysis)
    print(report)
    
    # å¯¼å‡ºæ•°æ®
    analyzer.export_to_excel(analysis, "t-shirt_analysis.xlsx")
    
    # ä¿å­˜æŠ¥å‘Š
    with open("competitor_report.md", "w", encoding="utf-8") as f:
        f.write(report)

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(main())
```

## æœ€ä½³å®è·µ

### æ€§èƒ½ä¼˜åŒ–

#### 1. ä½¿ç”¨æ‰¹é‡è¯·æ±‚

```python
# é”™è¯¯åšæ³•ï¼šå•æ¬¡è¯·æ±‚
for product_id in product_ids:
    product = client.products.get(product_id)  # 100ä¸ªè¯·æ±‚

# æ­£ç¡®åšæ³•ï¼šæ‰¹é‡è¯·æ±‚
products = client.products.batch_get(product_ids)  # 1ä¸ªè¯·æ±‚
```

#### 2. ä½¿ç”¨ç­›é€‰æ¡ä»¶

```python
# é”™è¯¯åšæ³•ï¼šè·å–æ‰€æœ‰æ•°æ®åè¿‡æ»¤
all_products = client.products.list(limit=10000)
filtered_products = [p for p in all_products if p.price > 50]

# æ­£ç¡®åšæ³•ï¼šæœåŠ¡ç«¯ç­›é€‰
filtered_products = client.products.list(
    price_min=50,
    limit=1000
)
```

#### 3. ä½¿ç”¨æµå¼å¤„ç†

```python
# å¤„ç†å¤§é‡æ•°æ®æ—¶ä½¿ç”¨æµå¼å¤„ç†
def process_large_dataset():
    count = 0
    for product in client.products.stream(platform="amazon"):
        process_product(product)
        count += 1
        
        if count % 1000 == 0:
            print(f"å·²å¤„ç† {count} ä¸ªäº§å“")

# è€Œä¸æ˜¯ä¸€æ¬¡æ€§è·å–
# products = client.products.list(limit=10000)  # å¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜
```

### é”™è¯¯å¤„ç†

#### 1. ä½¿ç”¨é€‚å½“çš„å¼‚å¸¸ç±»å‹

```python
from scraper_sdk import (
    ValidationError, RateLimitError, 
    AuthenticationError, APIError
)

try:
    result = client.products.list(invalid_param="value")
except ValidationError as e:
    # å¤„ç†å‚æ•°éªŒè¯é”™è¯¯
    logger.warning(f"å‚æ•°éªŒè¯å¤±è´¥: {e.details}")
except RateLimitError as e:
    # å¤„ç†é¢‘ç‡é™åˆ¶
    wait_time = e.retry_after
    logger.info(f"é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’")
    time.sleep(wait_time)
except AuthenticationError as e:
    # å¤„ç†è®¤è¯é”™è¯¯
    logger.error("è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥")
    raise
except APIError as e:
    # å¤„ç†å…¶ä»–APIé”™è¯¯
    logger.error(f"APIé”™è¯¯ {e.code}: {e.message}")
```

#### 2. å®ç°æŒ‡æ•°é€€é¿é‡è¯•

```python
import random
import asyncio

async def retry_with_jitter(func, max_retries=3):
    """å¸¦éšæœºæŠ–åŠ¨çš„é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            return await func()
        except RateLimitError as e:
            if attempt == max_retries - 1:
                raise
            
            # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
            base_delay = 2 ** attempt
            jitter = random.uniform(0, 1)
            delay = base_delay + jitter
            
            logger.info(f"é‡è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {delay:.2f} ç§’")
            await asyncio.sleep(delay)
```

### æ•°æ®ç¼“å­˜

#### 1. æœ¬åœ°ç¼“å­˜

```python
from functools import lru_cache
import time

class CachedScraperClient:
    def __init__(self, client):
        self.client = client
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
    
    @lru_cache(maxsize=1000)
    def get_product(self, product_id, cache_key=None):
        """ç¼“å­˜äº§å“æŸ¥è¯¢ç»“æœ"""
        return self.client.products.get(product_id)
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.get_product.cache_clear()

# ä½¿ç”¨ç¼“å­˜å®¢æˆ·ç«¯
cached_client = CachedScraperClient(client)
```

#### 2. ç»“æœç¼“å­˜

```python
import hashlib
import json

def cache_result(func):
    """ç®€å•çš„ç»“æœç¼“å­˜è£…é¥°å™¨"""
    cache = {}
    
    def wrapper(*args, **kwargs):
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(
            json.dumps((args, kwargs), sort_keys=True).encode()
        ).hexdigest()
        
        if cache_key in cache:
            result, timestamp = cache[cache_key]
            if time.time() - timestamp < 300:  # 5åˆ†é’ŸTTL
                return result
        
        # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
        result = func(*args, **kwargs)
        cache[cache_key] = (result, time.time())
        return result
    
    return wrapper

# ä½¿ç”¨ç¼“å­˜è£…é¥°å™¨
@cache_result
def get_cached_products(platform, category):
    return client.products.list(platform=platform, category=category)
```

### å¹¶å‘æ§åˆ¶

#### 1. é™åˆ¶å¹¶å‘æ•°

```python
import asyncio
from asyncio import Semaphore

async def concurrent_scraping(product_ids, max_concurrent=10):
    """é™åˆ¶å¹¶å‘æ•°çš„æ‰¹é‡æŠ“å–"""
    semaphore = Semaphore(max_concurrent)
    
    async def scrape_single(product_id):
        async with semaphore:
            try:
                return await client.products.get(product_id)
            except Exception as e:
                logger.error(f"æŠ“å–äº§å“ {product_id} å¤±è´¥: {e}")
                return None
    
    # åˆ›å»ºä»»åŠ¡
    tasks = [scrape_single(pid) for pid in product_ids]
    
    # é™åˆ¶å¹¶å‘æ‰§è¡Œ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # è¿‡æ»¤æœ‰æ•ˆç»“æœ
    valid_results = [r for r in results if r is not None and not isinstance(r, Exception)]
    
    return valid_results
```

#### 2. å¼‚æ­¥å¤„ç†

```python
async def async_data_processing():
    """å¼‚æ­¥æ•°æ®å¤„ç†æµç¨‹"""
    # å¹¶è¡Œè·å–ä¸åŒå¹³å°æ•°æ®
    tasks = [
        client.products.list(platform="amazon", limit=500),
        client.products.list(platform="tiktok", limit=500),
        client.analytics.trending(period="7d")
    ]
    
    amazon_products, tiktok_products, trends = await asyncio.gather(*tasks)
    
    # å¹¶è¡Œå¤„ç†æ•°æ®
    processing_tasks = [
        process_amazon_data(amazon_products),
        process_tiktok_data(tiktok_products),
        analyze_trends(trends)
    ]
    
    results = await asyncio.gather(*processing_tasks)
    
    return results
```

### ç›‘æ§å’Œæ—¥å¿—

#### 1. æ€§èƒ½ç›‘æ§

```python
import time
import functools
from contextlib import contextmanager

@contextmanager
def measure_time(operation_name):
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start_time = time.time()
    try:
        yield
    finally:
        duration = time.time() - start_time
        logger.info(f"{operation_name} è€—æ—¶: {duration:.2f} ç§’")

# ä½¿ç”¨æ€§èƒ½ç›‘æ§
def expensive_operation():
    with measure_time("expensive_operation"):
        # è€—æ—¶çš„æ“ä½œ
        products = client.products.list(limit=1000)

def monitor_api_calls(func):
    """APIè°ƒç”¨ç›‘æ§è£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            logger.info(f"APIè°ƒç”¨æˆåŠŸ: {func.__name__}")
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {func.__name__}, è€—æ—¶: {duration:.2f}s, é”™è¯¯: {e}")
            raise
        finally:
            pass
    return wrapper

# åº”ç”¨ç›‘æ§
client.products.list = monitor_api_calls(client.products.list)
```

#### 2. è¯¦ç»†æ—¥å¿—è®°å½•

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('scraper')

def log_api_usage(endpoint, params, result_count):
    """è®°å½•APIä½¿ç”¨æƒ…å†µ"""
    logger.info(
        f"APIè°ƒç”¨: {endpoint} | å‚æ•°: {params} | ç»“æœæ•°é‡: {result_count}"
    )

def log_error_with_context(error, context):
    """è®°å½•å¸¦ä¸Šä¸‹æ–‡çš„é”™è¯¯"""
    logger.error(
        f"é”™è¯¯: {error} | ä¸Šä¸‹æ–‡: {context}",
        exc_info=True
    )

# åœ¨APIè°ƒç”¨ä¸­ä½¿ç”¨
try:
    products = client.products.list(platform="amazon")
    log_api_usage("products.list", {"platform": "amazon"}, len(products))
except Exception as e:
    log_error_with_context(e, {"platform": "amazon", "operation": "list"})
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£å…¨é¢ä»‹ç»äº†TikTok & Amazonæœè£…æ•°æ®ç³»ç»Ÿçš„APIæ¥å£ï¼š

- **REST API**ï¼šæä¾›å®Œæ•´çš„HTTPæ¥å£
- **CLIæ¥å£**ï¼šä¾¿äºè„šæœ¬å’Œè‡ªåŠ¨åŒ–
- **Python SDK**ï¼šç¨‹åºåŒ–è®¿é—®å’Œé›†æˆ
- **æ•°æ®æ¨¡å‹**ï¼šæ ‡å‡†åŒ–çš„æ•°æ®ç»“æ„
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
- **æœ€ä½³å®è·µ**ï¼šæ€§èƒ½ä¼˜åŒ–å’Œä½¿ç”¨å»ºè®®

é€šè¿‡æœ¬APIæ–‡æ¡£ï¼Œæ‚¨å¯ä»¥ï¼š

1. é›†æˆç³»ç»Ÿåˆ°ç°æœ‰åº”ç”¨
2. æ„å»ºè‡ªå®šä¹‰çš„æ•°æ®åˆ†æå·¥å…·
3. è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†æµç¨‹
4. å¼€å‘å®æ—¶ç›‘æ§åº”ç”¨

å¦‚éœ€æ›´å¤šå¸®åŠ©ï¼Œè¯·å‚è€ƒ[æ•…éšœæ’é™¤æŒ‡å—](troubleshooting.md)æˆ–è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿã€‚