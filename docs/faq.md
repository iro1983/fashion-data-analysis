# å¸¸è§é—®é¢˜FAQ

æœ¬æ–‡æ¡£æ•´ç†äº†TikTok & Amazonæœè£…æ•°æ®ç³»ç»Ÿä½¿ç”¨è¿‡ç¨‹ä¸­çš„å¸¸è§é—®é¢˜å’Œè§£ç­”ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿæ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

- [åŸºç¡€é—®é¢˜](#åŸºç¡€é—®é¢˜)
- [å®‰è£…é…ç½®](#å®‰è£…é…ç½®)
- [ä½¿ç”¨æ“ä½œ](#ä½¿ç”¨æ“ä½œ)
- [æ•°æ®é—®é¢˜](#æ•°æ®é—®é¢˜)
- [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
- [ç½‘ç»œè¿æ¥](#ç½‘ç»œè¿æ¥)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [åŠŸèƒ½ç›¸å…³](#åŠŸèƒ½ç›¸å…³)
- [ä»˜è´¹è®¡è´¹](#ä»˜è´¹è®¡è´¹)
- [æŠ€æœ¯å’¨è¯¢](#æŠ€æœ¯å’¨è¯¢)

## åŸºç¡€é—®é¢˜

### Q1: ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: TikTok & Amazonæœè£…æ•°æ®ç³»ç»Ÿæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ•°æ®æŠ“å–å’Œåˆ†æå·¥å…·ï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

- **æ•°æ®æŠ“å–**ï¼šè‡ªåŠ¨ä»TikTokå’ŒAmazonå¹³å°æ”¶é›†æœè£…äº§å“ä¿¡æ¯
- **æ•°æ®å¯è§†åŒ–**ï¼šé€šè¿‡Webä»ªè¡¨æ¿æŸ¥çœ‹å®æ—¶æ•°æ®å’Œåˆ†æç»“æœ
- **ç«å“åˆ†æ**ï¼šæ¯”è¾ƒä¸åŒå¹³å°çš„äº§å“ä»·æ ¼ã€é”€é‡ã€è¯„ä»·ç­‰æŒ‡æ ‡
- **æ•°æ®å¯¼å‡º**ï¼šæ”¯æŒå¤šç§æ ¼å¼çš„æ•°æ®å¯¼å‡ºï¼ˆJSONã€CSVã€Excelï¼‰
- **ç›‘æ§å‘Šè­¦**ï¼šå®æ—¶ç›‘æ§ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å’Œæ•°æ®è´¨é‡

### Q2: ç³»ç»Ÿæ”¯æŒå“ªäº›æ•°æ®ç±»å‹ï¼Ÿ

**A**: ç³»ç»Ÿä¸»è¦æ”¶é›†ä»¥ä¸‹ç±»å‹çš„æœè£…äº§å“æ•°æ®ï¼š

- **åŸºæœ¬ä¿¡æ¯**ï¼šäº§å“åç§°ã€å“ç‰Œã€ä»·æ ¼ã€SKUã€URL
- **åˆ†ç±»ä¿¡æ¯**ï¼šäº§å“ç±»åˆ«ã€å­åˆ†ç±»ã€æ ‡ç­¾
- **é”€å”®æ•°æ®**ï¼šé”€é‡æ’åã€è¯„ä»·æ•°é‡ã€ç”¨æˆ·è¯„åˆ†
- **åº“å­˜çŠ¶æ€**ï¼šæ˜¯å¦æœ‰è´§ã€åº“å­˜æ•°é‡
- **åª’ä½“èµ„æº**ï¼šäº§å“å›¾ç‰‡ã€æè¿°è§†é¢‘é“¾æ¥
- **ä»·æ ¼å†å²**ï¼šä»·æ ¼å˜åŒ–è¶‹åŠ¿ï¼ˆå¦‚æœå¯ç”¨ï¼‰

### Q3: ç³»ç»Ÿé€‚åˆä»€ä¹ˆè§„æ¨¡çš„ç”¨æˆ·ï¼Ÿ

**A**: ç³»ç»Ÿè®¾è®¡é€‚ç”¨äºä¸åŒè§„æ¨¡çš„ç”¨æˆ·ï¼š

- **ä¸ªäººç”¨æˆ·**ï¼šä»·æ ¼ç›‘æ§ã€å…´è¶£ç ”ç©¶
- **å°å‹å•†å®¶**ï¼šç«å“åˆ†æã€å¸‚åœºè°ƒç ”
- **ä¸­å‹ä¼ä¸š**ï¼šäº§å“å¼€å‘ã€å®šä»·ç­–ç•¥
- **å¤§å‹ä¼ä¸š**ï¼šå¸‚åœºåˆ†æã€å•†ä¸šæ™ºèƒ½

ç³»ç»Ÿå…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´æ•°æ®æŠ“å–è§„æ¨¡å’Œå¤„ç†èƒ½åŠ›ã€‚

## å®‰è£…é…ç½®

### Q4: å®‰è£…ç³»ç»Ÿå¯¹ç”µè„‘æœ‰ä»€ä¹ˆè¦æ±‚ï¼Ÿ

**A**: ç³»ç»Ÿå¯¹ç¡¬ä»¶å’Œè½¯ä»¶çš„åŸºæœ¬è¦æ±‚ï¼š

**ç¡¬ä»¶è¦æ±‚**ï¼š
- CPUï¼šè‡³å°‘2æ ¸ï¼Œæ¨è4æ ¸+
- å†…å­˜ï¼šæœ€å°‘4GBï¼Œæ¨è8GB+
- å­˜å‚¨ï¼šè‡³å°‘2GBå¯ç”¨ç©ºé—´
- ç½‘ç»œï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥

**è½¯ä»¶è¦æ±‚**ï¼š
- æ“ä½œç³»ç»Ÿï¼šWindows 10+ã€macOS 10.15+ã€Ubuntu 18.04+
- Pythonï¼š3.8æˆ–æ›´é«˜ç‰ˆæœ¬
- Node.jsï¼š16æˆ–æ›´é«˜ç‰ˆæœ¬ï¼ˆä»…å‰ç«¯ä»ªè¡¨æ¿éœ€è¦ï¼‰
- æµè§ˆå™¨ï¼šChrome 90+ã€Firefox 88+ã€Safari 14+

### Q5: å®‰è£…è¿‡ç¨‹ä¸­é‡åˆ°æƒé™é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: æƒé™é”™è¯¯é€šå¸¸å‡ºç°åœ¨ä¾èµ–å®‰è£…æˆ–æ–‡ä»¶åˆ›å»ºæ—¶ï¼š

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ**ï¼š
   ```bash
   python3 -m venv scraper_env
   source scraper_env/bin/activate  # Linux/macOS
   # æˆ– scraper_env\Scripts\activate  # Windows
   pip install -r requirements.txt
   ```

2. **ç”¨æˆ·çº§å®‰è£…**ï¼š
   ```bash
   pip install --user -r requirements.txt
   ```

3. **ä¿®å¤æ–‡ä»¶æƒé™**ï¼ˆLinux/macOSï¼‰ï¼š
   ```bash
   chmod +x run.sh
   sudo chown -R $USER:$USER /path/to/project
   ```

4. **ç®¡ç†å‘˜æƒé™**ï¼ˆWindowsï¼‰ï¼š
   - å³é”®ç‚¹å‡»å‘½ä»¤æç¤ºç¬¦ï¼Œé€‰æ‹©"ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ"

### Q6: é…ç½®æ–‡ä»¶åœ¨å“ªé‡Œï¼Ÿå¦‚ä½•ä¿®æ”¹ï¼Ÿ

**A**: ä¸»è¦é…ç½®æ–‡ä»¶ä½äºï¼š

**é…ç½®æ–‡ä»¶ä½ç½®**ï¼š
- ä¸»é…ç½®ï¼š`config/config.yaml`
- ç¯å¢ƒé…ç½®ï¼š`config/environments/`
- ç”¨æˆ·é…ç½®ï¼š`~/.scraper/config.yaml`

**ä¿®æ”¹é…ç½®**ï¼š
1. **å‘½ä»¤è¡Œä¿®æ”¹**ï¼š
   ```bash
   python main.py config set scraping.amazon.max_concurrent 5
   python main.py config show
   ```

2. **ç›´æ¥ç¼–è¾‘æ–‡ä»¶**ï¼š
   ```bash
   nano config/config.yaml
   ```

3. **é€šè¿‡Webç•Œé¢**ï¼šåœ¨ä»ªè¡¨æ¿çš„è®¾ç½®é¡µé¢ä¿®æ”¹é…ç½®

**å¸¸ç”¨é…ç½®é¡¹**ï¼š
```yaml
# æŠ“å–è®¾ç½®
scraping:
  amazon:
    max_concurrent: 3      # å¹¶å‘æ•°
    request_delay: 1.0     # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    timeout: 30            # è¶…æ—¶æ—¶é—´
    categories: ["T-Shirt", "Hoodie"]  # äº§å“ç±»åˆ«
    keywords: ["print", "graphic"]     # æœç´¢å…³é”®è¯

# æ•°æ®åº“è®¾ç½®
database:
  backup_enabled: true     # å¯ç”¨å¤‡ä»½
  backup_interval: "24h"   # å¤‡ä»½é—´éš”

# ç›‘æ§è®¾ç½®
monitoring:
  log_level: INFO          # æ—¥å¿—çº§åˆ«
  performance_tracking: true  # æ€§èƒ½è·Ÿè¸ª
```

## ä½¿ç”¨æ“ä½œ

### Q7: å¦‚ä½•å¼€å§‹ç¬¬ä¸€æ¬¡æ•°æ®æŠ“å–ï¼Ÿ

**A**: æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¼€å§‹ç¬¬ä¸€æ¬¡æŠ“å–ï¼š

**æ­¥éª¤1ï¼šæ£€æŸ¥å®‰è£…**
```bash
python main.py --help
python main.py health-check
```

**æ­¥éª¤2ï¼šæ‰§è¡ŒæŠ“å–**
```bash
# æŠ“å–Amazonæ•°æ®
python main.py scrape --platform amazon

# æŠ“å–TikTokæ•°æ®
python main.py scrape --platform tiktok

# åŒæ—¶æŠ“å–ä¸¤ä¸ªå¹³å°
python main.py scrape --platform all
```

**æ­¥éª¤3ï¼šæŸ¥çœ‹ç»“æœ**
```bash
# æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
python main.py status

# æŸ¥çœ‹æŠ“å–åˆ°çš„äº§å“
python main.py query --platform amazon --limit 10

# å¯åŠ¨Webä»ªè¡¨æ¿
cd fashion-dashboard && npm run dev
```

**æ­¥éª¤4ï¼šè®¿é—®ä»ªè¡¨æ¿**
- æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼š`http://localhost:5173`
- æŸ¥çœ‹å®æ—¶æ•°æ®å’Œåˆ†æç»“æœ

### Q8: å¦‚ä½•è‡ªå®šä¹‰æŠ“å–å…³é”®è¯å’Œç±»åˆ«ï¼Ÿ

**A**: æœ‰å¤šç§æ–¹å¼è‡ªå®šä¹‰æŠ“å–å†…å®¹ï¼š

**æ–¹æ³•1ï¼šé…ç½®æ–‡ä»¶ä¿®æ”¹**
```yaml
# config/config.yaml
scraping:
  amazon:
    categories:
      - "T-Shirt"          # æ·»åŠ Tæ¤
      - "Hoodie"           # æ·»åŠ å«è¡£
      - "Jeans"            # æ·»åŠ ç‰›ä»”è£¤
      - "Jacket"           # æ·»åŠ å¤¹å…‹
    keywords:
      - "cotton"           # æ£‰è´¨
      - "organic"          # æœ‰æœº
      - "sustainable"      # å¯æŒç»­
      - "eco-friendly"     # ç¯ä¿
```

**æ–¹æ³•2ï¼šå‘½ä»¤è¡Œå‚æ•°**
```bash
# ä¸´æ—¶ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°
python main.py scrape --platform amazon \
    --category "T-Shirt,Hoodie" \
    --keywords "cotton,organic" \
    --max-products 500
```

**æ–¹æ³•3ï¼šWebä»ªè¡¨æ¿è®¾ç½®**
- ç™»å½•ä»ªè¡¨æ¿ â†’ è®¾ç½® â†’ æŠ“å–é…ç½®
- å›¾å½¢ç•Œé¢ä¿®æ”¹å‚æ•°
- å®æ—¶ä¿å­˜é…ç½®

### Q9: å¦‚ä½•æŸ¥çœ‹å’Œç®¡ç†æŠ“å–ä»»åŠ¡ï¼Ÿ

**A**: ç³»ç»Ÿæä¾›å¤šç§ä»»åŠ¡ç®¡ç†æ–¹å¼ï¼š

**å‘½ä»¤è¡Œç®¡ç†**ï¼š
```bash
# æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡
python main.py task list

# æŸ¥çœ‹è¿è¡Œä¸­çš„ä»»åŠ¡
python main.py task list --status running

# åœæ­¢æŒ‡å®šä»»åŠ¡
python main.py task stop task_id_123

# æŸ¥çœ‹ä»»åŠ¡è¯¦æƒ…
python main.py task details task_id_123

# é‡å¯å¤±è´¥çš„ä»»åŠ¡
python main.py task restart --platform amazon
```

**Webä»ªè¡¨æ¿ç®¡ç†**ï¼š
- è®¿é—® `http://localhost:5173`
- è¿›å…¥"ä»»åŠ¡ç®¡ç†"é¡µé¢
- æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€å’Œè¿›åº¦
- æ‰‹åŠ¨å¯åŠ¨/åœæ­¢ä»»åŠ¡
- æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—

**ä»»åŠ¡çŠ¶æ€è¯´æ˜**ï¼š
- `pending`ï¼šç­‰å¾…æ‰§è¡Œ
- `running`ï¼šæ­£åœ¨æ‰§è¡Œ
- `completed`ï¼šå·²å®Œæˆ
- `failed`ï¼šæ‰§è¡Œå¤±è´¥
- `cancelled`ï¼šå·²å–æ¶ˆ

### Q10: å¦‚ä½•è®¾ç½®å®šæ—¶è‡ªåŠ¨æŠ“å–ï¼Ÿ

**A**: ç³»ç»Ÿæ”¯æŒå¤šç§å®šæ—¶ä»»åŠ¡è®¾ç½®ï¼š

**æ–¹æ³•1ï¼šå†…å»ºè°ƒåº¦å™¨**
```bash
# æ·»åŠ å®šæ—¶ä»»åŠ¡
python main.py schedule add --name "morning-scrape" \
    --platform amazon \
    --cron "0 8 * * *" \
    --category "T-Shirt,Hoodie" \
    --keywords "cotton,organic"

# æŸ¥çœ‹å®šæ—¶ä»»åŠ¡
python main.py schedule list

# å¯ç”¨/ç¦ç”¨ä»»åŠ¡
python main.py schedule enable morning-scrape
python main.py schedule disable morning-scrape
```

**æ–¹æ³•2ï¼šç³»ç»Ÿcronï¼ˆLinux/macOSï¼‰**
```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ ä»»åŠ¡ï¼ˆæ¯å¤©ä¸Šåˆ8ç‚¹æ‰§è¡Œï¼‰
0 8 * * * cd /path/to/project && python main.py scrape --platform amazon --async

# æ·»åŠ ä»»åŠ¡ï¼ˆæ¯å‘¨æ—¥ä¸Šåˆ8ç‚¹æ‰§è¡Œï¼‰
0 8 * * 0 cd /path/to/project && python main.py scrape --platform all --async
```

**æ–¹æ³•3ï¼šWindowsä»»åŠ¡è®¡åˆ’ç¨‹åº**
- æ‰“å¼€"ä»»åŠ¡è®¡åˆ’ç¨‹åº"
- åˆ›å»ºåŸºæœ¬ä»»åŠ¡
- è®¾ç½®è§¦å‘å™¨ï¼ˆæ¯æ—¥ã€æ¯å‘¨ç­‰ï¼‰
- è®¾ç½®æ“ä½œï¼ˆè¿è¡Œpythonè„šæœ¬ï¼‰
- è®¾ç½®å‚æ•°

**cronè¡¨è¾¾å¼è¯´æ˜**ï¼š
```
# æ ¼å¼ï¼šç§’ åˆ† æ—¶ æ—¥ æœˆ æ˜ŸæœŸ
0 8 * * *        # æ¯å¤©8ç‚¹
0 8 * * 1        # æ¯å‘¨ä¸€8ç‚¹
0 9-17 * * 1-5   # å‘¨ä¸€åˆ°å‘¨äº”9ç‚¹åˆ°17ç‚¹æ¯å°æ—¶
0 */2 * * *      # æ¯2å°æ—¶
```

## æ•°æ®é—®é¢˜

### Q11: ä¸ºä»€ä¹ˆæŠ“å–åˆ°çš„æ•°æ®å¾ˆå°‘ï¼Ÿ

**A**: æ•°æ®é‡å°‘å¯èƒ½ç”±å¤šç§åŸå› é€ æˆï¼š

**æ£€æŸ¥åˆ—è¡¨**ï¼š
1. **å…³é”®è¯åŒ¹é…**ï¼š
   ```bash
   # æ£€æŸ¥å½“å‰å…³é”®è¯
   python main.py config show scraping.amazon.keywords
   
   # æµ‹è¯•å…³é”®è¯æœ‰æ•ˆæ€§
   python main.py scrape test --platform amazon --keywords "test-keyword"
   ```

2. **ç½‘ç»œè¿æ¥**ï¼š
   ```bash
   # æµ‹è¯•ç½‘ç»œè¿æ¥
   python main.py network test --platform amazon
   
   # æ£€æŸ¥ä»£ç†è®¾ç½®
   python main.py config show scraping.amazon.proxy
   ```

3. **å¹³å°é™åˆ¶**ï¼š
   - å¹³å°å¯èƒ½æœ‰åçˆ¬è™«æªæ–½
   - è¯·æ±‚é¢‘ç‡è¿‡é«˜è¢«é™åˆ¶
   - User-Agentè¢«è¯†åˆ«ä¸ºæœºå™¨äºº

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **è°ƒæ•´å…³é”®è¯**ï¼š
   ```yaml
   # ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯
   scraping:
     amazon:
       keywords:
         - "clothing"     # é€šç”¨è¯æ±‡
         - "fashion"      # æ—¶å°š
         - "apparel"      # æœè£…
   ```

2. **é™ä½è¯·æ±‚é¢‘ç‡**ï¼š
   ```yaml
   scraping:
     amazon:
       request_delay: 3.0     # å¢åŠ é—´éš”åˆ°3ç§’
       max_concurrent: 1      # é™ä½å¹¶å‘æ•°
   ```

3. **ä½¿ç”¨ä»£ç†**ï¼š
   ```yaml
   scraping:
     amazon:
       proxy:
         enabled: true
         rotation: true
         pool_size: 10
   ```

### Q12: æŠ“å–åˆ°çš„æ•°æ®ä¸å‡†ç¡®æ€ä¹ˆåŠï¼Ÿ

**A**: æ•°æ®å‡†ç¡®æ€§é—®é¢˜çš„å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

**å¸¸è§é—®é¢˜**ï¼š
1. **ä»·æ ¼æ ¼å¼ä¸ä¸€è‡´**
   - `$29.99`, `29.99`, `$30`

2. **äº§å“ä¿¡æ¯ç¼ºå¤±**
   - ç¼ºå°‘å“ç‰Œã€ä»·æ ¼ã€å›¾ç‰‡ç­‰

3. **é‡å¤æ•°æ®è¿‡å¤š**
   - åŒä¸€äº§å“å¤šæ¬¡æŠ“å–

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **å¯ç”¨æ•°æ®éªŒè¯**ï¼š
   ```yaml
   advanced:
     data_validation:
       enabled: true
       strict_mode: false
       required_fields:
         - "title"
         - "price"
         - "url"
   ```

2. **æ•°æ®æ¸…æ´—**ï¼š
   ```bash
   # æ¸…ç†é‡å¤æ•°æ®
   python main.py db cleanup --duplicates
   
   # ä¿®å¤ä»·æ ¼æ ¼å¼
   python main.py data fix-prices --format decimal
   
   # éªŒè¯æ•°æ®è´¨é‡
   python main.py data validate --fix-errors
   ```

3. **æ›´æ–°é€‰æ‹©å™¨**ï¼š
   ```bash
   # å¦‚æœé¡µé¢ç»“æ„å˜åŒ–ï¼Œæ›´æ–°CSSé€‰æ‹©å™¨
   python main.py config update-selectors --platform amazon
   ```

### Q13: å¦‚ä½•å¯¼å‡ºç‰¹å®šæ¡ä»¶çš„æ•°æ®ï¼Ÿ

**A**: ç³»ç»Ÿæä¾›çµæ´»çš„æ•°æ®å¯¼å‡ºåŠŸèƒ½ï¼š

**å‘½ä»¤è¡Œå¯¼å‡º**ï¼š
```bash
# å¯¼å‡ºAmazonçš„Tæ¤æ•°æ®
python main.py export --format csv \
    --platform amazon \
    --category "T-Shirt" \
    --output amazon_tshirts.csv

# å¯¼å‡ºä»·æ ¼èŒƒå›´æ•°æ®
python main.py export --format json \
    --price-min 20 \
    --price-max 100 \
    --output price_range_products.json

# å¯¼å‡ºç‰¹å®šå“ç‰Œ
python main.py export --format excel \
    --brand "Nike,Adidas" \
    --output nike_adidas_products.xlsx
```

**APIå¯¼å‡º**ï¼š
```python
from scraper_sdk import ScraperClient

client = ScraperClient(api_key="your-key")

# åˆ›å»ºå¯¼å‡ºä»»åŠ¡
export_task = client.export.create(
    format="json",
    filters={
        "platform": "amazon",
        "category": "T-Shirt",
        "created_after": "2025-11-01"
    },
    fields=["id", "title", "price", "rating", "brand"],
    options={
        "compress": True
    }
)

# ç­‰å¾…å®Œæˆå¹¶ä¸‹è½½
result = client.export.wait_for_completion(export_task.export_id)
download_url = result.download_url
```

**Webä»ªè¡¨æ¿å¯¼å‡º**ï¼š
- è®¿é—®ä»ªè¡¨æ¿ â†’ æ•°æ®å¯¼å‡ºé¡µé¢
- é€‰æ‹©å¯¼å‡ºæ ¼å¼å’Œç­›é€‰æ¡ä»¶
- è®¾ç½®å¯¼å‡ºå­—æ®µ
- æäº¤å¯¼å‡ºä»»åŠ¡
- ä¸‹è½½ç”Ÿæˆçš„å¯¼å‡ºæ–‡ä»¶

### Q14: æ•°æ®åº“æ–‡ä»¶å¾ˆå¤§ï¼Œå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**A**: æ•°æ®åº“ä¼˜åŒ–ç­–ç•¥ï¼š

**è‡ªåŠ¨ä¼˜åŒ–**ï¼š
```bash
# å¯ç”¨è‡ªåŠ¨æ¸…ç†
python main.py config set monitoring.auto_cleanup true
python main.py config set monitoring.cleanup_interval "24h"

# è®¾ç½®æ•°æ®ä¿ç•™æœŸ
python main.py config set data.retention_days 90
```

**æ‰‹åŠ¨ä¼˜åŒ–**ï¼š
```bash
# æ¸…ç†æ—§æ•°æ®
python main.py db cleanup --older-than 30d

# å‹ç¼©æ•°æ®åº“
python main.py db compress

# é‡å»ºç´¢å¼•
python main.py db reindex

# æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
python main.py db analyze
```

**é…ç½®ä¼˜åŒ–**ï¼š
```yaml
# è®¾ç½®æ•°æ®ä¿ç•™
data:
  retention_days: 90
  auto_cleanup: true
  batch_size: 1000

# æ•°æ®åº“ä¼˜åŒ–
database:
  optimization_level: "maximum"
  auto_vacuum: true
  cache_size: 10000
```

**åˆ†åº“ç­–ç•¥**ï¼š
```bash
# æŒ‰æ—¶é—´åˆ†å‰²æ•°æ®
python main.py db split --by-month --start-date 2025-01-01

# æŒ‰å¹³å°åˆ†å‰²
python main.py db split --by-platform
```

## æ€§èƒ½é—®é¢˜

### Q15: ç³»ç»Ÿè¿è¡Œé€Ÿåº¦å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š

**ç³»ç»Ÿçº§ä¼˜åŒ–**ï¼š
1. **æ£€æŸ¥ç³»ç»Ÿèµ„æº**ï¼š
   ```bash
   python main.py system resources
   
   # å¦‚æœCPUæˆ–å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
   # è€ƒè™‘å‡çº§ç¡¬ä»¶æˆ–å…³é—­å…¶ä»–ç¨‹åº
   ```

2. **ä¼˜åŒ–å¹¶å‘è®¾ç½®**ï¼š
   ```yaml
   scraping:
     amazon:
       max_concurrent: 8        # å¢åŠ å¹¶å‘æ•°ï¼ˆæ ¹æ®CPUè°ƒæ•´ï¼‰
       request_delay: 0.5       # é™ä½å»¶è¿Ÿï¼ˆæ³¨æ„ä¸è¦å¤ªä½ï¼‰
       connection_pool_size: 20
   ```

3. **å¯ç”¨ç¼“å­˜**ï¼š
   ```yaml
   performance:
     cache:
       enabled: true
       backend: "memory"
       ttl: 3600
       max_size: "500MB"
   ```

**åº”ç”¨çº§ä¼˜åŒ–**ï¼š
```bash
# ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
python main.py db optimize-queries

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find . -name "*.tmp" -delete
find . -name "*.cache" -delete

# ç›‘æ§ç³»ç»Ÿæ€§èƒ½
python main.py monitor performance --interval 10s
```

### Q16: å¦‚ä½•æé«˜æ•°æ®æŠ“å–é€Ÿåº¦ï¼Ÿ

**A**: æå‡æŠ“å–é€Ÿåº¦çš„ç­–ç•¥ï¼š

**å¹¶å‘ä¼˜åŒ–**ï¼š
```yaml
scraping:
  amazon:
    max_concurrent: 10      # æ ¹æ®CPUå’Œç½‘é€Ÿè°ƒæ•´
    request_delay: 0.3      # æœ€å°å®‰å…¨å»¶è¿Ÿ
    connection_pool_size: 50
    timeout: 15             # é™ä½è¶…æ—¶æ—¶é—´
  
  tiktok:
    max_concurrent: 8
    request_delay: 0.5
```

**ç½‘ç»œä¼˜åŒ–**ï¼š
```yaml
# ä½¿ç”¨é«˜è´¨é‡ä»£ç†
scraping:
  amazon:
    proxy:
      enabled: true
      rotation: true
      pool_size: 20
      test_url: "https://httpbin.org/ip"

# è¿æ¥æ± ä¼˜åŒ–
performance:
  connection_pool:
    max_connections: 100
    keep_alive: true
    pool_timeout: 30
```

**æ‰¹é‡å¤„ç†**ï¼š
```python
# ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢è€Œä¸æ˜¯å•ä¸ªæŸ¥è¯¢
# é”™è¯¯åšæ³•
for product_id in product_ids:
    product = client.products.get(product_id)

# æ­£ç¡®åšæ³•
products = client.products.batch_get(product_ids)
```

### Q17: å†…å­˜ä½¿ç”¨è¿‡é«˜æ€ä¹ˆå¤„ç†ï¼Ÿ

**A**: å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆï¼š

**åº”ç”¨é…ç½®**ï¼š
```yaml
performance:
  memory:
    max_heap_size: "2GB"
    gc_threshold: 700
    object_cache_size: 500
  
  # å¯ç”¨æµå¼å¤„ç†
  streaming:
    enabled: true
    batch_size: 100
    buffer_size: "50MB"
```

**ä»£ç ä¼˜åŒ–**ï¼š
```python
# åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡
import gc

def process_products():
    products = client.products.list(limit=10000)
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(products), 100):
        batch = products[i:i+100]
        process_batch(batch)
        
        # æ¸…ç†å†…å­˜
        del batch
        gc.collect()
```

**ç³»ç»Ÿçº§ä¼˜åŒ–**ï¼š
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
python main.py system memory-profile --duration 5m

# é‡å¯æœåŠ¡é‡Šæ”¾å†…å­˜
sudo systemctl restart tiktok-amazon-scraper

# è°ƒæ•´ç³»ç»Ÿvm.swappiness
echo 'vm.swappiness=10' >> /etc/sysctl.conf
sysctl -p
```

## ç½‘ç»œè¿æ¥

### Q18: å‡ºç°ç½‘ç»œè¿æ¥é”™è¯¯æ€ä¹ˆè§£å†³ï¼Ÿ

**A**: ç½‘ç»œè¿æ¥é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³ï¼š

**åŸºç¡€è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping google.com
ping amazon.com
ping tiktok.com

# æµ‹è¯•DNSè§£æ
nslookup amazon.com
dig tiktok.com

# æ£€æŸ¥ç«¯å£è¿æ¥
telnet amazon.com 443
telnet tiktok.com 443
```

**å¸¸è§è§£å†³æ–¹æ¡ˆ**ï¼š
1. **æ›´æ¢DNSæœåŠ¡å™¨**ï¼š
   ```bash
   # ç¼–è¾‘ /etc/resolv.conf
   nameserver 8.8.8.8
   nameserver 8.8.4.4
   ```

2. **é…ç½®ä»£ç†**ï¼š
   ```yaml
   scraping:
     amazon:
       proxy:
         enabled: true
         url: "http://proxy.example.com:8080"
         username: "user"
         password: "pass"
   ```

3. **è°ƒæ•´è¶…æ—¶è®¾ç½®**ï¼š
   ```yaml
   scraping:
     amazon:
       timeout: 60          # å¢åŠ è¶…æ—¶æ—¶é—´
       request_delay: 3.0   # å¢åŠ è¯·æ±‚é—´éš”
   ```

4. **æ›´æ–°User-Agent**ï¼š
   ```yaml
   scraping:
     amazon:
       user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
   ```

### Q19: å¦‚ä½•è§£å†³SSLè¯ä¹¦éªŒè¯å¤±è´¥ï¼Ÿ

**A**: SSLè¯ä¹¦é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼š

**æ›´æ–°è¯ä¹¦**ï¼š
```bash
# æ›´æ–°ç³»ç»Ÿè¯ä¹¦
sudo apt update && sudo apt install ca-certificates

# æ›´æ–°Pythonè¯ä¹¦
pip install --upgrade certifi
python -m certifi
```

**ä¸´æ—¶ç¦ç”¨éªŒè¯**ï¼ˆä¸æ¨èï¼‰ï¼š
```yaml
scraping:
  amazon:
    ssl_verify: false
    # ä»…ç”¨äºæµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨æ­£ç¡®è¯ä¹¦
```

**ä½¿ç”¨è‡ªå®šä¹‰è¯ä¹¦**ï¼š
```yaml
scraping:
  amazon:
    ssl_ca_bundle: "/path/to/ca-bundle.crt"
    ssl_verify: true
```

**ç¯å¢ƒå˜é‡è®¾ç½®**ï¼š
```bash
export PYTHONHTTPSVERIFY=0
export SSL_CERT_FILE=/path/to/cert.pem
```

## é”™è¯¯å¤„ç†

### Q20: å‡ºç°"database is locked"é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: æ•°æ®åº“é”å®šé—®é¢˜çš„è§£å†³ï¼š

**ç«‹å³å¤„ç†**ï¼š
```bash
# 1. æ£€æŸ¥æ´»è·ƒè¿æ¥
lsof data/scraping.db

# 2. ç»ˆæ­¢å ç”¨è¿›ç¨‹
kill -9 <PID>

# 3. é‡å¯æœåŠ¡
sudo systemctl restart tiktok-amazon-scraper
```

**æ ¹æœ¬è§£å†³**ï¼š
1. **æ£€æŸ¥å¹¶å‘è®¿é—®**ï¼š
   ```bash
   # ç¡®ä¿æ²¡æœ‰å¤šä¸ªè¿›ç¨‹åŒæ—¶è®¿é—®æ•°æ®åº“
   pgrep -f "python main.py"
   ```

2. **å¢åŠ è¿æ¥è¶…æ—¶**ï¼š
   ```yaml
   database:
     timeout: 30
     isolation_level: "SERIALIZABLE"
   ```

3. **ä½¿ç”¨WALæ¨¡å¼**ï¼š
   ```yaml
   database:
     journal_mode: WAL
     synchronous: NORMAL
   ```

### Q21: ç³»ç»Ÿæ—¥å¿—æ˜¾ç¤ºå¾ˆå¤šé”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: é”™è¯¯åˆ†æå’Œå¤„ç†ï¼š

**æŸ¥çœ‹é”™è¯¯ç±»å‹**ï¼š
```bash
# åˆ†æé”™è¯¯ç»Ÿè®¡
python main.py error analysis --period 24h

# æŸ¥çœ‹æœ€å¸¸è§é”™è¯¯
python main.py error top-errors --count 10

# æŒ‰çº§åˆ«ç­›é€‰æ—¥å¿—
grep "ERROR" logs/coordinator.log
```

**å¸¸è§é”™è¯¯å¤„ç†**ï¼š
1. **è¿æ¥è¶…æ—¶**ï¼š
   ```yaml
   scraping:
     amazon:
       timeout: 60
       retry_count: 5
       retry_delay: 10
   ```

2. **è§£æé”™è¯¯**ï¼š
   ```yaml
   advanced:
     data_validation:
       strict_mode: false
       skip_errors: true
   ```

3. **å†…å­˜ä¸è¶³**ï¼š
   ```bash
   # é™ä½å¹¶å‘æ•°
   python main.py config set scraping.amazon.max_concurrent 2
   
   # æ¸…ç†å†…å­˜
   python main.py system memory-cleanup
   ```

**é”™è¯¯ç›‘æ§**ï¼š
```bash
# å®æ—¶é”™è¯¯ç›‘æ§
python main.py monitor --level ERROR --alert-email admin@example.com

# è‡ªåŠ¨é”™è¯¯æŠ¥å‘Š
python main.py error report --auto-send --daily
```

## åŠŸèƒ½ç›¸å…³

### Q22: Webä»ªè¡¨æ¿æ— æ³•è®¿é—®æ€ä¹ˆåŠï¼Ÿ

**A**: å‰ç«¯ä»ªè¡¨æ¿é—®é¢˜æ’æŸ¥ï¼š

**æ£€æŸ¥æœåŠ¡çŠ¶æ€**ï¼š
```bash
# æ£€æŸ¥å‰ç«¯æœåŠ¡
curl http://localhost:5173

# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
lsof -i :5173

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
tail -f fashion-dashboard/logs/*.log
```

**é‡æ–°å¯åŠ¨æœåŠ¡**ï¼š
```bash
cd fashion-dashboard

# å®‰è£…ä¾èµ–
npm install

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬
npm run build

# å¯åŠ¨æœåŠ¡
npm run dev

# æˆ–ä½¿ç”¨ä¸åŒç«¯å£
npm run dev -- --port 3000
```

**å¸¸è§è§£å†³æ–¹æ¡ˆ**ï¼š
1. **ç«¯å£å†²çª**ï¼š
   ```bash
   # æŸ¥æ‰¾å ç”¨è¿›ç¨‹
   lsof -i :5173
   
   # æ€æ­»è¿›ç¨‹æˆ–ä½¿ç”¨ä¸åŒç«¯å£
   npm run dev -- --port 3001
   ```

2. **ä¾èµ–é—®é¢˜**ï¼š
   ```bash
   # æ¸…ç†é‡æ–°å®‰è£…
   rm -rf node_modules package-lock.json
   npm cache clean --force
   npm install
   ```

3. **é˜²ç«å¢™é—®é¢˜**ï¼š
   ```bash
   # å¼€æ”¾ç«¯å£
   sudo ufw allow 5173/tcp
   
   # æˆ–ä¸´æ—¶å…³é—­é˜²ç«å¢™æµ‹è¯•
   sudo ufw disable
   ```

### Q23: å¦‚ä½•è‡ªå®šä¹‰æ•°æ®å­—æ®µå’Œæ ¼å¼ï¼Ÿ

**A**: æ•°æ®è‡ªå®šä¹‰é€‰é¡¹ï¼š

**é€‰æ‹©å¯¼å‡ºå­—æ®µ**ï¼š
```bash
# å‘½ä»¤è¡ŒæŒ‡å®šå­—æ®µ
python main.py export --fields "id,title,price,brand,rating" --format csv

# APIè‡ªå®šä¹‰å­—æ®µ
export_task = client.export.create(
    fields=[
        "id", "title", "price", "currency", 
        "brand", "category", "rating", "review_count",
        "url", "image_url", "created_at"
    ]
)
```

**è‡ªå®šä¹‰æ•°æ®æ ¼å¼**ï¼š
```yaml
# config/config.yaml
output:
  format: "json"           # "json", "csv", "excel", "xml"
  date_format: "%Y-%m-%d"  # æ—¥æœŸæ ¼å¼
  number_format: "%.2f"    # æ•°å­—æ ¼å¼
  encoding: "utf-8"        # ç¼–ç æ ¼å¼
  
  # JSONæ ¼å¼åŒ–
  json:
    indent: 2
    sort_keys: true
    ensure_ascii: false
  
  # CSVè®¾ç½®
  csv:
    delimiter: ","
    quote_char: '"'
    include_headers: true
```

**æ•°æ®è½¬æ¢**ï¼š
```python
# ä½¿ç”¨Pythonè„šæœ¬è‡ªå®šä¹‰
import pandas as pd
from scraper_sdk import ScraperClient

def custom_data_export():
    client = ScraperClient(api_key="your-key")
    
    # è·å–åŸå§‹æ•°æ®
    products = client.products.list(platform="amazon", limit=1000)
    
    # è½¬æ¢ä¸ºDataFrame
    data = [p.to_dict() for p in products]
    df = pd.DataFrame(data)
    
    # è‡ªå®šä¹‰å­—æ®µå¤„ç†
    df['price_usd'] = df['price']  # ä»·æ ¼å­—æ®µé‡å‘½å
    df['brand_upper'] = df['brand'].str.upper()  # å“ç‰Œè½¬å¤§å†™
    df['created_date'] = pd.to_datetime(df['created_at']).dt.date  # æ—¥æœŸæ ¼å¼åŒ–
    
    # é€‰æ‹©æœ€ç»ˆå­—æ®µ
    final_df = df[['title', 'price_usd', 'brand_upper', 'rating', 'created_date']]
    
    # å¯¼å‡º
    final_df.to_excel('custom_export.xlsx', index=False)
```

### Q24: ç³»ç»Ÿæ”¯æŒå“ªäº›æ•°æ®æ ¼å¼å¯¼å‡ºï¼Ÿ

**A**: æ”¯æŒçš„å¯¼å‡ºæ ¼å¼ï¼š

**JSONæ ¼å¼**ï¼š
```bash
# æ ‡å‡†JSON
python main.py export --format json --output products.json

# ç¾åŒ–JSON
python main.py export --format json --pretty --output pretty_products.json

# å‹ç¼©JSON
python main.py export --format json --compress --output products.json.gz
```

**CSVæ ¼å¼**ï¼š
```bash
# æ ‡å‡†CSV
python main.py export --format csv --output products.csv

# è‡ªå®šä¹‰åˆ†éš”ç¬¦
python main.py export --format csv --delimiter ";" --output products_semicolon.csv

# åŒ…å«å…ƒæ•°æ®
python main.py export --format csv --include-metadata --output products_with_meta.csv
```

**Excelæ ¼å¼**ï¼š
```bash
# åŸºç¡€Excel
python main.py export --format excel --output products.xlsx

# å¤šå·¥ä½œè¡¨
python main.py export --format excel --worksheets "Amazon,TikTok" --output platforms.xlsx

# å¸¦æ ¼å¼çš„Excel
python main.py export --format excel --styling --output styled_products.xlsx
```

**XMLæ ¼å¼**ï¼š
```bash
python main.py export --format xml --output products.xml
```

**æ•°æ®åº“å¯¼å‡º**ï¼š
```bash
# SQLå¯¼å‡º
python main.py export --format sql --output products.sql

# SQLiteå¯¼å‡º
python main.py export --format sqlite --output products_backup.db
```

## ä»˜è´¹è®¡è´¹

### Q25: ç³»ç»Ÿçš„å®šä»·æ¨¡å¼æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: å®šä»·ä¿¡æ¯ï¼ˆè¯·ä»¥æœ€æ–°å®˜æ–¹ä¿¡æ¯ä¸ºå‡†ï¼‰ï¼š

**å…è´¹ç‰ˆæœ¬**ï¼š
- æ¯æ—¥æŠ“å–é™é¢ï¼š100ä¸ªäº§å“
- æ•°æ®ä¿ç•™æœŸï¼š7å¤©
- æ”¯æŒå¹³å°ï¼šAmazon
- å¯¼å‡ºæ ¼å¼ï¼šCSV
- å®¢æœæ”¯æŒï¼šç¤¾åŒºè®ºå›

**æ ‡å‡†ç‰ˆï¼ˆæœˆä»˜ï¼‰**ï¼š
- ä»·æ ¼ï¼š$29/æœˆ
- æ¯æ—¥æŠ“å–é™é¢ï¼š5,000ä¸ªäº§å“
- æ•°æ®ä¿ç•™æœŸï¼š30å¤©
- æ”¯æŒå¹³å°ï¼šAmazon + TikTok
- å¯¼å‡ºæ ¼å¼ï¼šJSON, CSV, Excel
- å®¢æœæ”¯æŒï¼šé‚®ä»¶æ”¯æŒ

**ä¸“ä¸šç‰ˆï¼ˆæœˆä»˜ï¼‰**ï¼š
- ä»·æ ¼ï¼š$99/æœˆ
- æ¯æ—¥æŠ“å–é™é¢ï¼š50,000ä¸ªäº§å“
- æ•°æ®ä¿ç•™æœŸï¼š90å¤©
- æ”¯æŒå¹³å°ï¼šAmazon + TikTok + æ›´å¤š
- å¯¼å‡ºæ ¼å¼ï¼šæ‰€æœ‰æ ¼å¼ + APIè®¿é—®
- å®¢æœæ”¯æŒï¼šä¼˜å…ˆæ”¯æŒ + ç”µè¯æ”¯æŒ

**ä¼ä¸šç‰ˆ**ï¼š
- ä»·æ ¼ï¼šå®šåˆ¶æŠ¥ä»·
- æ¯æ—¥æŠ“å–é™é¢ï¼šæ— é™åˆ¶
- æ•°æ®ä¿ç•™æœŸï¼šæ— é™åˆ¶
- æ”¯æŒå¹³å°ï¼šå…¨éƒ¨å¹³å°
- å¯¼å‡ºæ ¼å¼ï¼šå…¨éƒ¨æ ¼å¼ + å®šåˆ¶
- å®¢æœæ”¯æŒï¼šä¸“å±å®¢æˆ·ç»ç† + SLAä¿éšœ

### Q26: å¦‚ä½•å‡çº§æˆ–å–æ¶ˆè®¢é˜…ï¼Ÿ

**A**: è®¢é˜…ç®¡ç†æ“ä½œï¼š

**å‡çº§è®¢é˜…**ï¼š
```bash
# é€šè¿‡å‘½ä»¤è¡Œå‡çº§
python main.py billing upgrade --plan professional

# é€šè¿‡Webä»ªè¡¨æ¿
# ç™»å½•ä»ªè¡¨æ¿ â†’ è´¦æˆ·è®¾ç½® â†’ è®¢é˜…ç®¡ç† â†’ å‡çº§è®¡åˆ’
```

**å–æ¶ˆè®¢é˜…**ï¼š
```bash
# é€šè¿‡å‘½ä»¤è¡Œå–æ¶ˆ
python main.py billing cancel --reason "ä½¿ç”¨é‡å‡å°‘"

# é€šè¿‡Webä»ªè¡¨æ¿
# ç™»å½•ä»ªè¡¨æ¿ â†’ è´¦æˆ·è®¾ç½® â†’ è®¢é˜…ç®¡ç† â†’ å–æ¶ˆè®¢é˜…
```

**æŸ¥çœ‹è´¦å•**ï¼š
```bash
# æŸ¥çœ‹å½“å‰è®¡åˆ’
python main.py billing status

# æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡
python main.py billing usage --period current

# æŸ¥çœ‹è´¦å•å†å²
python main.py billing invoices
```

**æ³¨æ„äº‹é¡¹**ï¼š
- å–æ¶ˆè®¢é˜…åï¼Œæ•°æ®ä»å¯è®¿é—®åˆ°å½“å‰è®¡è´¹å‘¨æœŸç»“æŸ
- å‡çº§å³æ—¶ç”Ÿæ•ˆï¼Œç«‹å³è·å¾—æ–°é…é¢
- ä¼ä¸šç‰ˆå˜æ›´éœ€è¦è”ç³»é”€å”®å›¢é˜Ÿ

## æŠ€æœ¯å’¨è¯¢

### Q27: å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ

**A**: å¤šç§æŠ€æœ¯æ”¯æŒæ¸ é“ï¼š

**å…è´¹ç”¨æˆ·**ï¼š
- æ–‡æ¡£ä¸­å¿ƒï¼š[docs.scraper-system.com](https://docs.scraper-system.com)
- ç¤¾åŒºè®ºå›ï¼š[community.scraper-system.com](https://community.scraper-system.com)
- GitHub Issuesï¼š[github.com/scraper-system/issues](https://github.com/scraper-system/issues)

**ä»˜è´¹ç”¨æˆ·**ï¼š
- é‚®ä»¶æ”¯æŒï¼šsupport@scraper-system.com
- å“åº”æ—¶é—´ï¼š24å°æ—¶å†…
- æ”¯æŒå†…å®¹ï¼šæŠ€æœ¯é—®é¢˜ã€åŠŸèƒ½å’¨è¯¢ã€æ•…éšœæ’é™¤

**ä¼ä¸šç”¨æˆ·**ï¼š
- ä¸“å±å®¢æˆ·ç»ç†
- ç”µè¯æ”¯æŒï¼š+1-XXX-XXX-XXXX
- ç´§æ€¥æ”¯æŒï¼š7x24å°æ—¶
- SLAä¿éšœï¼š99.9%å¯ç”¨æ€§

**æ”¯æŒå›¢é˜Ÿå·¥ä½œæ—¶é—´**ï¼š
- ç¾å›½ä¸œéƒ¨æ—¶é—´ï¼šå‘¨ä¸€è‡³å‘¨äº” 9:00-18:00
- ä¸­å›½æ—¶é—´ï¼šå‘¨ä¸€è‡³å‘¨äº” 9:00-18:00
- ä¼ä¸šå®¢æˆ·ï¼š7x24å°æ—¶

### Q28: å¦‚ä½•æŠ¥å‘ŠBugæˆ–æäº¤åŠŸèƒ½å»ºè®®ï¼Ÿ

**A**: é—®é¢˜åé¦ˆæµç¨‹ï¼š

**BugæŠ¥å‘Š**ï¼š
```bash
# ç”Ÿæˆç³»ç»ŸæŠ¥å‘Š
python main.py system report --output bug_report.json

# åŒ…å«æ—¥å¿—å’Œé…ç½®
python main.py system report --include-logs --include-config --output complete_report.json
```

**GitHub Issues**ï¼š
1. è®¿é—® [GitHub Issuesé¡µé¢](https://github.com/scraper-system/issues)
2. ç‚¹å‡»"New Issue"
3. é€‰æ‹©"Bug Report"æ¨¡æ¿
4. å¡«å†™è¯¦ç»†ä¿¡æ¯ï¼š
   - é—®é¢˜æè¿°
   - å¤ç°æ­¥éª¤
   - ç³»ç»Ÿç¯å¢ƒ
   - æ—¥å¿—æ–‡ä»¶
   - æˆªå›¾æˆ–å½•å±

**åŠŸèƒ½å»ºè®®**ï¼š
1. åœ¨GitHubä¸Šæœç´¢æ˜¯å¦å·²æœ‰ç±»ä¼¼å»ºè®®
2. ä½¿ç”¨"Feature Request"æ¨¡æ¿
3. æè¿°é¢„æœŸåŠŸèƒ½å’Œç”¨ä¾‹
4. è¯´æ˜å¯¹ç°æœ‰åŠŸèƒ½çš„å½±å“

**é‚®ä»¶åé¦ˆ**ï¼š
- å‘é€é‚®ä»¶åˆ°ï¼šfeedback@scraper-system.com
- åŒ…å«è¯¦ç»†çš„ç”¨ä¾‹è¯´æ˜
- é™„ä¸Šç›¸å…³æˆªå›¾æˆ–ç¤ºä¾‹

### Q29: ç³»ç»Ÿæ˜¯å¦æ”¯æŒAPIè®¿é—®ï¼Ÿ

**A**: æ˜¯çš„ï¼Œç³»ç»Ÿæä¾›å®Œæ•´çš„APIè®¿é—®ï¼š

**APIåŠŸèƒ½**ï¼š
- äº§å“æŸ¥è¯¢å’Œæœç´¢
- æ•°æ®åˆ†æå’Œç»Ÿè®¡
- æŠ“å–ä»»åŠ¡ç®¡ç†
- æ•°æ®å¯¼å‡ºæ¥å£
- ç³»ç»ŸçŠ¶æ€ç›‘æ§

**è®¿é—®æ–¹å¼**ï¼š
```python
from scraper_sdk import ScraperClient

client = ScraperClient(
    base_url="https://api.scraper-system.com/v1",
    api_key="your-api-key"
)

# æŸ¥è¯¢äº§å“
products = client.products.list(platform="amazon", limit=100)

# å¯åŠ¨æŠ“å–ä»»åŠ¡
task = client.scraping.start_task(
    platform="tiktok",
    categories=["æœè£…"],
    keywords=["å°èŠ±"]
)

# å¯¼å‡ºæ•°æ®
export_task = client.export.create(
    format="json",
    filters={"platform": "amazon"}
)
```

**APIæ–‡æ¡£**ï¼š
- åœ¨çº¿æ–‡æ¡£ï¼š[api.scraper-system.com](https://api.scraper-system.com)
- OpenAPIè§„èŒƒï¼š[api.scraper-system.com/docs](https://api.scraper-system.com/docs)
- SDKç¤ºä¾‹ï¼š[github.com/scraper-system/sdk-examples](https://github.com/scraper-system/sdk-examples)

### Q30: å¦‚ä½•å¤‡ä»½å’Œè¿ç§»æ•°æ®ï¼Ÿ

**A**: æ•°æ®å¤‡ä»½å’Œè¿ç§»æŒ‡å—ï¼š

**æ‰‹åŠ¨å¤‡ä»½**ï¼š
```bash
# åˆ›å»ºå®Œæ•´å¤‡ä»½
python main.py backup create --type full --comment "2025-11-14-å¤‡ä»½"

# åˆ›å»ºå¢é‡å¤‡ä»½
python main.py backup create --type incremental

# å¤‡ä»½åˆ°æŒ‡å®šä½ç½®
python main.py backup create --path "/backup/scraping_$(date +%Y%m%d).db"
```

**è‡ªåŠ¨å¤‡ä»½é…ç½®**ï¼š
```yaml
# config/config.yaml
backup:
  enabled: true
  schedule:
    full_backup:
      cron: "0 2 * * 0"    # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
      retention: 4         # ä¿ç•™4å‘¨
    incremental_backup:
      cron: "0 2 * * 1-6"  # æ¯å¤©å‡Œæ™¨2ç‚¹ï¼ˆå‘¨æ—¥é™¤å¤–ï¼‰
      retention: 7         # ä¿ç•™7å¤©
  
  locations:
    local:
      path: "/backup/scraper/"
    remote:
      enabled: true
      type: "s3"
      bucket: "scraper-backups"
```

**æ•°æ®è¿ç§»**ï¼š
```bash
# å¯¼å‡ºæ•°æ®
python main.py export --format json --output migration_data.json

# åœ¨æ–°ç³»ç»Ÿä¸Šå¯¼å…¥
python main.py import --file migration_data.json --merge

# æˆ–ç›´æ¥å¤åˆ¶æ•°æ®åº“æ–‡ä»¶
cp old_data/scraping.db new_data/scraping.db
python main.py db verify --fix-errors
```

**è¿ç§»æ£€æŸ¥æ¸…å•**ï¼š
- [ ] å¤‡ä»½åŸå§‹æ•°æ®
- [ ] å¯¼å‡ºé…ç½®ä¿¡æ¯
- [ ] è®°å½•APIå¯†é’¥å’Œè®¾ç½®
- [ ] å¯¼å‡ºå†å²ä»»åŠ¡å’ŒæŠ¥å‘Š
- [ ] æµ‹è¯•æ–°ç³»ç»ŸåŠŸèƒ½
- [ ] éªŒè¯æ•°æ®å®Œæ•´æ€§

## æœ€ä½³å®è·µ

### Q31: ä½¿ç”¨ç³»ç»Ÿæœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿ

**A**: ä½¿ç”¨æœ€ä½³å®è·µå»ºè®®ï¼š

**åˆè§„ä½¿ç”¨**ï¼š
- éµå®ˆå¹³å°æœåŠ¡æ¡æ¬¾
- åˆç†æ§åˆ¶æŠ“å–é¢‘ç‡
- ä¸è¿›è¡Œæ¶æ„æ”»å‡»
- å°Šé‡ç‰ˆæƒå’ŒçŸ¥è¯†äº§æƒ

**æ€§èƒ½ä¼˜åŒ–**ï¼š
- æ ¹æ®ç¡¬ä»¶è°ƒæ•´å¹¶å‘æ•°
- å®šæœŸæ¸…ç†æ— ç”¨æ•°æ®
- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨
- å¯ç”¨åˆé€‚çš„ç¼“å­˜ç­–ç•¥

**æ•°æ®ç®¡ç†**ï¼š
- å®šæœŸå¤‡ä»½é‡è¦æ•°æ®
- è®¾ç½®åˆç†çš„æ•°æ®ä¿ç•™æœŸ
- ä½¿ç”¨æ•°æ®éªŒè¯åŠŸèƒ½
- ç›‘æ§æ•°æ®è´¨é‡æŒ‡æ ‡

**å®‰å…¨è€ƒè™‘**ï¼š
- ä¿æŠ¤APIå¯†é’¥å®‰å…¨
- å®šæœŸæ›´æ–°ç³»ç»Ÿç»„ä»¶
- ä½¿ç”¨HTTPSåŠ å¯†ä¼ è¾“
- ç›‘æ§ç³»ç»Ÿè®¿é—®æ—¥å¿—

### Q32: å¦‚ä½•ç¡®ä¿æ•°æ®å®‰å…¨å’Œéšç§ï¼Ÿ

**A**: æ•°æ®å®‰å…¨ä¿éšœæªæ–½ï¼š

**æ•°æ®å­˜å‚¨**ï¼š
- æœ¬åœ°SQLiteæ•°æ®åº“å­˜å‚¨
- å¯é€‰åŠ å¯†å­˜å‚¨æ•æ„Ÿä¿¡æ¯
- å®šæœŸæ•°æ®åº“å¤‡ä»½
- è®¿é—®æ—¥å¿—è®°å½•

**ç½‘ç»œä¼ è¾“**ï¼š
- æ‰€æœ‰APIé€šä¿¡ä½¿ç”¨HTTPS
- æ”¯æŒSSL/TLSåŠ å¯†
- å¯é…ç½®ä»£ç†æœåŠ¡å™¨
- ç½‘ç»œæµé‡ç›‘æ§

**è®¿é—®æ§åˆ¶**ï¼š
- APIå¯†é’¥è®¤è¯
- ç”¨æˆ·æƒé™ç®¡ç†
- æ“ä½œå®¡è®¡æ—¥å¿—
- ä¼šè¯è¶…æ—¶æœºåˆ¶

**éšç§ä¿æŠ¤**ï¼š
- ä¸æ”¶é›†ä¸ªäººä¿¡æ¯
- äº§å“æ•°æ®è„±æ•å¤„ç†
- éµå®ˆGDPRç­‰æ³•è§„
- æ•°æ®åˆ é™¤åŠŸèƒ½

**å®‰å…¨é…ç½®ç¤ºä¾‹**ï¼š
```yaml
security:
  encryption:
    enabled: true
    algorithm: "AES-256-GCM"
    key_rotation: "90d"
  
  audit:
    enabled: true
    log_access: true
    log_changes: true
    retention: "1y"
  
  authentication:
    api_key_rotation: "30d"
    session_timeout: "24h"
```

---

## æ€»ç»“

æœ¬FAQæ–‡æ¡£æ¶µç›–äº†TikTok & Amazonæœè£…æ•°æ®ç³»ç»Ÿçš„å¸¸è§é—®é¢˜ï¼š

- **åŸºç¡€æ“ä½œ**ï¼šå®‰è£…ã€é…ç½®ã€ä½¿ç”¨
- **æŠ€æœ¯é—®é¢˜**ï¼šç½‘ç»œã€æ€§èƒ½ã€é”™è¯¯å¤„ç†
- **åŠŸèƒ½ä½¿ç”¨**ï¼šæ•°æ®æŠ“å–ã€åˆ†æã€å¯¼å‡º
- **æŠ€æœ¯æ”¯æŒ**ï¼šè”ç³»æ¸ é“ã€é—®é¢˜åé¦ˆ

å¦‚æœæ‚¨çš„é—®é¢˜åœ¨FAQä¸­æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆï¼š

1. **æŸ¥çœ‹å®Œæ•´æ–‡æ¡£**ï¼š[ç”¨æˆ·æŒ‡å—](user_guide.md)ã€[APIæ–‡æ¡£](api_reference.md)
2. **æœç´¢ç¤¾åŒºè®ºå›**ï¼šå…¶ä»–ç”¨æˆ·çš„è®¨è®ºå’Œè§£å†³æ–¹æ¡ˆ
3. **è”ç³»æŠ€æœ¯æ”¯æŒ**ï¼šæäº¤è¯¦ç»†çš„é—®é¢˜æè¿°å’Œæ—¥å¿—

æˆ‘ä»¬è‡´åŠ›äºä¸ºç”¨æˆ·æä¾›æœ€å¥½çš„ä½¿ç”¨ä½“éªŒï¼ŒæŒç»­æ”¹è¿›äº§å“è´¨é‡å’ŒæœåŠ¡æ°´å¹³ã€‚

**è®°ä½**ï¼šåœ¨ä½¿ç”¨ç³»ç»Ÿå‰ï¼Œè¯·åŠ¡å¿…é˜…è¯»[ç”¨æˆ·æŒ‡å—](user_guide.md)å¹¶éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œå¹³å°æœåŠ¡æ¡æ¬¾ã€‚